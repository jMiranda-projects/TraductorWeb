{
  "best_global_step": 30000,
  "best_metric": 0.2469032108783722,
  "best_model_checkpoint": "./output\\checkpoint-30000",
  "epoch": 39.99746626598315,
  "eval_steps": 5000,
  "global_step": 84840,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023569618761416535,
      "grad_norm": 8.841391563415527,
      "learning_rate": 1.7326732673267326e-07,
      "loss": 1.4511,
      "step": 50
    },
    {
      "epoch": 0.04713923752283307,
      "grad_norm": 9.775507926940918,
      "learning_rate": 3.500707213578501e-07,
      "loss": 1.3697,
      "step": 100
    },
    {
      "epoch": 0.0707088562842496,
      "grad_norm": 6.792088508605957,
      "learning_rate": 5.26874115983027e-07,
      "loss": 1.3688,
      "step": 150
    },
    {
      "epoch": 0.09427847504566614,
      "grad_norm": 7.706776142120361,
      "learning_rate": 7.036775106082037e-07,
      "loss": 1.1963,
      "step": 200
    },
    {
      "epoch": 0.11784809380708267,
      "grad_norm": 6.4689741134643555,
      "learning_rate": 8.804809052333805e-07,
      "loss": 1.1313,
      "step": 250
    },
    {
      "epoch": 0.1414177125684992,
      "grad_norm": 7.565453052520752,
      "learning_rate": 1.0572842998585574e-06,
      "loss": 1.049,
      "step": 300
    },
    {
      "epoch": 0.16498733132991575,
      "grad_norm": 6.0627970695495605,
      "learning_rate": 1.2340876944837342e-06,
      "loss": 0.9933,
      "step": 350
    },
    {
      "epoch": 0.18855695009133228,
      "grad_norm": 5.993167400360107,
      "learning_rate": 1.410891089108911e-06,
      "loss": 0.9919,
      "step": 400
    },
    {
      "epoch": 0.2121265688527488,
      "grad_norm": 7.244719982147217,
      "learning_rate": 1.5876944837340877e-06,
      "loss": 0.9746,
      "step": 450
    },
    {
      "epoch": 0.23569618761416533,
      "grad_norm": 4.76690673828125,
      "learning_rate": 1.7644978783592645e-06,
      "loss": 0.8736,
      "step": 500
    },
    {
      "epoch": 0.2592658063755819,
      "grad_norm": 6.728882312774658,
      "learning_rate": 1.9413012729844415e-06,
      "loss": 0.8921,
      "step": 550
    },
    {
      "epoch": 0.2828354251369984,
      "grad_norm": 6.294009685516357,
      "learning_rate": 2.1181046676096183e-06,
      "loss": 0.833,
      "step": 600
    },
    {
      "epoch": 0.30640504389841494,
      "grad_norm": 7.145051956176758,
      "learning_rate": 2.294908062234795e-06,
      "loss": 0.8359,
      "step": 650
    },
    {
      "epoch": 0.3299746626598315,
      "grad_norm": 5.290494918823242,
      "learning_rate": 2.471711456859972e-06,
      "loss": 0.785,
      "step": 700
    },
    {
      "epoch": 0.353544281421248,
      "grad_norm": 6.543895244598389,
      "learning_rate": 2.6485148514851486e-06,
      "loss": 0.8169,
      "step": 750
    },
    {
      "epoch": 0.37711390018266455,
      "grad_norm": 6.393647193908691,
      "learning_rate": 2.8253182461103254e-06,
      "loss": 0.79,
      "step": 800
    },
    {
      "epoch": 0.40068351894408105,
      "grad_norm": 5.43754243850708,
      "learning_rate": 3.002121640735502e-06,
      "loss": 0.7537,
      "step": 850
    },
    {
      "epoch": 0.4242531377054976,
      "grad_norm": 5.292486190795898,
      "learning_rate": 3.178925035360679e-06,
      "loss": 0.7302,
      "step": 900
    },
    {
      "epoch": 0.44782275646691416,
      "grad_norm": 4.516923904418945,
      "learning_rate": 3.3557284299858557e-06,
      "loss": 0.71,
      "step": 950
    },
    {
      "epoch": 0.47139237522833066,
      "grad_norm": 5.0908308029174805,
      "learning_rate": 3.5325318246110325e-06,
      "loss": 0.7114,
      "step": 1000
    },
    {
      "epoch": 0.4949619939897472,
      "grad_norm": 5.187556743621826,
      "learning_rate": 3.7093352192362093e-06,
      "loss": 0.7083,
      "step": 1050
    },
    {
      "epoch": 0.5185316127511638,
      "grad_norm": 4.349359512329102,
      "learning_rate": 3.886138613861387e-06,
      "loss": 0.6811,
      "step": 1100
    },
    {
      "epoch": 0.5421012315125803,
      "grad_norm": 5.90949010848999,
      "learning_rate": 4.062942008486563e-06,
      "loss": 0.6901,
      "step": 1150
    },
    {
      "epoch": 0.5656708502739968,
      "grad_norm": 7.364394187927246,
      "learning_rate": 4.2397454031117405e-06,
      "loss": 0.6651,
      "step": 1200
    },
    {
      "epoch": 0.5892404690354134,
      "grad_norm": 6.23889684677124,
      "learning_rate": 4.416548797736917e-06,
      "loss": 0.6777,
      "step": 1250
    },
    {
      "epoch": 0.6128100877968299,
      "grad_norm": 6.815250873565674,
      "learning_rate": 4.593352192362094e-06,
      "loss": 0.6503,
      "step": 1300
    },
    {
      "epoch": 0.6363797065582464,
      "grad_norm": 5.341848850250244,
      "learning_rate": 4.77015558698727e-06,
      "loss": 0.6763,
      "step": 1350
    },
    {
      "epoch": 0.659949325319663,
      "grad_norm": 5.505080699920654,
      "learning_rate": 4.946958981612448e-06,
      "loss": 0.6355,
      "step": 1400
    },
    {
      "epoch": 0.6835189440810795,
      "grad_norm": 5.608134746551514,
      "learning_rate": 5.123762376237624e-06,
      "loss": 0.6458,
      "step": 1450
    },
    {
      "epoch": 0.707088562842496,
      "grad_norm": 4.51871395111084,
      "learning_rate": 5.300565770862801e-06,
      "loss": 0.6021,
      "step": 1500
    },
    {
      "epoch": 0.7306581816039126,
      "grad_norm": 4.250080108642578,
      "learning_rate": 5.4773691654879775e-06,
      "loss": 0.6232,
      "step": 1550
    },
    {
      "epoch": 0.7542278003653291,
      "grad_norm": 4.514811038970947,
      "learning_rate": 5.654172560113155e-06,
      "loss": 0.6407,
      "step": 1600
    },
    {
      "epoch": 0.7777974191267456,
      "grad_norm": 5.5440592765808105,
      "learning_rate": 5.830975954738332e-06,
      "loss": 0.6019,
      "step": 1650
    },
    {
      "epoch": 0.8013670378881621,
      "grad_norm": 3.8611326217651367,
      "learning_rate": 6.007779349363508e-06,
      "loss": 0.6251,
      "step": 1700
    },
    {
      "epoch": 0.8249366566495787,
      "grad_norm": 5.817086219787598,
      "learning_rate": 6.1845827439886855e-06,
      "loss": 0.5907,
      "step": 1750
    },
    {
      "epoch": 0.8485062754109952,
      "grad_norm": 5.346348762512207,
      "learning_rate": 6.361386138613862e-06,
      "loss": 0.5875,
      "step": 1800
    },
    {
      "epoch": 0.8720758941724117,
      "grad_norm": 5.6445841789245605,
      "learning_rate": 6.538189533239039e-06,
      "loss": 0.5607,
      "step": 1850
    },
    {
      "epoch": 0.8956455129338283,
      "grad_norm": 4.484804153442383,
      "learning_rate": 6.714992927864215e-06,
      "loss": 0.5649,
      "step": 1900
    },
    {
      "epoch": 0.9192151316952448,
      "grad_norm": 4.727878093719482,
      "learning_rate": 6.891796322489393e-06,
      "loss": 0.5627,
      "step": 1950
    },
    {
      "epoch": 0.9427847504566613,
      "grad_norm": 5.438507556915283,
      "learning_rate": 7.068599717114569e-06,
      "loss": 0.5592,
      "step": 2000
    },
    {
      "epoch": 0.9663543692180779,
      "grad_norm": 4.886102199554443,
      "learning_rate": 7.245403111739746e-06,
      "loss": 0.5788,
      "step": 2050
    },
    {
      "epoch": 0.9899239879794944,
      "grad_norm": 5.976737022399902,
      "learning_rate": 7.4222065063649225e-06,
      "loss": 0.5752,
      "step": 2100
    },
    {
      "epoch": 1.0131989865063933,
      "grad_norm": 4.681585788726807,
      "learning_rate": 7.599009900990099e-06,
      "loss": 0.5132,
      "step": 2150
    },
    {
      "epoch": 1.0367686052678098,
      "grad_norm": 5.238368511199951,
      "learning_rate": 7.772277227722774e-06,
      "loss": 0.4998,
      "step": 2200
    },
    {
      "epoch": 1.0603382240292263,
      "grad_norm": 4.4695611000061035,
      "learning_rate": 7.94908062234795e-06,
      "loss": 0.5054,
      "step": 2250
    },
    {
      "epoch": 1.0839078427906428,
      "grad_norm": 4.674942970275879,
      "learning_rate": 8.125884016973127e-06,
      "loss": 0.5008,
      "step": 2300
    },
    {
      "epoch": 1.1074774615520595,
      "grad_norm": 4.342811584472656,
      "learning_rate": 8.302687411598304e-06,
      "loss": 0.5459,
      "step": 2350
    },
    {
      "epoch": 1.131047080313476,
      "grad_norm": 5.7018351554870605,
      "learning_rate": 8.479490806223481e-06,
      "loss": 0.5178,
      "step": 2400
    },
    {
      "epoch": 1.1546166990748925,
      "grad_norm": 4.290247917175293,
      "learning_rate": 8.656294200848658e-06,
      "loss": 0.5148,
      "step": 2450
    },
    {
      "epoch": 1.178186317836309,
      "grad_norm": 3.6935276985168457,
      "learning_rate": 8.833097595473834e-06,
      "loss": 0.4924,
      "step": 2500
    },
    {
      "epoch": 1.2017559365977255,
      "grad_norm": 4.729968547821045,
      "learning_rate": 9.009900990099011e-06,
      "loss": 0.4646,
      "step": 2550
    },
    {
      "epoch": 1.225325555359142,
      "grad_norm": 4.145240306854248,
      "learning_rate": 9.186704384724188e-06,
      "loss": 0.5348,
      "step": 2600
    },
    {
      "epoch": 1.2488951741205585,
      "grad_norm": 4.006193161010742,
      "learning_rate": 9.363507779349365e-06,
      "loss": 0.4793,
      "step": 2650
    },
    {
      "epoch": 1.272464792881975,
      "grad_norm": 3.805757522583008,
      "learning_rate": 9.54031117397454e-06,
      "loss": 0.4792,
      "step": 2700
    },
    {
      "epoch": 1.2960344116433917,
      "grad_norm": 3.37536358833313,
      "learning_rate": 9.717114568599718e-06,
      "loss": 0.4692,
      "step": 2750
    },
    {
      "epoch": 1.3196040304048082,
      "grad_norm": 5.088290214538574,
      "learning_rate": 9.893917963224895e-06,
      "loss": 0.5004,
      "step": 2800
    },
    {
      "epoch": 1.3431736491662247,
      "grad_norm": 4.413224220275879,
      "learning_rate": 1.0070721357850072e-05,
      "loss": 0.4681,
      "step": 2850
    },
    {
      "epoch": 1.3667432679276412,
      "grad_norm": 5.269515037536621,
      "learning_rate": 1.0247524752475248e-05,
      "loss": 0.4591,
      "step": 2900
    },
    {
      "epoch": 1.3903128866890577,
      "grad_norm": 3.4826574325561523,
      "learning_rate": 1.0424328147100425e-05,
      "loss": 0.4738,
      "step": 2950
    },
    {
      "epoch": 1.4138825054504744,
      "grad_norm": 4.169246673583984,
      "learning_rate": 1.0601131541725602e-05,
      "loss": 0.4764,
      "step": 3000
    },
    {
      "epoch": 1.437452124211891,
      "grad_norm": 4.009769916534424,
      "learning_rate": 1.077793493635078e-05,
      "loss": 0.4692,
      "step": 3050
    },
    {
      "epoch": 1.4610217429733074,
      "grad_norm": 5.061286926269531,
      "learning_rate": 1.0954738330975955e-05,
      "loss": 0.4973,
      "step": 3100
    },
    {
      "epoch": 1.484591361734724,
      "grad_norm": 3.202829360961914,
      "learning_rate": 1.1131541725601132e-05,
      "loss": 0.4545,
      "step": 3150
    },
    {
      "epoch": 1.5081609804961404,
      "grad_norm": 4.138211250305176,
      "learning_rate": 1.130834512022631e-05,
      "loss": 0.4554,
      "step": 3200
    },
    {
      "epoch": 1.531730599257557,
      "grad_norm": 3.535611629486084,
      "learning_rate": 1.1485148514851487e-05,
      "loss": 0.4662,
      "step": 3250
    },
    {
      "epoch": 1.5553002180189734,
      "grad_norm": 3.641118288040161,
      "learning_rate": 1.1661951909476664e-05,
      "loss": 0.48,
      "step": 3300
    },
    {
      "epoch": 1.57886983678039,
      "grad_norm": 4.2096171379089355,
      "learning_rate": 1.183875530410184e-05,
      "loss": 0.4565,
      "step": 3350
    },
    {
      "epoch": 1.6024394555418064,
      "grad_norm": 3.695600986480713,
      "learning_rate": 1.2015558698727017e-05,
      "loss": 0.4538,
      "step": 3400
    },
    {
      "epoch": 1.6260090743032232,
      "grad_norm": 5.128083229064941,
      "learning_rate": 1.2192362093352194e-05,
      "loss": 0.4377,
      "step": 3450
    },
    {
      "epoch": 1.6495786930646397,
      "grad_norm": 3.686063289642334,
      "learning_rate": 1.2369165487977371e-05,
      "loss": 0.4404,
      "step": 3500
    },
    {
      "epoch": 1.6731483118260562,
      "grad_norm": 4.741617679595947,
      "learning_rate": 1.2545968882602546e-05,
      "loss": 0.477,
      "step": 3550
    },
    {
      "epoch": 1.6967179305874729,
      "grad_norm": 3.227956771850586,
      "learning_rate": 1.2722772277227724e-05,
      "loss": 0.4317,
      "step": 3600
    },
    {
      "epoch": 1.7202875493488894,
      "grad_norm": 2.8432793617248535,
      "learning_rate": 1.28995756718529e-05,
      "loss": 0.466,
      "step": 3650
    },
    {
      "epoch": 1.7438571681103059,
      "grad_norm": 4.305752277374268,
      "learning_rate": 1.3076379066478078e-05,
      "loss": 0.4723,
      "step": 3700
    },
    {
      "epoch": 1.7674267868717224,
      "grad_norm": 4.8317108154296875,
      "learning_rate": 1.3253182461103254e-05,
      "loss": 0.4382,
      "step": 3750
    },
    {
      "epoch": 1.7909964056331389,
      "grad_norm": 2.9729740619659424,
      "learning_rate": 1.342998585572843e-05,
      "loss": 0.4363,
      "step": 3800
    },
    {
      "epoch": 1.8145660243945554,
      "grad_norm": 3.4812939167022705,
      "learning_rate": 1.3606789250353608e-05,
      "loss": 0.4322,
      "step": 3850
    },
    {
      "epoch": 1.8381356431559719,
      "grad_norm": 2.8482682704925537,
      "learning_rate": 1.3783592644978785e-05,
      "loss": 0.3963,
      "step": 3900
    },
    {
      "epoch": 1.8617052619173884,
      "grad_norm": 3.2451024055480957,
      "learning_rate": 1.396039603960396e-05,
      "loss": 0.4366,
      "step": 3950
    },
    {
      "epoch": 1.8852748806788049,
      "grad_norm": 4.676848411560059,
      "learning_rate": 1.4137199434229138e-05,
      "loss": 0.428,
      "step": 4000
    },
    {
      "epoch": 1.9088444994402216,
      "grad_norm": 3.6413230895996094,
      "learning_rate": 1.4314002828854315e-05,
      "loss": 0.4493,
      "step": 4050
    },
    {
      "epoch": 1.932414118201638,
      "grad_norm": 3.2940280437469482,
      "learning_rate": 1.4490806223479492e-05,
      "loss": 0.418,
      "step": 4100
    },
    {
      "epoch": 1.9559837369630546,
      "grad_norm": 4.053857326507568,
      "learning_rate": 1.466760961810467e-05,
      "loss": 0.4207,
      "step": 4150
    },
    {
      "epoch": 1.979553355724471,
      "grad_norm": 4.785057067871094,
      "learning_rate": 1.4844413012729845e-05,
      "loss": 0.4224,
      "step": 4200
    },
    {
      "epoch": 2.00282835425137,
      "grad_norm": 4.361907958984375,
      "learning_rate": 1.502121640735502e-05,
      "loss": 0.4176,
      "step": 4250
    },
    {
      "epoch": 2.0263979730127866,
      "grad_norm": 3.401597738265991,
      "learning_rate": 1.5198019801980198e-05,
      "loss": 0.3726,
      "step": 4300
    },
    {
      "epoch": 2.049967591774203,
      "grad_norm": 4.086523532867432,
      "learning_rate": 1.5374823196605373e-05,
      "loss": 0.3755,
      "step": 4350
    },
    {
      "epoch": 2.0735372105356196,
      "grad_norm": 4.3385491371154785,
      "learning_rate": 1.5551626591230552e-05,
      "loss": 0.3727,
      "step": 4400
    },
    {
      "epoch": 2.097106829297036,
      "grad_norm": 3.96553111076355,
      "learning_rate": 1.5728429985855728e-05,
      "loss": 0.3739,
      "step": 4450
    },
    {
      "epoch": 2.1206764480584526,
      "grad_norm": 2.8655247688293457,
      "learning_rate": 1.5905233380480903e-05,
      "loss": 0.3869,
      "step": 4500
    },
    {
      "epoch": 2.144246066819869,
      "grad_norm": 5.081576824188232,
      "learning_rate": 1.6082036775106082e-05,
      "loss": 0.3786,
      "step": 4550
    },
    {
      "epoch": 2.1678156855812856,
      "grad_norm": 4.800468921661377,
      "learning_rate": 1.6258840169731258e-05,
      "loss": 0.3635,
      "step": 4600
    },
    {
      "epoch": 2.191385304342702,
      "grad_norm": 3.789586067199707,
      "learning_rate": 1.6435643564356436e-05,
      "loss": 0.3821,
      "step": 4650
    },
    {
      "epoch": 2.214954923104119,
      "grad_norm": 4.0591607093811035,
      "learning_rate": 1.6612446958981612e-05,
      "loss": 0.3623,
      "step": 4700
    },
    {
      "epoch": 2.2385245418655355,
      "grad_norm": 4.645422458648682,
      "learning_rate": 1.6789250353606787e-05,
      "loss": 0.3893,
      "step": 4750
    },
    {
      "epoch": 2.262094160626952,
      "grad_norm": 3.7258005142211914,
      "learning_rate": 1.6966053748231966e-05,
      "loss": 0.3666,
      "step": 4800
    },
    {
      "epoch": 2.2856637793883685,
      "grad_norm": 3.4569387435913086,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 0.3574,
      "step": 4850
    },
    {
      "epoch": 2.309233398149785,
      "grad_norm": 3.5347609519958496,
      "learning_rate": 1.731966053748232e-05,
      "loss": 0.3805,
      "step": 4900
    },
    {
      "epoch": 2.3328030169112015,
      "grad_norm": 3.7994868755340576,
      "learning_rate": 1.7496463932107496e-05,
      "loss": 0.3558,
      "step": 4950
    },
    {
      "epoch": 2.356372635672618,
      "grad_norm": 5.330169677734375,
      "learning_rate": 1.7673267326732672e-05,
      "loss": 0.3581,
      "step": 5000
    },
    {
      "epoch": 2.356372635672618,
      "eval_loss": 0.38101187348365784,
      "eval_runtime": 165.2878,
      "eval_samples_per_second": 176.02,
      "eval_steps_per_second": 44.008,
      "step": 5000
    },
    {
      "epoch": 2.3799422544340345,
      "grad_norm": 4.15966272354126,
      "learning_rate": 1.785007072135785e-05,
      "loss": 0.3593,
      "step": 5050
    },
    {
      "epoch": 2.403511873195451,
      "grad_norm": 3.470900297164917,
      "learning_rate": 1.8026874115983026e-05,
      "loss": 0.3533,
      "step": 5100
    },
    {
      "epoch": 2.4270814919568675,
      "grad_norm": 3.9466302394866943,
      "learning_rate": 1.82036775106082e-05,
      "loss": 0.3772,
      "step": 5150
    },
    {
      "epoch": 2.450651110718284,
      "grad_norm": 2.460118532180786,
      "learning_rate": 1.838048090523338e-05,
      "loss": 0.3567,
      "step": 5200
    },
    {
      "epoch": 2.4742207294797005,
      "grad_norm": 4.443327903747559,
      "learning_rate": 1.8557284299858556e-05,
      "loss": 0.3607,
      "step": 5250
    },
    {
      "epoch": 2.497790348241117,
      "grad_norm": 3.8630528450012207,
      "learning_rate": 1.8734087694483735e-05,
      "loss": 0.3561,
      "step": 5300
    },
    {
      "epoch": 2.5213599670025335,
      "grad_norm": 3.4382431507110596,
      "learning_rate": 1.891089108910891e-05,
      "loss": 0.3483,
      "step": 5350
    },
    {
      "epoch": 2.54492958576395,
      "grad_norm": 3.6629128456115723,
      "learning_rate": 1.9087694483734086e-05,
      "loss": 0.3479,
      "step": 5400
    },
    {
      "epoch": 2.568499204525367,
      "grad_norm": 3.883563995361328,
      "learning_rate": 1.9260961810466763e-05,
      "loss": 0.3706,
      "step": 5450
    },
    {
      "epoch": 2.5920688232867835,
      "grad_norm": 3.588174343109131,
      "learning_rate": 1.9437765205091938e-05,
      "loss": 0.367,
      "step": 5500
    },
    {
      "epoch": 2.6156384420482,
      "grad_norm": 4.544034004211426,
      "learning_rate": 1.9614568599717114e-05,
      "loss": 0.3641,
      "step": 5550
    },
    {
      "epoch": 2.6392080608096165,
      "grad_norm": 5.034407138824463,
      "learning_rate": 1.9791371994342293e-05,
      "loss": 0.3743,
      "step": 5600
    },
    {
      "epoch": 2.662777679571033,
      "grad_norm": 4.2291083335876465,
      "learning_rate": 1.9968175388967468e-05,
      "loss": 0.3193,
      "step": 5650
    },
    {
      "epoch": 2.6863472983324495,
      "grad_norm": 4.069446563720703,
      "learning_rate": 2.0144978783592647e-05,
      "loss": 0.3446,
      "step": 5700
    },
    {
      "epoch": 2.709916917093866,
      "grad_norm": 2.980616331100464,
      "learning_rate": 2.0321782178217822e-05,
      "loss": 0.3483,
      "step": 5750
    },
    {
      "epoch": 2.7334865358552825,
      "grad_norm": 3.5084228515625,
      "learning_rate": 2.0498585572842998e-05,
      "loss": 0.3399,
      "step": 5800
    },
    {
      "epoch": 2.757056154616699,
      "grad_norm": 4.056827545166016,
      "learning_rate": 2.0675388967468177e-05,
      "loss": 0.3521,
      "step": 5850
    },
    {
      "epoch": 2.7806257733781155,
      "grad_norm": 5.799953460693359,
      "learning_rate": 2.0852192362093352e-05,
      "loss": 0.3658,
      "step": 5900
    },
    {
      "epoch": 2.8041953921395324,
      "grad_norm": 4.806807041168213,
      "learning_rate": 2.102899575671853e-05,
      "loss": 0.3745,
      "step": 5950
    },
    {
      "epoch": 2.827765010900949,
      "grad_norm": 4.770646095275879,
      "learning_rate": 2.1205799151343707e-05,
      "loss": 0.3367,
      "step": 6000
    },
    {
      "epoch": 2.8513346296623654,
      "grad_norm": 4.056053638458252,
      "learning_rate": 2.1382602545968882e-05,
      "loss": 0.3489,
      "step": 6050
    },
    {
      "epoch": 2.874904248423782,
      "grad_norm": 4.585602283477783,
      "learning_rate": 2.155940594059406e-05,
      "loss": 0.3378,
      "step": 6100
    },
    {
      "epoch": 2.8984738671851984,
      "grad_norm": 5.933922290802002,
      "learning_rate": 2.1736209335219237e-05,
      "loss": 0.3479,
      "step": 6150
    },
    {
      "epoch": 2.922043485946615,
      "grad_norm": 5.471593856811523,
      "learning_rate": 2.1913012729844412e-05,
      "loss": 0.3227,
      "step": 6200
    },
    {
      "epoch": 2.9456131047080314,
      "grad_norm": 3.9926717281341553,
      "learning_rate": 2.208981612446959e-05,
      "loss": 0.3489,
      "step": 6250
    },
    {
      "epoch": 2.969182723469448,
      "grad_norm": 3.694948673248291,
      "learning_rate": 2.2266619519094767e-05,
      "loss": 0.3495,
      "step": 6300
    },
    {
      "epoch": 2.9927523422308644,
      "grad_norm": 2.6297664642333984,
      "learning_rate": 2.2443422913719946e-05,
      "loss": 0.3427,
      "step": 6350
    },
    {
      "epoch": 3.016027340757763,
      "grad_norm": 3.480004072189331,
      "learning_rate": 2.262022630834512e-05,
      "loss": 0.3009,
      "step": 6400
    },
    {
      "epoch": 3.0395969595191796,
      "grad_norm": 2.481553077697754,
      "learning_rate": 2.2797029702970297e-05,
      "loss": 0.2708,
      "step": 6450
    },
    {
      "epoch": 3.063166578280596,
      "grad_norm": 4.118981838226318,
      "learning_rate": 2.2973833097595475e-05,
      "loss": 0.2842,
      "step": 6500
    },
    {
      "epoch": 3.0867361970420126,
      "grad_norm": 3.2646939754486084,
      "learning_rate": 2.315063649222065e-05,
      "loss": 0.2892,
      "step": 6550
    },
    {
      "epoch": 3.1103058158034296,
      "grad_norm": 3.49711537361145,
      "learning_rate": 2.332743988684583e-05,
      "loss": 0.2986,
      "step": 6600
    },
    {
      "epoch": 3.133875434564846,
      "grad_norm": 2.7463555335998535,
      "learning_rate": 2.3504243281471005e-05,
      "loss": 0.2923,
      "step": 6650
    },
    {
      "epoch": 3.1574450533262626,
      "grad_norm": 2.8316171169281006,
      "learning_rate": 2.368104667609618e-05,
      "loss": 0.3128,
      "step": 6700
    },
    {
      "epoch": 3.181014672087679,
      "grad_norm": 2.4142582416534424,
      "learning_rate": 2.385785007072136e-05,
      "loss": 0.2767,
      "step": 6750
    },
    {
      "epoch": 3.2045842908490956,
      "grad_norm": 3.185906171798706,
      "learning_rate": 2.4034653465346535e-05,
      "loss": 0.2876,
      "step": 6800
    },
    {
      "epoch": 3.228153909610512,
      "grad_norm": 3.0908708572387695,
      "learning_rate": 2.421145685997171e-05,
      "loss": 0.2937,
      "step": 6850
    },
    {
      "epoch": 3.2517235283719286,
      "grad_norm": 3.016706943511963,
      "learning_rate": 2.438826025459689e-05,
      "loss": 0.285,
      "step": 6900
    },
    {
      "epoch": 3.275293147133345,
      "grad_norm": 4.290531635284424,
      "learning_rate": 2.4565063649222065e-05,
      "loss": 0.2985,
      "step": 6950
    },
    {
      "epoch": 3.2988627658947616,
      "grad_norm": 3.500737428665161,
      "learning_rate": 2.4741867043847244e-05,
      "loss": 0.2803,
      "step": 7000
    },
    {
      "epoch": 3.322432384656178,
      "grad_norm": 3.631087303161621,
      "learning_rate": 2.491867043847242e-05,
      "loss": 0.2827,
      "step": 7050
    },
    {
      "epoch": 3.3460020034175946,
      "grad_norm": 3.1413042545318604,
      "learning_rate": 2.5095473833097595e-05,
      "loss": 0.2865,
      "step": 7100
    },
    {
      "epoch": 3.369571622179011,
      "grad_norm": 4.223089694976807,
      "learning_rate": 2.5268741159830272e-05,
      "loss": 0.2835,
      "step": 7150
    },
    {
      "epoch": 3.3931412409404276,
      "grad_norm": 4.63199520111084,
      "learning_rate": 2.5445544554455447e-05,
      "loss": 0.3141,
      "step": 7200
    },
    {
      "epoch": 3.4167108597018445,
      "grad_norm": 3.8155627250671387,
      "learning_rate": 2.5622347949080626e-05,
      "loss": 0.2981,
      "step": 7250
    },
    {
      "epoch": 3.440280478463261,
      "grad_norm": 3.441527843475342,
      "learning_rate": 2.57991513437058e-05,
      "loss": 0.3084,
      "step": 7300
    },
    {
      "epoch": 3.4638500972246775,
      "grad_norm": 3.4484152793884277,
      "learning_rate": 2.5975954738330977e-05,
      "loss": 0.3055,
      "step": 7350
    },
    {
      "epoch": 3.487419715986094,
      "grad_norm": 2.497840404510498,
      "learning_rate": 2.6152758132956156e-05,
      "loss": 0.2861,
      "step": 7400
    },
    {
      "epoch": 3.5109893347475105,
      "grad_norm": 3.155686616897583,
      "learning_rate": 2.632956152758133e-05,
      "loss": 0.3112,
      "step": 7450
    },
    {
      "epoch": 3.534558953508927,
      "grad_norm": 3.4210684299468994,
      "learning_rate": 2.6506364922206507e-05,
      "loss": 0.3111,
      "step": 7500
    },
    {
      "epoch": 3.5581285722703435,
      "grad_norm": 3.2729849815368652,
      "learning_rate": 2.6683168316831686e-05,
      "loss": 0.2748,
      "step": 7550
    },
    {
      "epoch": 3.58169819103176,
      "grad_norm": 4.15079402923584,
      "learning_rate": 2.685997171145686e-05,
      "loss": 0.309,
      "step": 7600
    },
    {
      "epoch": 3.6052678097931765,
      "grad_norm": 3.1439874172210693,
      "learning_rate": 2.703677510608204e-05,
      "loss": 0.3024,
      "step": 7650
    },
    {
      "epoch": 3.628837428554593,
      "grad_norm": 3.3943357467651367,
      "learning_rate": 2.7213578500707216e-05,
      "loss": 0.2934,
      "step": 7700
    },
    {
      "epoch": 3.6524070473160095,
      "grad_norm": 4.424273490905762,
      "learning_rate": 2.739038189533239e-05,
      "loss": 0.319,
      "step": 7750
    },
    {
      "epoch": 3.6759766660774265,
      "grad_norm": 2.571603298187256,
      "learning_rate": 2.756718528995757e-05,
      "loss": 0.3055,
      "step": 7800
    },
    {
      "epoch": 3.699546284838843,
      "grad_norm": 2.3481009006500244,
      "learning_rate": 2.7743988684582746e-05,
      "loss": 0.2998,
      "step": 7850
    },
    {
      "epoch": 3.7231159036002595,
      "grad_norm": 3.7417733669281006,
      "learning_rate": 2.792079207920792e-05,
      "loss": 0.2749,
      "step": 7900
    },
    {
      "epoch": 3.746685522361676,
      "grad_norm": 2.5440399646759033,
      "learning_rate": 2.80975954738331e-05,
      "loss": 0.2815,
      "step": 7950
    },
    {
      "epoch": 3.7702551411230925,
      "grad_norm": 3.00195050239563,
      "learning_rate": 2.8274398868458276e-05,
      "loss": 0.2789,
      "step": 8000
    },
    {
      "epoch": 3.793824759884509,
      "grad_norm": 3.244292974472046,
      "learning_rate": 2.8451202263083455e-05,
      "loss": 0.2914,
      "step": 8050
    },
    {
      "epoch": 3.8173943786459255,
      "grad_norm": 3.5002126693725586,
      "learning_rate": 2.862800565770863e-05,
      "loss": 0.2823,
      "step": 8100
    },
    {
      "epoch": 3.840963997407342,
      "grad_norm": 3.344986915588379,
      "learning_rate": 2.8804809052333806e-05,
      "loss": 0.2912,
      "step": 8150
    },
    {
      "epoch": 3.8645336161687585,
      "grad_norm": 4.4168925285339355,
      "learning_rate": 2.8981612446958985e-05,
      "loss": 0.2914,
      "step": 8200
    },
    {
      "epoch": 3.888103234930175,
      "grad_norm": 2.9447672367095947,
      "learning_rate": 2.915841584158416e-05,
      "loss": 0.2935,
      "step": 8250
    },
    {
      "epoch": 3.9116728536915915,
      "grad_norm": 2.981882095336914,
      "learning_rate": 2.933521923620934e-05,
      "loss": 0.2942,
      "step": 8300
    },
    {
      "epoch": 3.935242472453008,
      "grad_norm": 3.207955837249756,
      "learning_rate": 2.9512022630834514e-05,
      "loss": 0.2967,
      "step": 8350
    },
    {
      "epoch": 3.9588120912144245,
      "grad_norm": 2.3842787742614746,
      "learning_rate": 2.968882602545969e-05,
      "loss": 0.2781,
      "step": 8400
    },
    {
      "epoch": 3.982381709975841,
      "grad_norm": 3.8424572944641113,
      "learning_rate": 2.986562942008487e-05,
      "loss": 0.2784,
      "step": 8450
    },
    {
      "epoch": 4.00565670850274,
      "grad_norm": 2.92303466796875,
      "learning_rate": 2.9995285242809997e-05,
      "loss": 0.2653,
      "step": 8500
    },
    {
      "epoch": 4.029226327264157,
      "grad_norm": 4.13413667678833,
      "learning_rate": 2.9975640421184976e-05,
      "loss": 0.2385,
      "step": 8550
    },
    {
      "epoch": 4.052795946025573,
      "grad_norm": 4.494542121887207,
      "learning_rate": 2.9955995599559958e-05,
      "loss": 0.2368,
      "step": 8600
    },
    {
      "epoch": 4.07636556478699,
      "grad_norm": 3.768954038619995,
      "learning_rate": 2.9936350777934937e-05,
      "loss": 0.2452,
      "step": 8650
    },
    {
      "epoch": 4.099935183548406,
      "grad_norm": 3.6611618995666504,
      "learning_rate": 2.9916705956309916e-05,
      "loss": 0.2223,
      "step": 8700
    },
    {
      "epoch": 4.123504802309823,
      "grad_norm": 3.355914831161499,
      "learning_rate": 2.9897061134684895e-05,
      "loss": 0.2416,
      "step": 8750
    },
    {
      "epoch": 4.147074421071239,
      "grad_norm": 3.712444543838501,
      "learning_rate": 2.9877416313059877e-05,
      "loss": 0.2271,
      "step": 8800
    },
    {
      "epoch": 4.170644039832656,
      "grad_norm": 2.856045722961426,
      "learning_rate": 2.985777149143486e-05,
      "loss": 0.2486,
      "step": 8850
    },
    {
      "epoch": 4.194213658594072,
      "grad_norm": 2.4799885749816895,
      "learning_rate": 2.983812666980984e-05,
      "loss": 0.2387,
      "step": 8900
    },
    {
      "epoch": 4.217783277355489,
      "grad_norm": 2.6248860359191895,
      "learning_rate": 2.981848184818482e-05,
      "loss": 0.2489,
      "step": 8950
    },
    {
      "epoch": 4.241352896116905,
      "grad_norm": 3.3394412994384766,
      "learning_rate": 2.97988370265598e-05,
      "loss": 0.2488,
      "step": 9000
    },
    {
      "epoch": 4.264922514878322,
      "grad_norm": 2.7860069274902344,
      "learning_rate": 2.977919220493478e-05,
      "loss": 0.2317,
      "step": 9050
    },
    {
      "epoch": 4.288492133639738,
      "grad_norm": 3.5021824836730957,
      "learning_rate": 2.9759547383309758e-05,
      "loss": 0.2287,
      "step": 9100
    },
    {
      "epoch": 4.312061752401155,
      "grad_norm": 4.291834831237793,
      "learning_rate": 2.973990256168474e-05,
      "loss": 0.2395,
      "step": 9150
    },
    {
      "epoch": 4.335631371162571,
      "grad_norm": 2.3625895977020264,
      "learning_rate": 2.972025774005972e-05,
      "loss": 0.2405,
      "step": 9200
    },
    {
      "epoch": 4.359200989923988,
      "grad_norm": 3.173111915588379,
      "learning_rate": 2.97006129184347e-05,
      "loss": 0.2455,
      "step": 9250
    },
    {
      "epoch": 4.382770608685404,
      "grad_norm": 4.004364013671875,
      "learning_rate": 2.968136099324218e-05,
      "loss": 0.2494,
      "step": 9300
    },
    {
      "epoch": 4.406340227446821,
      "grad_norm": 4.0030927658081055,
      "learning_rate": 2.966171617161716e-05,
      "loss": 0.2342,
      "step": 9350
    },
    {
      "epoch": 4.429909846208238,
      "grad_norm": 5.088149070739746,
      "learning_rate": 2.9642071349992143e-05,
      "loss": 0.2347,
      "step": 9400
    },
    {
      "epoch": 4.4534794649696545,
      "grad_norm": 3.5823159217834473,
      "learning_rate": 2.9622426528367125e-05,
      "loss": 0.2383,
      "step": 9450
    },
    {
      "epoch": 4.477049083731071,
      "grad_norm": 3.8327083587646484,
      "learning_rate": 2.9602781706742104e-05,
      "loss": 0.2462,
      "step": 9500
    },
    {
      "epoch": 4.5006187024924875,
      "grad_norm": 3.765707015991211,
      "learning_rate": 2.9583136885117083e-05,
      "loss": 0.2389,
      "step": 9550
    },
    {
      "epoch": 4.524188321253904,
      "grad_norm": 2.959423542022705,
      "learning_rate": 2.9563492063492066e-05,
      "loss": 0.2214,
      "step": 9600
    },
    {
      "epoch": 4.5477579400153205,
      "grad_norm": 2.818901777267456,
      "learning_rate": 2.9543847241867045e-05,
      "loss": 0.2413,
      "step": 9650
    },
    {
      "epoch": 4.571327558776737,
      "grad_norm": 3.1647636890411377,
      "learning_rate": 2.9524202420242024e-05,
      "loss": 0.2459,
      "step": 9700
    },
    {
      "epoch": 4.5948971775381535,
      "grad_norm": 2.6232709884643555,
      "learning_rate": 2.9504557598617006e-05,
      "loss": 0.243,
      "step": 9750
    },
    {
      "epoch": 4.61846679629957,
      "grad_norm": 3.1976993083953857,
      "learning_rate": 2.9484912776991985e-05,
      "loss": 0.2397,
      "step": 9800
    },
    {
      "epoch": 4.6420364150609865,
      "grad_norm": 2.623544216156006,
      "learning_rate": 2.9465267955366964e-05,
      "loss": 0.2383,
      "step": 9850
    },
    {
      "epoch": 4.665606033822403,
      "grad_norm": 2.8326799869537354,
      "learning_rate": 2.9445623133741946e-05,
      "loss": 0.2282,
      "step": 9900
    },
    {
      "epoch": 4.6891756525838195,
      "grad_norm": 4.0653204917907715,
      "learning_rate": 2.942597831211693e-05,
      "loss": 0.2447,
      "step": 9950
    },
    {
      "epoch": 4.712745271345236,
      "grad_norm": 4.03955602645874,
      "learning_rate": 2.9406333490491908e-05,
      "loss": 0.2463,
      "step": 10000
    },
    {
      "epoch": 4.712745271345236,
      "eval_loss": 0.30165910720825195,
      "eval_runtime": 168.5164,
      "eval_samples_per_second": 172.648,
      "eval_steps_per_second": 43.165,
      "step": 10000
    },
    {
      "epoch": 4.7363148901066525,
      "grad_norm": 3.9494588375091553,
      "learning_rate": 2.9386688668866887e-05,
      "loss": 0.2327,
      "step": 10050
    },
    {
      "epoch": 4.759884508868069,
      "grad_norm": 3.2487077713012695,
      "learning_rate": 2.936704384724187e-05,
      "loss": 0.238,
      "step": 10100
    },
    {
      "epoch": 4.7834541276294855,
      "grad_norm": 3.521289825439453,
      "learning_rate": 2.9347399025616848e-05,
      "loss": 0.2396,
      "step": 10150
    },
    {
      "epoch": 4.807023746390902,
      "grad_norm": 3.086062431335449,
      "learning_rate": 2.9327754203991827e-05,
      "loss": 0.2296,
      "step": 10200
    },
    {
      "epoch": 4.8305933651523185,
      "grad_norm": 3.028679847717285,
      "learning_rate": 2.930810938236681e-05,
      "loss": 0.2278,
      "step": 10250
    },
    {
      "epoch": 4.854162983913735,
      "grad_norm": 3.46563982963562,
      "learning_rate": 2.928846456074179e-05,
      "loss": 0.2464,
      "step": 10300
    },
    {
      "epoch": 4.8777326026751515,
      "grad_norm": 3.8337485790252686,
      "learning_rate": 2.926881973911677e-05,
      "loss": 0.25,
      "step": 10350
    },
    {
      "epoch": 4.901302221436568,
      "grad_norm": 3.0609731674194336,
      "learning_rate": 2.924917491749175e-05,
      "loss": 0.2474,
      "step": 10400
    },
    {
      "epoch": 4.9248718401979845,
      "grad_norm": 4.6044440269470215,
      "learning_rate": 2.9229530095866732e-05,
      "loss": 0.2504,
      "step": 10450
    },
    {
      "epoch": 4.948441458959401,
      "grad_norm": 3.8612892627716064,
      "learning_rate": 2.920988527424171e-05,
      "loss": 0.2321,
      "step": 10500
    },
    {
      "epoch": 4.9720110777208175,
      "grad_norm": 2.3632030487060547,
      "learning_rate": 2.919024045261669e-05,
      "loss": 0.2232,
      "step": 10550
    },
    {
      "epoch": 4.995580696482234,
      "grad_norm": 4.555328845977783,
      "learning_rate": 2.9170595630991673e-05,
      "loss": 0.2369,
      "step": 10600
    },
    {
      "epoch": 5.018855695009133,
      "grad_norm": 3.0673933029174805,
      "learning_rate": 2.915095080936665e-05,
      "loss": 0.184,
      "step": 10650
    },
    {
      "epoch": 5.04242531377055,
      "grad_norm": 3.3205103874206543,
      "learning_rate": 2.913130598774163e-05,
      "loss": 0.1739,
      "step": 10700
    },
    {
      "epoch": 5.065994932531966,
      "grad_norm": 7.261421203613281,
      "learning_rate": 2.911166116611661e-05,
      "loss": 0.1967,
      "step": 10750
    },
    {
      "epoch": 5.089564551293383,
      "grad_norm": 2.4686481952667236,
      "learning_rate": 2.9092016344491595e-05,
      "loss": 0.1906,
      "step": 10800
    },
    {
      "epoch": 5.113134170054799,
      "grad_norm": 2.8799757957458496,
      "learning_rate": 2.9072371522866574e-05,
      "loss": 0.1933,
      "step": 10850
    },
    {
      "epoch": 5.136703788816216,
      "grad_norm": 3.8539233207702637,
      "learning_rate": 2.9052726701241553e-05,
      "loss": 0.1789,
      "step": 10900
    },
    {
      "epoch": 5.160273407577632,
      "grad_norm": 4.108344078063965,
      "learning_rate": 2.9033081879616536e-05,
      "loss": 0.2057,
      "step": 10950
    },
    {
      "epoch": 5.183843026339049,
      "grad_norm": 3.314664125442505,
      "learning_rate": 2.9013437057991515e-05,
      "loss": 0.1882,
      "step": 11000
    },
    {
      "epoch": 5.207412645100465,
      "grad_norm": 3.2201504707336426,
      "learning_rate": 2.8993792236366494e-05,
      "loss": 0.2008,
      "step": 11050
    },
    {
      "epoch": 5.230982263861882,
      "grad_norm": 2.7837932109832764,
      "learning_rate": 2.8974147414741473e-05,
      "loss": 0.19,
      "step": 11100
    },
    {
      "epoch": 5.254551882623298,
      "grad_norm": 2.8161048889160156,
      "learning_rate": 2.8954502593116455e-05,
      "loss": 0.1863,
      "step": 11150
    },
    {
      "epoch": 5.278121501384715,
      "grad_norm": 2.5448899269104004,
      "learning_rate": 2.8934857771491434e-05,
      "loss": 0.1889,
      "step": 11200
    },
    {
      "epoch": 5.301691120146131,
      "grad_norm": 2.5579965114593506,
      "learning_rate": 2.8915212949866416e-05,
      "loss": 0.198,
      "step": 11250
    },
    {
      "epoch": 5.325260738907549,
      "grad_norm": 5.833276271820068,
      "learning_rate": 2.88955681282414e-05,
      "loss": 0.2059,
      "step": 11300
    },
    {
      "epoch": 5.348830357668965,
      "grad_norm": 3.1019608974456787,
      "learning_rate": 2.8875923306616378e-05,
      "loss": 0.2019,
      "step": 11350
    },
    {
      "epoch": 5.372399976430382,
      "grad_norm": 2.408252477645874,
      "learning_rate": 2.8856278484991357e-05,
      "loss": 0.1873,
      "step": 11400
    },
    {
      "epoch": 5.395969595191798,
      "grad_norm": 3.775036573410034,
      "learning_rate": 2.8836633663366336e-05,
      "loss": 0.1915,
      "step": 11450
    },
    {
      "epoch": 5.419539213953215,
      "grad_norm": 3.8000311851501465,
      "learning_rate": 2.8816988841741318e-05,
      "loss": 0.1956,
      "step": 11500
    },
    {
      "epoch": 5.443108832714631,
      "grad_norm": 3.4552268981933594,
      "learning_rate": 2.8797344020116297e-05,
      "loss": 0.192,
      "step": 11550
    },
    {
      "epoch": 5.466678451476048,
      "grad_norm": 3.100661516189575,
      "learning_rate": 2.8777699198491276e-05,
      "loss": 0.1964,
      "step": 11600
    },
    {
      "epoch": 5.490248070237464,
      "grad_norm": 3.831697463989258,
      "learning_rate": 2.875844727329876e-05,
      "loss": 0.1974,
      "step": 11650
    },
    {
      "epoch": 5.513817688998881,
      "grad_norm": 2.9355735778808594,
      "learning_rate": 2.8738802451673738e-05,
      "loss": 0.2037,
      "step": 11700
    },
    {
      "epoch": 5.537387307760297,
      "grad_norm": 2.812661647796631,
      "learning_rate": 2.871915763004872e-05,
      "loss": 0.1865,
      "step": 11750
    },
    {
      "epoch": 5.560956926521714,
      "grad_norm": 2.606696844100952,
      "learning_rate": 2.86995128084237e-05,
      "loss": 0.2091,
      "step": 11800
    },
    {
      "epoch": 5.58452654528313,
      "grad_norm": 2.3960041999816895,
      "learning_rate": 2.867986798679868e-05,
      "loss": 0.1941,
      "step": 11850
    },
    {
      "epoch": 5.608096164044547,
      "grad_norm": 2.2890071868896484,
      "learning_rate": 2.866022316517366e-05,
      "loss": 0.2028,
      "step": 11900
    },
    {
      "epoch": 5.631665782805963,
      "grad_norm": 3.0259833335876465,
      "learning_rate": 2.8640578343548643e-05,
      "loss": 0.1813,
      "step": 11950
    },
    {
      "epoch": 5.65523540156738,
      "grad_norm": 3.08101487159729,
      "learning_rate": 2.8620933521923622e-05,
      "loss": 0.1894,
      "step": 12000
    },
    {
      "epoch": 5.678805020328796,
      "grad_norm": 2.6536788940429688,
      "learning_rate": 2.86012887002986e-05,
      "loss": 0.1892,
      "step": 12050
    },
    {
      "epoch": 5.702374639090213,
      "grad_norm": 3.638169765472412,
      "learning_rate": 2.8581643878673584e-05,
      "loss": 0.195,
      "step": 12100
    },
    {
      "epoch": 5.725944257851629,
      "grad_norm": 3.698392391204834,
      "learning_rate": 2.8561999057048563e-05,
      "loss": 0.1961,
      "step": 12150
    },
    {
      "epoch": 5.749513876613046,
      "grad_norm": 2.60490083694458,
      "learning_rate": 2.854235423542354e-05,
      "loss": 0.2002,
      "step": 12200
    },
    {
      "epoch": 5.773083495374462,
      "grad_norm": 4.744781970977783,
      "learning_rate": 2.852270941379852e-05,
      "loss": 0.2051,
      "step": 12250
    },
    {
      "epoch": 5.796653114135879,
      "grad_norm": 2.9910178184509277,
      "learning_rate": 2.8503064592173503e-05,
      "loss": 0.2006,
      "step": 12300
    },
    {
      "epoch": 5.820222732897295,
      "grad_norm": 3.7866032123565674,
      "learning_rate": 2.8483419770548485e-05,
      "loss": 0.1903,
      "step": 12350
    },
    {
      "epoch": 5.843792351658712,
      "grad_norm": 3.593319892883301,
      "learning_rate": 2.8463774948923464e-05,
      "loss": 0.1931,
      "step": 12400
    },
    {
      "epoch": 5.867361970420128,
      "grad_norm": 3.194129228591919,
      "learning_rate": 2.8444130127298447e-05,
      "loss": 0.2179,
      "step": 12450
    },
    {
      "epoch": 5.890931589181545,
      "grad_norm": 3.331702709197998,
      "learning_rate": 2.8424485305673426e-05,
      "loss": 0.2069,
      "step": 12500
    },
    {
      "epoch": 5.914501207942962,
      "grad_norm": 1.921513319015503,
      "learning_rate": 2.8404840484048405e-05,
      "loss": 0.1866,
      "step": 12550
    },
    {
      "epoch": 5.938070826704378,
      "grad_norm": 2.8236377239227295,
      "learning_rate": 2.8385195662423387e-05,
      "loss": 0.1918,
      "step": 12600
    },
    {
      "epoch": 5.961640445465795,
      "grad_norm": 3.2569408416748047,
      "learning_rate": 2.8365550840798366e-05,
      "loss": 0.199,
      "step": 12650
    },
    {
      "epoch": 5.9852100642272115,
      "grad_norm": 3.042013645172119,
      "learning_rate": 2.8345906019173345e-05,
      "loss": 0.1897,
      "step": 12700
    },
    {
      "epoch": 6.00848506275411,
      "grad_norm": 2.8446855545043945,
      "learning_rate": 2.8326261197548324e-05,
      "loss": 0.1862,
      "step": 12750
    },
    {
      "epoch": 6.032054681515526,
      "grad_norm": 2.5531270503997803,
      "learning_rate": 2.830661637592331e-05,
      "loss": 0.1446,
      "step": 12800
    },
    {
      "epoch": 6.055624300276943,
      "grad_norm": 2.201847553253174,
      "learning_rate": 2.828697155429829e-05,
      "loss": 0.146,
      "step": 12850
    },
    {
      "epoch": 6.079193919038359,
      "grad_norm": 2.593505859375,
      "learning_rate": 2.8267326732673268e-05,
      "loss": 0.1596,
      "step": 12900
    },
    {
      "epoch": 6.102763537799776,
      "grad_norm": 3.4067370891571045,
      "learning_rate": 2.824768191104825e-05,
      "loss": 0.1402,
      "step": 12950
    },
    {
      "epoch": 6.126333156561192,
      "grad_norm": 1.9943350553512573,
      "learning_rate": 2.822803708942323e-05,
      "loss": 0.1553,
      "step": 13000
    },
    {
      "epoch": 6.149902775322609,
      "grad_norm": 3.713719606399536,
      "learning_rate": 2.8208392267798208e-05,
      "loss": 0.1515,
      "step": 13050
    },
    {
      "epoch": 6.173472394084025,
      "grad_norm": 2.179511308670044,
      "learning_rate": 2.8188747446173187e-05,
      "loss": 0.1712,
      "step": 13100
    },
    {
      "epoch": 6.197042012845443,
      "grad_norm": 3.6973299980163574,
      "learning_rate": 2.816910262454817e-05,
      "loss": 0.1667,
      "step": 13150
    },
    {
      "epoch": 6.220611631606859,
      "grad_norm": 4.076506614685059,
      "learning_rate": 2.814945780292315e-05,
      "loss": 0.1702,
      "step": 13200
    },
    {
      "epoch": 6.244181250368276,
      "grad_norm": 2.7563223838806152,
      "learning_rate": 2.812981298129813e-05,
      "loss": 0.1718,
      "step": 13250
    },
    {
      "epoch": 6.267750869129692,
      "grad_norm": 2.717287540435791,
      "learning_rate": 2.8110168159673113e-05,
      "loss": 0.1762,
      "step": 13300
    },
    {
      "epoch": 6.291320487891109,
      "grad_norm": 3.0161306858062744,
      "learning_rate": 2.8090523338048092e-05,
      "loss": 0.1693,
      "step": 13350
    },
    {
      "epoch": 6.314890106652525,
      "grad_norm": 1.4371893405914307,
      "learning_rate": 2.807087851642307e-05,
      "loss": 0.1539,
      "step": 13400
    },
    {
      "epoch": 6.338459725413942,
      "grad_norm": 3.3179879188537598,
      "learning_rate": 2.805123369479805e-05,
      "loss": 0.164,
      "step": 13450
    },
    {
      "epoch": 6.362029344175358,
      "grad_norm": 2.4029600620269775,
      "learning_rate": 2.8031588873173033e-05,
      "loss": 0.1552,
      "step": 13500
    },
    {
      "epoch": 6.385598962936775,
      "grad_norm": 2.7137293815612793,
      "learning_rate": 2.8011944051548012e-05,
      "loss": 0.1516,
      "step": 13550
    },
    {
      "epoch": 6.409168581698191,
      "grad_norm": 2.8100268840789795,
      "learning_rate": 2.799229922992299e-05,
      "loss": 0.1676,
      "step": 13600
    },
    {
      "epoch": 6.432738200459608,
      "grad_norm": 3.25270938873291,
      "learning_rate": 2.7972654408297973e-05,
      "loss": 0.1523,
      "step": 13650
    },
    {
      "epoch": 6.456307819221024,
      "grad_norm": 3.1934657096862793,
      "learning_rate": 2.7953009586672956e-05,
      "loss": 0.1683,
      "step": 13700
    },
    {
      "epoch": 6.479877437982441,
      "grad_norm": 2.412243127822876,
      "learning_rate": 2.7933364765047935e-05,
      "loss": 0.152,
      "step": 13750
    },
    {
      "epoch": 6.503447056743857,
      "grad_norm": 3.261054277420044,
      "learning_rate": 2.7913719943422913e-05,
      "loss": 0.1676,
      "step": 13800
    },
    {
      "epoch": 6.527016675505274,
      "grad_norm": 2.7764294147491455,
      "learning_rate": 2.7894075121797896e-05,
      "loss": 0.1598,
      "step": 13850
    },
    {
      "epoch": 6.55058629426669,
      "grad_norm": 2.59578275680542,
      "learning_rate": 2.7874430300172875e-05,
      "loss": 0.163,
      "step": 13900
    },
    {
      "epoch": 6.574155913028107,
      "grad_norm": 3.83978009223938,
      "learning_rate": 2.7854785478547854e-05,
      "loss": 0.1671,
      "step": 13950
    },
    {
      "epoch": 6.597725531789523,
      "grad_norm": 2.467808961868286,
      "learning_rate": 2.7835140656922836e-05,
      "loss": 0.1701,
      "step": 14000
    },
    {
      "epoch": 6.62129515055094,
      "grad_norm": 2.719066858291626,
      "learning_rate": 2.7815495835297815e-05,
      "loss": 0.1634,
      "step": 14050
    },
    {
      "epoch": 6.644864769312356,
      "grad_norm": 2.679409980773926,
      "learning_rate": 2.7795851013672794e-05,
      "loss": 0.1503,
      "step": 14100
    },
    {
      "epoch": 6.668434388073773,
      "grad_norm": 2.905869960784912,
      "learning_rate": 2.7776599088480277e-05,
      "loss": 0.1811,
      "step": 14150
    },
    {
      "epoch": 6.692004006835189,
      "grad_norm": 2.6978635787963867,
      "learning_rate": 2.7756954266855256e-05,
      "loss": 0.1606,
      "step": 14200
    },
    {
      "epoch": 6.715573625596606,
      "grad_norm": 2.05505633354187,
      "learning_rate": 2.7737309445230235e-05,
      "loss": 0.1515,
      "step": 14250
    },
    {
      "epoch": 6.739143244358022,
      "grad_norm": 3.090311050415039,
      "learning_rate": 2.771766462360522e-05,
      "loss": 0.1573,
      "step": 14300
    },
    {
      "epoch": 6.7627128631194395,
      "grad_norm": 2.8982632160186768,
      "learning_rate": 2.76980198019802e-05,
      "loss": 0.1737,
      "step": 14350
    },
    {
      "epoch": 6.786282481880855,
      "grad_norm": 2.111527681350708,
      "learning_rate": 2.767837498035518e-05,
      "loss": 0.1586,
      "step": 14400
    },
    {
      "epoch": 6.8098521006422725,
      "grad_norm": 5.736539363861084,
      "learning_rate": 2.765873015873016e-05,
      "loss": 0.159,
      "step": 14450
    },
    {
      "epoch": 6.833421719403689,
      "grad_norm": 2.2441086769104004,
      "learning_rate": 2.763908533710514e-05,
      "loss": 0.1538,
      "step": 14500
    },
    {
      "epoch": 6.8569913381651055,
      "grad_norm": 2.925631523132324,
      "learning_rate": 2.761944051548012e-05,
      "loss": 0.1697,
      "step": 14550
    },
    {
      "epoch": 6.880560956926522,
      "grad_norm": 2.741739511489868,
      "learning_rate": 2.75997956938551e-05,
      "loss": 0.1627,
      "step": 14600
    },
    {
      "epoch": 6.9041305756879385,
      "grad_norm": 3.139482021331787,
      "learning_rate": 2.758015087223008e-05,
      "loss": 0.1634,
      "step": 14650
    },
    {
      "epoch": 6.927700194449355,
      "grad_norm": 2.508366584777832,
      "learning_rate": 2.756050605060506e-05,
      "loss": 0.1743,
      "step": 14700
    },
    {
      "epoch": 6.9512698132107715,
      "grad_norm": 2.3735947608947754,
      "learning_rate": 2.7540861228980042e-05,
      "loss": 0.1687,
      "step": 14750
    },
    {
      "epoch": 6.974839431972188,
      "grad_norm": 3.3530807495117188,
      "learning_rate": 2.7521216407355024e-05,
      "loss": 0.1596,
      "step": 14800
    },
    {
      "epoch": 6.9984090507336045,
      "grad_norm": 4.954464435577393,
      "learning_rate": 2.7501571585730003e-05,
      "loss": 0.1581,
      "step": 14850
    },
    {
      "epoch": 7.021684049260503,
      "grad_norm": 2.6456892490386963,
      "learning_rate": 2.7481926764104982e-05,
      "loss": 0.1267,
      "step": 14900
    },
    {
      "epoch": 7.045253668021919,
      "grad_norm": 2.341752290725708,
      "learning_rate": 2.7462281942479965e-05,
      "loss": 0.131,
      "step": 14950
    },
    {
      "epoch": 7.068823286783337,
      "grad_norm": 2.479468822479248,
      "learning_rate": 2.7442637120854944e-05,
      "loss": 0.1225,
      "step": 15000
    },
    {
      "epoch": 7.068823286783337,
      "eval_loss": 0.2606530487537384,
      "eval_runtime": 182.7428,
      "eval_samples_per_second": 159.207,
      "eval_steps_per_second": 39.805,
      "step": 15000
    },
    {
      "epoch": 7.092392905544753,
      "grad_norm": 2.3266305923461914,
      "learning_rate": 2.7422992299229923e-05,
      "loss": 0.1267,
      "step": 15050
    },
    {
      "epoch": 7.11596252430617,
      "grad_norm": 3.4661192893981934,
      "learning_rate": 2.7403347477604902e-05,
      "loss": 0.1395,
      "step": 15100
    },
    {
      "epoch": 7.139532143067586,
      "grad_norm": 2.012882947921753,
      "learning_rate": 2.7383702655979884e-05,
      "loss": 0.1365,
      "step": 15150
    },
    {
      "epoch": 7.163101761829003,
      "grad_norm": 2.381775379180908,
      "learning_rate": 2.7364057834354867e-05,
      "loss": 0.1287,
      "step": 15200
    },
    {
      "epoch": 7.186671380590419,
      "grad_norm": 2.2979090213775635,
      "learning_rate": 2.7344413012729846e-05,
      "loss": 0.1367,
      "step": 15250
    },
    {
      "epoch": 7.210240999351836,
      "grad_norm": 3.0135700702667236,
      "learning_rate": 2.7324768191104828e-05,
      "loss": 0.134,
      "step": 15300
    },
    {
      "epoch": 7.233810618113252,
      "grad_norm": 3.3070104122161865,
      "learning_rate": 2.7305123369479807e-05,
      "loss": 0.1502,
      "step": 15350
    },
    {
      "epoch": 7.257380236874669,
      "grad_norm": 3.0093719959259033,
      "learning_rate": 2.7285478547854786e-05,
      "loss": 0.1436,
      "step": 15400
    },
    {
      "epoch": 7.280949855636085,
      "grad_norm": 2.479499578475952,
      "learning_rate": 2.7265833726229765e-05,
      "loss": 0.1307,
      "step": 15450
    },
    {
      "epoch": 7.304519474397502,
      "grad_norm": 2.7841598987579346,
      "learning_rate": 2.7246188904604747e-05,
      "loss": 0.1341,
      "step": 15500
    },
    {
      "epoch": 7.328089093158918,
      "grad_norm": 3.0340447425842285,
      "learning_rate": 2.7226544082979726e-05,
      "loss": 0.1481,
      "step": 15550
    },
    {
      "epoch": 7.351658711920335,
      "grad_norm": 2.7566299438476562,
      "learning_rate": 2.7206899261354705e-05,
      "loss": 0.1251,
      "step": 15600
    },
    {
      "epoch": 7.375228330681751,
      "grad_norm": 2.2501895427703857,
      "learning_rate": 2.718725443972969e-05,
      "loss": 0.1429,
      "step": 15650
    },
    {
      "epoch": 7.398797949443168,
      "grad_norm": 3.5876872539520264,
      "learning_rate": 2.716760961810467e-05,
      "loss": 0.1361,
      "step": 15700
    },
    {
      "epoch": 7.422367568204584,
      "grad_norm": 3.3842804431915283,
      "learning_rate": 2.714796479647965e-05,
      "loss": 0.1279,
      "step": 15750
    },
    {
      "epoch": 7.445937186966001,
      "grad_norm": 9.448119163513184,
      "learning_rate": 2.7128319974854628e-05,
      "loss": 0.1226,
      "step": 15800
    },
    {
      "epoch": 7.469506805727417,
      "grad_norm": 1.8389519453048706,
      "learning_rate": 2.710867515322961e-05,
      "loss": 0.1451,
      "step": 15850
    },
    {
      "epoch": 7.493076424488834,
      "grad_norm": 2.5769331455230713,
      "learning_rate": 2.708903033160459e-05,
      "loss": 0.1372,
      "step": 15900
    },
    {
      "epoch": 7.51664604325025,
      "grad_norm": 2.4404056072235107,
      "learning_rate": 2.706938550997957e-05,
      "loss": 0.1402,
      "step": 15950
    },
    {
      "epoch": 7.540215662011667,
      "grad_norm": 1.8452563285827637,
      "learning_rate": 2.704974068835455e-05,
      "loss": 0.1483,
      "step": 16000
    },
    {
      "epoch": 7.563785280773083,
      "grad_norm": 1.9299213886260986,
      "learning_rate": 2.703009586672953e-05,
      "loss": 0.1377,
      "step": 16050
    },
    {
      "epoch": 7.5873548995345,
      "grad_norm": 3.2002313137054443,
      "learning_rate": 2.7010451045104512e-05,
      "loss": 0.1348,
      "step": 16100
    },
    {
      "epoch": 7.610924518295916,
      "grad_norm": 2.4390482902526855,
      "learning_rate": 2.699080622347949e-05,
      "loss": 0.1415,
      "step": 16150
    },
    {
      "epoch": 7.634494137057333,
      "grad_norm": 3.4653549194335938,
      "learning_rate": 2.6971161401854474e-05,
      "loss": 0.1556,
      "step": 16200
    },
    {
      "epoch": 7.65806375581875,
      "grad_norm": 3.0413169860839844,
      "learning_rate": 2.6951516580229453e-05,
      "loss": 0.1356,
      "step": 16250
    },
    {
      "epoch": 7.681633374580166,
      "grad_norm": 2.958613395690918,
      "learning_rate": 2.693187175860443e-05,
      "loss": 0.1395,
      "step": 16300
    },
    {
      "epoch": 7.705202993341583,
      "grad_norm": 1.9652104377746582,
      "learning_rate": 2.6912226936979414e-05,
      "loss": 0.1453,
      "step": 16350
    },
    {
      "epoch": 7.728772612103,
      "grad_norm": 2.722874402999878,
      "learning_rate": 2.6892582115354393e-05,
      "loss": 0.1306,
      "step": 16400
    },
    {
      "epoch": 7.752342230864416,
      "grad_norm": 2.9589576721191406,
      "learning_rate": 2.6872937293729372e-05,
      "loss": 0.1373,
      "step": 16450
    },
    {
      "epoch": 7.775911849625833,
      "grad_norm": 2.1731882095336914,
      "learning_rate": 2.6853292472104354e-05,
      "loss": 0.1371,
      "step": 16500
    },
    {
      "epoch": 7.799481468387249,
      "grad_norm": 1.8190711736679077,
      "learning_rate": 2.6834040546911834e-05,
      "loss": 0.1284,
      "step": 16550
    },
    {
      "epoch": 7.823051087148666,
      "grad_norm": 3.430293560028076,
      "learning_rate": 2.6814395725286813e-05,
      "loss": 0.1297,
      "step": 16600
    },
    {
      "epoch": 7.846620705910082,
      "grad_norm": 2.803954839706421,
      "learning_rate": 2.6794750903661795e-05,
      "loss": 0.1383,
      "step": 16650
    },
    {
      "epoch": 7.870190324671499,
      "grad_norm": 2.432250499725342,
      "learning_rate": 2.6775106082036774e-05,
      "loss": 0.1457,
      "step": 16700
    },
    {
      "epoch": 7.893759943432915,
      "grad_norm": 2.48203444480896,
      "learning_rate": 2.6755461260411757e-05,
      "loss": 0.1432,
      "step": 16750
    },
    {
      "epoch": 7.917329562194332,
      "grad_norm": 3.8104536533355713,
      "learning_rate": 2.673581643878674e-05,
      "loss": 0.134,
      "step": 16800
    },
    {
      "epoch": 7.940899180955748,
      "grad_norm": 3.0015735626220703,
      "learning_rate": 2.6716171617161718e-05,
      "loss": 0.149,
      "step": 16850
    },
    {
      "epoch": 7.964468799717165,
      "grad_norm": 3.192826747894287,
      "learning_rate": 2.6696526795536697e-05,
      "loss": 0.1394,
      "step": 16900
    },
    {
      "epoch": 7.988038418478581,
      "grad_norm": 1.7764925956726074,
      "learning_rate": 2.6676881973911676e-05,
      "loss": 0.1332,
      "step": 16950
    },
    {
      "epoch": 8.01131341700548,
      "grad_norm": 2.658639669418335,
      "learning_rate": 2.665723715228666e-05,
      "loss": 0.1204,
      "step": 17000
    },
    {
      "epoch": 8.034883035766896,
      "grad_norm": 3.051112413406372,
      "learning_rate": 2.6637592330661637e-05,
      "loss": 0.112,
      "step": 17050
    },
    {
      "epoch": 8.058452654528313,
      "grad_norm": 2.4391822814941406,
      "learning_rate": 2.6617947509036616e-05,
      "loss": 0.1158,
      "step": 17100
    },
    {
      "epoch": 8.082022273289729,
      "grad_norm": 2.3293957710266113,
      "learning_rate": 2.65983026874116e-05,
      "loss": 0.113,
      "step": 17150
    },
    {
      "epoch": 8.105591892051146,
      "grad_norm": 2.155078411102295,
      "learning_rate": 2.657865786578658e-05,
      "loss": 0.1145,
      "step": 17200
    },
    {
      "epoch": 8.129161510812562,
      "grad_norm": 2.590703248977661,
      "learning_rate": 2.655901304416156e-05,
      "loss": 0.1102,
      "step": 17250
    },
    {
      "epoch": 8.15273112957398,
      "grad_norm": 2.4329895973205566,
      "learning_rate": 2.653936822253654e-05,
      "loss": 0.1039,
      "step": 17300
    },
    {
      "epoch": 8.176300748335395,
      "grad_norm": 2.7119288444519043,
      "learning_rate": 2.651972340091152e-05,
      "loss": 0.1105,
      "step": 17350
    },
    {
      "epoch": 8.199870367096812,
      "grad_norm": 3.0470120906829834,
      "learning_rate": 2.65000785792865e-05,
      "loss": 0.1134,
      "step": 17400
    },
    {
      "epoch": 8.223439985858228,
      "grad_norm": 1.9672632217407227,
      "learning_rate": 2.648043375766148e-05,
      "loss": 0.1144,
      "step": 17450
    },
    {
      "epoch": 8.247009604619645,
      "grad_norm": 2.530578136444092,
      "learning_rate": 2.6460788936036462e-05,
      "loss": 0.1122,
      "step": 17500
    },
    {
      "epoch": 8.270579223381063,
      "grad_norm": 2.9958384037017822,
      "learning_rate": 2.644114411441144e-05,
      "loss": 0.1111,
      "step": 17550
    },
    {
      "epoch": 8.294148842142478,
      "grad_norm": 2.129971742630005,
      "learning_rate": 2.642149929278642e-05,
      "loss": 0.1216,
      "step": 17600
    },
    {
      "epoch": 8.317718460903896,
      "grad_norm": 2.493025779724121,
      "learning_rate": 2.6401854471161406e-05,
      "loss": 0.1146,
      "step": 17650
    },
    {
      "epoch": 8.341288079665311,
      "grad_norm": 2.132519483566284,
      "learning_rate": 2.6382209649536385e-05,
      "loss": 0.1256,
      "step": 17700
    },
    {
      "epoch": 8.364857698426729,
      "grad_norm": 1.9111765623092651,
      "learning_rate": 2.6362564827911364e-05,
      "loss": 0.118,
      "step": 17750
    },
    {
      "epoch": 8.388427317188144,
      "grad_norm": 3.694531202316284,
      "learning_rate": 2.6342920006286343e-05,
      "loss": 0.1187,
      "step": 17800
    },
    {
      "epoch": 8.411996935949562,
      "grad_norm": 1.9915761947631836,
      "learning_rate": 2.6323275184661325e-05,
      "loss": 0.1122,
      "step": 17850
    },
    {
      "epoch": 8.435566554710977,
      "grad_norm": 2.1501381397247314,
      "learning_rate": 2.6303630363036304e-05,
      "loss": 0.1203,
      "step": 17900
    },
    {
      "epoch": 8.459136173472395,
      "grad_norm": 3.323918104171753,
      "learning_rate": 2.6283985541411283e-05,
      "loss": 0.1224,
      "step": 17950
    },
    {
      "epoch": 8.48270579223381,
      "grad_norm": 1.7847226858139038,
      "learning_rate": 2.6264340719786265e-05,
      "loss": 0.1187,
      "step": 18000
    },
    {
      "epoch": 8.506275410995228,
      "grad_norm": 2.340909242630005,
      "learning_rate": 2.6244695898161244e-05,
      "loss": 0.1241,
      "step": 18050
    },
    {
      "epoch": 8.529845029756643,
      "grad_norm": 3.1509814262390137,
      "learning_rate": 2.6225051076536227e-05,
      "loss": 0.1162,
      "step": 18100
    },
    {
      "epoch": 8.55341464851806,
      "grad_norm": 1.9559602737426758,
      "learning_rate": 2.6205406254911206e-05,
      "loss": 0.1159,
      "step": 18150
    },
    {
      "epoch": 8.576984267279476,
      "grad_norm": 1.732845664024353,
      "learning_rate": 2.6185761433286188e-05,
      "loss": 0.1203,
      "step": 18200
    },
    {
      "epoch": 8.600553886040894,
      "grad_norm": 2.344543933868408,
      "learning_rate": 2.6166116611661167e-05,
      "loss": 0.1188,
      "step": 18250
    },
    {
      "epoch": 8.62412350480231,
      "grad_norm": 2.497784376144409,
      "learning_rate": 2.6146471790036146e-05,
      "loss": 0.1137,
      "step": 18300
    },
    {
      "epoch": 8.647693123563727,
      "grad_norm": 3.878506660461426,
      "learning_rate": 2.612682696841113e-05,
      "loss": 0.1262,
      "step": 18350
    },
    {
      "epoch": 8.671262742325142,
      "grad_norm": 3.4839656352996826,
      "learning_rate": 2.6107182146786107e-05,
      "loss": 0.1273,
      "step": 18400
    },
    {
      "epoch": 8.69483236108656,
      "grad_norm": 2.8435850143432617,
      "learning_rate": 2.6087537325161086e-05,
      "loss": 0.1284,
      "step": 18450
    },
    {
      "epoch": 8.718401979847975,
      "grad_norm": 2.44563889503479,
      "learning_rate": 2.6067892503536065e-05,
      "loss": 0.129,
      "step": 18500
    },
    {
      "epoch": 8.741971598609393,
      "grad_norm": 2.2725441455841064,
      "learning_rate": 2.604824768191105e-05,
      "loss": 0.1108,
      "step": 18550
    },
    {
      "epoch": 8.765541217370808,
      "grad_norm": 2.963784694671631,
      "learning_rate": 2.602860286028603e-05,
      "loss": 0.1224,
      "step": 18600
    },
    {
      "epoch": 8.789110836132226,
      "grad_norm": 6.0530219078063965,
      "learning_rate": 2.600895803866101e-05,
      "loss": 0.1125,
      "step": 18650
    },
    {
      "epoch": 8.812680454893641,
      "grad_norm": 2.3581228256225586,
      "learning_rate": 2.598931321703599e-05,
      "loss": 0.1181,
      "step": 18700
    },
    {
      "epoch": 8.836250073655059,
      "grad_norm": 2.314937114715576,
      "learning_rate": 2.596966839541097e-05,
      "loss": 0.1168,
      "step": 18750
    },
    {
      "epoch": 8.859819692416476,
      "grad_norm": 1.5684669017791748,
      "learning_rate": 2.595002357378595e-05,
      "loss": 0.1341,
      "step": 18800
    },
    {
      "epoch": 8.883389311177892,
      "grad_norm": 1.9218024015426636,
      "learning_rate": 2.5930378752160932e-05,
      "loss": 0.1083,
      "step": 18850
    },
    {
      "epoch": 8.906958929939309,
      "grad_norm": 2.444584369659424,
      "learning_rate": 2.591073393053591e-05,
      "loss": 0.1092,
      "step": 18900
    },
    {
      "epoch": 8.930528548700725,
      "grad_norm": 3.1549079418182373,
      "learning_rate": 2.589108910891089e-05,
      "loss": 0.1257,
      "step": 18950
    },
    {
      "epoch": 8.954098167462142,
      "grad_norm": 1.7631343603134155,
      "learning_rate": 2.5871444287285872e-05,
      "loss": 0.1236,
      "step": 19000
    },
    {
      "epoch": 8.977667786223558,
      "grad_norm": 1.7034989595413208,
      "learning_rate": 2.5851799465660855e-05,
      "loss": 0.126,
      "step": 19050
    },
    {
      "epoch": 9.000942784750457,
      "grad_norm": 1.9896574020385742,
      "learning_rate": 2.5832154644035834e-05,
      "loss": 0.1248,
      "step": 19100
    },
    {
      "epoch": 9.024512403511872,
      "grad_norm": 2.7188944816589355,
      "learning_rate": 2.5812509822410813e-05,
      "loss": 0.1013,
      "step": 19150
    },
    {
      "epoch": 9.04808202227329,
      "grad_norm": 1.823943853378296,
      "learning_rate": 2.5792865000785795e-05,
      "loss": 0.0908,
      "step": 19200
    },
    {
      "epoch": 9.071651641034705,
      "grad_norm": 1.8511239290237427,
      "learning_rate": 2.5773220179160774e-05,
      "loss": 0.0992,
      "step": 19250
    },
    {
      "epoch": 9.095221259796123,
      "grad_norm": 2.9182345867156982,
      "learning_rate": 2.5753575357535753e-05,
      "loss": 0.0898,
      "step": 19300
    },
    {
      "epoch": 9.11879087855754,
      "grad_norm": 2.5864672660827637,
      "learning_rate": 2.5733930535910732e-05,
      "loss": 0.0984,
      "step": 19350
    },
    {
      "epoch": 9.142360497318956,
      "grad_norm": 1.7816579341888428,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 0.1017,
      "step": 19400
    },
    {
      "epoch": 9.165930116080373,
      "grad_norm": 1.6494501829147339,
      "learning_rate": 2.5694640892660697e-05,
      "loss": 0.0943,
      "step": 19450
    },
    {
      "epoch": 9.189499734841789,
      "grad_norm": 2.112037420272827,
      "learning_rate": 2.5674996071035676e-05,
      "loss": 0.0984,
      "step": 19500
    },
    {
      "epoch": 9.213069353603206,
      "grad_norm": 2.767266273498535,
      "learning_rate": 2.5655351249410658e-05,
      "loss": 0.1104,
      "step": 19550
    },
    {
      "epoch": 9.236638972364622,
      "grad_norm": 2.730043888092041,
      "learning_rate": 2.5635706427785637e-05,
      "loss": 0.1006,
      "step": 19600
    },
    {
      "epoch": 9.26020859112604,
      "grad_norm": 1.9854880571365356,
      "learning_rate": 2.5616061606160616e-05,
      "loss": 0.1037,
      "step": 19650
    },
    {
      "epoch": 9.283778209887455,
      "grad_norm": 3.3450872898101807,
      "learning_rate": 2.5596416784535595e-05,
      "loss": 0.1015,
      "step": 19700
    },
    {
      "epoch": 9.307347828648872,
      "grad_norm": 2.6600420475006104,
      "learning_rate": 2.5576771962910578e-05,
      "loss": 0.111,
      "step": 19750
    },
    {
      "epoch": 9.330917447410288,
      "grad_norm": 2.241533041000366,
      "learning_rate": 2.5557127141285557e-05,
      "loss": 0.1006,
      "step": 19800
    },
    {
      "epoch": 9.354487066171705,
      "grad_norm": 27.705486297607422,
      "learning_rate": 2.5537482319660536e-05,
      "loss": 0.1088,
      "step": 19850
    },
    {
      "epoch": 9.37805668493312,
      "grad_norm": 4.788182258605957,
      "learning_rate": 2.551783749803552e-05,
      "loss": 0.1027,
      "step": 19900
    },
    {
      "epoch": 9.401626303694538,
      "grad_norm": 3.2653772830963135,
      "learning_rate": 2.54981926764105e-05,
      "loss": 0.108,
      "step": 19950
    },
    {
      "epoch": 9.425195922455954,
      "grad_norm": 5.456648349761963,
      "learning_rate": 2.547854785478548e-05,
      "loss": 0.1009,
      "step": 20000
    },
    {
      "epoch": 9.425195922455954,
      "eval_loss": 0.24852116405963898,
      "eval_runtime": 187.141,
      "eval_samples_per_second": 155.466,
      "eval_steps_per_second": 38.869,
      "step": 20000
    },
    {
      "epoch": 9.448765541217371,
      "grad_norm": 2.4583029747009277,
      "learning_rate": 2.5458903033160458e-05,
      "loss": 0.1123,
      "step": 20050
    },
    {
      "epoch": 9.472335159978787,
      "grad_norm": 2.651641845703125,
      "learning_rate": 2.543925821153544e-05,
      "loss": 0.1082,
      "step": 20100
    },
    {
      "epoch": 9.495904778740204,
      "grad_norm": 1.9166182279586792,
      "learning_rate": 2.541961338991042e-05,
      "loss": 0.096,
      "step": 20150
    },
    {
      "epoch": 9.51947439750162,
      "grad_norm": 2.4034547805786133,
      "learning_rate": 2.53999685682854e-05,
      "loss": 0.107,
      "step": 20200
    },
    {
      "epoch": 9.543044016263037,
      "grad_norm": 2.1654775142669678,
      "learning_rate": 2.538032374666038e-05,
      "loss": 0.1015,
      "step": 20250
    },
    {
      "epoch": 9.566613635024453,
      "grad_norm": 1.6468007564544678,
      "learning_rate": 2.536067892503536e-05,
      "loss": 0.1069,
      "step": 20300
    },
    {
      "epoch": 9.59018325378587,
      "grad_norm": 2.638850212097168,
      "learning_rate": 2.5341034103410342e-05,
      "loss": 0.1094,
      "step": 20350
    },
    {
      "epoch": 9.613752872547286,
      "grad_norm": 3.2296600341796875,
      "learning_rate": 2.5321389281785325e-05,
      "loss": 0.1052,
      "step": 20400
    },
    {
      "epoch": 9.637322491308703,
      "grad_norm": 2.4797515869140625,
      "learning_rate": 2.5301744460160304e-05,
      "loss": 0.1105,
      "step": 20450
    },
    {
      "epoch": 9.660892110070119,
      "grad_norm": 3.585251569747925,
      "learning_rate": 2.5282099638535283e-05,
      "loss": 0.109,
      "step": 20500
    },
    {
      "epoch": 9.684461728831536,
      "grad_norm": 2.876394271850586,
      "learning_rate": 2.5262454816910262e-05,
      "loss": 0.1097,
      "step": 20550
    },
    {
      "epoch": 9.708031347592954,
      "grad_norm": 5.340420246124268,
      "learning_rate": 2.5242809995285244e-05,
      "loss": 0.106,
      "step": 20600
    },
    {
      "epoch": 9.73160096635437,
      "grad_norm": 1.8986384868621826,
      "learning_rate": 2.5223165173660223e-05,
      "loss": 0.106,
      "step": 20650
    },
    {
      "epoch": 9.755170585115785,
      "grad_norm": 1.931344985961914,
      "learning_rate": 2.5203520352035202e-05,
      "loss": 0.113,
      "step": 20700
    },
    {
      "epoch": 9.778740203877202,
      "grad_norm": 2.526815414428711,
      "learning_rate": 2.5183875530410185e-05,
      "loss": 0.107,
      "step": 20750
    },
    {
      "epoch": 9.80230982263862,
      "grad_norm": 3.347668409347534,
      "learning_rate": 2.5164230708785167e-05,
      "loss": 0.1065,
      "step": 20800
    },
    {
      "epoch": 9.825879441400035,
      "grad_norm": 2.734592914581299,
      "learning_rate": 2.5144585887160146e-05,
      "loss": 0.1105,
      "step": 20850
    },
    {
      "epoch": 9.849449060161453,
      "grad_norm": 2.616706371307373,
      "learning_rate": 2.5124941065535125e-05,
      "loss": 0.1132,
      "step": 20900
    },
    {
      "epoch": 9.873018678922868,
      "grad_norm": 2.317852020263672,
      "learning_rate": 2.5105296243910107e-05,
      "loss": 0.11,
      "step": 20950
    },
    {
      "epoch": 9.896588297684286,
      "grad_norm": 2.61836838722229,
      "learning_rate": 2.5085651422285086e-05,
      "loss": 0.0986,
      "step": 21000
    },
    {
      "epoch": 9.920157916445701,
      "grad_norm": 2.919907808303833,
      "learning_rate": 2.5066006600660065e-05,
      "loss": 0.1229,
      "step": 21050
    },
    {
      "epoch": 9.943727535207119,
      "grad_norm": 2.225034236907959,
      "learning_rate": 2.5046361779035048e-05,
      "loss": 0.1033,
      "step": 21100
    },
    {
      "epoch": 9.967297153968534,
      "grad_norm": 1.9104818105697632,
      "learning_rate": 2.5027109853842527e-05,
      "loss": 0.1045,
      "step": 21150
    },
    {
      "epoch": 9.990866772729952,
      "grad_norm": 3.373857021331787,
      "learning_rate": 2.500746503221751e-05,
      "loss": 0.1049,
      "step": 21200
    },
    {
      "epoch": 10.01414177125685,
      "grad_norm": 1.8620951175689697,
      "learning_rate": 2.498782021059249e-05,
      "loss": 0.0866,
      "step": 21250
    },
    {
      "epoch": 10.037711390018266,
      "grad_norm": 1.855323076248169,
      "learning_rate": 2.4968175388967468e-05,
      "loss": 0.0856,
      "step": 21300
    },
    {
      "epoch": 10.061281008779684,
      "grad_norm": 2.6884970664978027,
      "learning_rate": 2.4948530567342447e-05,
      "loss": 0.0909,
      "step": 21350
    },
    {
      "epoch": 10.0848506275411,
      "grad_norm": 3.408541679382324,
      "learning_rate": 2.492888574571743e-05,
      "loss": 0.0882,
      "step": 21400
    },
    {
      "epoch": 10.108420246302517,
      "grad_norm": 2.1324219703674316,
      "learning_rate": 2.490924092409241e-05,
      "loss": 0.089,
      "step": 21450
    },
    {
      "epoch": 10.131989865063932,
      "grad_norm": 2.6393895149230957,
      "learning_rate": 2.488959610246739e-05,
      "loss": 0.0888,
      "step": 21500
    },
    {
      "epoch": 10.15555948382535,
      "grad_norm": 2.594667434692383,
      "learning_rate": 2.4869951280842373e-05,
      "loss": 0.0873,
      "step": 21550
    },
    {
      "epoch": 10.179129102586765,
      "grad_norm": 2.3823587894439697,
      "learning_rate": 2.4850306459217352e-05,
      "loss": 0.0845,
      "step": 21600
    },
    {
      "epoch": 10.202698721348183,
      "grad_norm": 3.1397135257720947,
      "learning_rate": 2.483066163759233e-05,
      "loss": 0.0858,
      "step": 21650
    },
    {
      "epoch": 10.226268340109598,
      "grad_norm": 2.6247341632843018,
      "learning_rate": 2.481101681596731e-05,
      "loss": 0.0969,
      "step": 21700
    },
    {
      "epoch": 10.249837958871016,
      "grad_norm": 2.1287312507629395,
      "learning_rate": 2.4791371994342292e-05,
      "loss": 0.0922,
      "step": 21750
    },
    {
      "epoch": 10.273407577632431,
      "grad_norm": 2.548004388809204,
      "learning_rate": 2.477172717271727e-05,
      "loss": 0.0942,
      "step": 21800
    },
    {
      "epoch": 10.296977196393849,
      "grad_norm": 5.365366458892822,
      "learning_rate": 2.475208235109225e-05,
      "loss": 0.0941,
      "step": 21850
    },
    {
      "epoch": 10.320546815155264,
      "grad_norm": 2.560063123703003,
      "learning_rate": 2.4732437529467236e-05,
      "loss": 0.0996,
      "step": 21900
    },
    {
      "epoch": 10.344116433916682,
      "grad_norm": 2.623929500579834,
      "learning_rate": 2.4712792707842215e-05,
      "loss": 0.0944,
      "step": 21950
    },
    {
      "epoch": 10.367686052678097,
      "grad_norm": 1.6253234148025513,
      "learning_rate": 2.4693147886217194e-05,
      "loss": 0.0893,
      "step": 22000
    },
    {
      "epoch": 10.391255671439515,
      "grad_norm": 2.102797269821167,
      "learning_rate": 2.4673895961024677e-05,
      "loss": 0.0912,
      "step": 22050
    },
    {
      "epoch": 10.41482529020093,
      "grad_norm": 2.9221160411834717,
      "learning_rate": 2.4654251139399656e-05,
      "loss": 0.0966,
      "step": 22100
    },
    {
      "epoch": 10.438394908962348,
      "grad_norm": 1.6524978876113892,
      "learning_rate": 2.4634606317774635e-05,
      "loss": 0.0856,
      "step": 22150
    },
    {
      "epoch": 10.461964527723763,
      "grad_norm": 1.7405853271484375,
      "learning_rate": 2.4614961496149617e-05,
      "loss": 0.0887,
      "step": 22200
    },
    {
      "epoch": 10.48553414648518,
      "grad_norm": 2.192683458328247,
      "learning_rate": 2.4595316674524596e-05,
      "loss": 0.0885,
      "step": 22250
    },
    {
      "epoch": 10.509103765246596,
      "grad_norm": 1.6571390628814697,
      "learning_rate": 2.4575671852899575e-05,
      "loss": 0.0855,
      "step": 22300
    },
    {
      "epoch": 10.532673384008014,
      "grad_norm": 2.1894521713256836,
      "learning_rate": 2.4556027031274558e-05,
      "loss": 0.0871,
      "step": 22350
    },
    {
      "epoch": 10.55624300276943,
      "grad_norm": 2.2067761421203613,
      "learning_rate": 2.4536382209649537e-05,
      "loss": 0.105,
      "step": 22400
    },
    {
      "epoch": 10.579812621530847,
      "grad_norm": 2.3526830673217773,
      "learning_rate": 2.4516737388024516e-05,
      "loss": 0.0963,
      "step": 22450
    },
    {
      "epoch": 10.603382240292262,
      "grad_norm": 3.188072919845581,
      "learning_rate": 2.4497092566399498e-05,
      "loss": 0.0952,
      "step": 22500
    },
    {
      "epoch": 10.62695185905368,
      "grad_norm": 3.7132136821746826,
      "learning_rate": 2.447744774477448e-05,
      "loss": 0.0917,
      "step": 22550
    },
    {
      "epoch": 10.650521477815097,
      "grad_norm": 2.89111065864563,
      "learning_rate": 2.445780292314946e-05,
      "loss": 0.0891,
      "step": 22600
    },
    {
      "epoch": 10.674091096576513,
      "grad_norm": 2.7135274410247803,
      "learning_rate": 2.443815810152444e-05,
      "loss": 0.0951,
      "step": 22650
    },
    {
      "epoch": 10.69766071533793,
      "grad_norm": 2.0742013454437256,
      "learning_rate": 2.441851327989942e-05,
      "loss": 0.089,
      "step": 22700
    },
    {
      "epoch": 10.721230334099346,
      "grad_norm": 2.524627208709717,
      "learning_rate": 2.43988684582744e-05,
      "loss": 0.0985,
      "step": 22750
    },
    {
      "epoch": 10.744799952860763,
      "grad_norm": 2.200314521789551,
      "learning_rate": 2.437922363664938e-05,
      "loss": 0.0863,
      "step": 22800
    },
    {
      "epoch": 10.768369571622179,
      "grad_norm": 1.6706398725509644,
      "learning_rate": 2.4359578815024358e-05,
      "loss": 0.0872,
      "step": 22850
    },
    {
      "epoch": 10.791939190383596,
      "grad_norm": 2.0384533405303955,
      "learning_rate": 2.433993399339934e-05,
      "loss": 0.0977,
      "step": 22900
    },
    {
      "epoch": 10.815508809145012,
      "grad_norm": 2.0966145992279053,
      "learning_rate": 2.4320289171774322e-05,
      "loss": 0.0999,
      "step": 22950
    },
    {
      "epoch": 10.83907842790643,
      "grad_norm": 2.0409140586853027,
      "learning_rate": 2.43006443501493e-05,
      "loss": 0.0905,
      "step": 23000
    },
    {
      "epoch": 10.862648046667845,
      "grad_norm": 2.9096288681030273,
      "learning_rate": 2.4280999528524284e-05,
      "loss": 0.099,
      "step": 23050
    },
    {
      "epoch": 10.886217665429262,
      "grad_norm": 2.9603817462921143,
      "learning_rate": 2.4261354706899263e-05,
      "loss": 0.0932,
      "step": 23100
    },
    {
      "epoch": 10.909787284190678,
      "grad_norm": 1.9210821390151978,
      "learning_rate": 2.4241709885274242e-05,
      "loss": 0.0906,
      "step": 23150
    },
    {
      "epoch": 10.933356902952095,
      "grad_norm": 2.4585282802581787,
      "learning_rate": 2.422206506364922e-05,
      "loss": 0.0959,
      "step": 23200
    },
    {
      "epoch": 10.95692652171351,
      "grad_norm": 2.7493560314178467,
      "learning_rate": 2.4202420242024203e-05,
      "loss": 0.0933,
      "step": 23250
    },
    {
      "epoch": 10.980496140474928,
      "grad_norm": 2.3975281715393066,
      "learning_rate": 2.4182775420399182e-05,
      "loss": 0.0998,
      "step": 23300
    },
    {
      "epoch": 11.003771139001827,
      "grad_norm": 1.7946234941482544,
      "learning_rate": 2.416313059877416e-05,
      "loss": 0.09,
      "step": 23350
    },
    {
      "epoch": 11.027340757763243,
      "grad_norm": 2.0652260780334473,
      "learning_rate": 2.4143485777149147e-05,
      "loss": 0.0768,
      "step": 23400
    },
    {
      "epoch": 11.05091037652466,
      "grad_norm": 2.278787612915039,
      "learning_rate": 2.4123840955524126e-05,
      "loss": 0.0825,
      "step": 23450
    },
    {
      "epoch": 11.074479995286076,
      "grad_norm": 2.4466683864593506,
      "learning_rate": 2.4104196133899105e-05,
      "loss": 0.0741,
      "step": 23500
    },
    {
      "epoch": 11.098049614047493,
      "grad_norm": 2.5642268657684326,
      "learning_rate": 2.4084551312274087e-05,
      "loss": 0.0751,
      "step": 23550
    },
    {
      "epoch": 11.121619232808909,
      "grad_norm": 2.2962732315063477,
      "learning_rate": 2.4064906490649066e-05,
      "loss": 0.0795,
      "step": 23600
    },
    {
      "epoch": 11.145188851570326,
      "grad_norm": 3.224898338317871,
      "learning_rate": 2.4045261669024045e-05,
      "loss": 0.0801,
      "step": 23650
    },
    {
      "epoch": 11.168758470331742,
      "grad_norm": 1.6284462213516235,
      "learning_rate": 2.4025616847399024e-05,
      "loss": 0.0731,
      "step": 23700
    },
    {
      "epoch": 11.19232808909316,
      "grad_norm": 2.258411407470703,
      "learning_rate": 2.4005972025774007e-05,
      "loss": 0.0748,
      "step": 23750
    },
    {
      "epoch": 11.215897707854575,
      "grad_norm": 1.7667148113250732,
      "learning_rate": 2.3986327204148986e-05,
      "loss": 0.0837,
      "step": 23800
    },
    {
      "epoch": 11.239467326615992,
      "grad_norm": 2.832271099090576,
      "learning_rate": 2.3966682382523968e-05,
      "loss": 0.0791,
      "step": 23850
    },
    {
      "epoch": 11.263036945377408,
      "grad_norm": 1.916905164718628,
      "learning_rate": 2.394703756089895e-05,
      "loss": 0.0842,
      "step": 23900
    },
    {
      "epoch": 11.286606564138825,
      "grad_norm": 3.136363983154297,
      "learning_rate": 2.392739273927393e-05,
      "loss": 0.0793,
      "step": 23950
    },
    {
      "epoch": 11.310176182900241,
      "grad_norm": 2.1581900119781494,
      "learning_rate": 2.390774791764891e-05,
      "loss": 0.0825,
      "step": 24000
    },
    {
      "epoch": 11.333745801661658,
      "grad_norm": 2.272516965866089,
      "learning_rate": 2.3888103096023887e-05,
      "loss": 0.0749,
      "step": 24050
    },
    {
      "epoch": 11.357315420423074,
      "grad_norm": 2.5798182487487793,
      "learning_rate": 2.386845827439887e-05,
      "loss": 0.0792,
      "step": 24100
    },
    {
      "epoch": 11.380885039184491,
      "grad_norm": 1.1410950422286987,
      "learning_rate": 2.384881345277385e-05,
      "loss": 0.0912,
      "step": 24150
    },
    {
      "epoch": 11.404454657945907,
      "grad_norm": 2.5839836597442627,
      "learning_rate": 2.3829168631148828e-05,
      "loss": 0.0816,
      "step": 24200
    },
    {
      "epoch": 11.428024276707324,
      "grad_norm": 3.247277021408081,
      "learning_rate": 2.380952380952381e-05,
      "loss": 0.0877,
      "step": 24250
    },
    {
      "epoch": 11.45159389546874,
      "grad_norm": 2.203435182571411,
      "learning_rate": 2.3789878987898793e-05,
      "loss": 0.0793,
      "step": 24300
    },
    {
      "epoch": 11.475163514230157,
      "grad_norm": 3.444498300552368,
      "learning_rate": 2.377023416627377e-05,
      "loss": 0.0818,
      "step": 24350
    },
    {
      "epoch": 11.498733132991575,
      "grad_norm": 1.8401585817337036,
      "learning_rate": 2.375058934464875e-05,
      "loss": 0.0842,
      "step": 24400
    },
    {
      "epoch": 11.52230275175299,
      "grad_norm": 2.713519811630249,
      "learning_rate": 2.3730944523023733e-05,
      "loss": 0.0939,
      "step": 24450
    },
    {
      "epoch": 11.545872370514408,
      "grad_norm": 1.9988025426864624,
      "learning_rate": 2.3711299701398712e-05,
      "loss": 0.0806,
      "step": 24500
    },
    {
      "epoch": 11.569441989275823,
      "grad_norm": 1.790338158607483,
      "learning_rate": 2.369165487977369e-05,
      "loss": 0.0844,
      "step": 24550
    },
    {
      "epoch": 11.59301160803724,
      "grad_norm": 1.885651707649231,
      "learning_rate": 2.3672010058148673e-05,
      "loss": 0.0846,
      "step": 24600
    },
    {
      "epoch": 11.616581226798656,
      "grad_norm": 1.754900336265564,
      "learning_rate": 2.3652365236523652e-05,
      "loss": 0.0842,
      "step": 24650
    },
    {
      "epoch": 11.640150845560074,
      "grad_norm": 1.716858983039856,
      "learning_rate": 2.363272041489863e-05,
      "loss": 0.093,
      "step": 24700
    },
    {
      "epoch": 11.66372046432149,
      "grad_norm": 2.838853359222412,
      "learning_rate": 2.3613075593273614e-05,
      "loss": 0.0885,
      "step": 24750
    },
    {
      "epoch": 11.687290083082907,
      "grad_norm": 2.6669087409973145,
      "learning_rate": 2.3593430771648596e-05,
      "loss": 0.08,
      "step": 24800
    },
    {
      "epoch": 11.710859701844322,
      "grad_norm": 1.772337555885315,
      "learning_rate": 2.3573785950023575e-05,
      "loss": 0.0868,
      "step": 24850
    },
    {
      "epoch": 11.73442932060574,
      "grad_norm": 1.9585773944854736,
      "learning_rate": 2.3554141128398554e-05,
      "loss": 0.0782,
      "step": 24900
    },
    {
      "epoch": 11.757998939367155,
      "grad_norm": 2.115893602371216,
      "learning_rate": 2.3534496306773536e-05,
      "loss": 0.0888,
      "step": 24950
    },
    {
      "epoch": 11.781568558128573,
      "grad_norm": 1.4699312448501587,
      "learning_rate": 2.3514851485148515e-05,
      "loss": 0.0803,
      "step": 25000
    },
    {
      "epoch": 11.781568558128573,
      "eval_loss": 0.24692654609680176,
      "eval_runtime": 184.1118,
      "eval_samples_per_second": 158.024,
      "eval_steps_per_second": 39.509,
      "step": 25000
    },
    {
      "epoch": 11.805138176889988,
      "grad_norm": 7.299472332000732,
      "learning_rate": 2.3495206663523494e-05,
      "loss": 0.0876,
      "step": 25050
    },
    {
      "epoch": 11.828707795651406,
      "grad_norm": 2.915254831314087,
      "learning_rate": 2.3475561841898477e-05,
      "loss": 0.0829,
      "step": 25100
    },
    {
      "epoch": 11.852277414412821,
      "grad_norm": 1.8544130325317383,
      "learning_rate": 2.3455917020273456e-05,
      "loss": 0.0947,
      "step": 25150
    },
    {
      "epoch": 11.875847033174239,
      "grad_norm": 1.3877222537994385,
      "learning_rate": 2.3436272198648438e-05,
      "loss": 0.0876,
      "step": 25200
    },
    {
      "epoch": 11.899416651935654,
      "grad_norm": 3.4001710414886475,
      "learning_rate": 2.3416627377023417e-05,
      "loss": 0.0898,
      "step": 25250
    },
    {
      "epoch": 11.922986270697072,
      "grad_norm": 2.319326162338257,
      "learning_rate": 2.33969825553984e-05,
      "loss": 0.086,
      "step": 25300
    },
    {
      "epoch": 11.946555889458487,
      "grad_norm": 2.3736746311187744,
      "learning_rate": 2.337733773377338e-05,
      "loss": 0.0851,
      "step": 25350
    },
    {
      "epoch": 11.970125508219905,
      "grad_norm": 4.511404514312744,
      "learning_rate": 2.3357692912148358e-05,
      "loss": 0.0906,
      "step": 25400
    },
    {
      "epoch": 11.99369512698132,
      "grad_norm": 3.5249199867248535,
      "learning_rate": 2.333804809052334e-05,
      "loss": 0.0878,
      "step": 25450
    },
    {
      "epoch": 12.01697012550822,
      "grad_norm": 1.9079742431640625,
      "learning_rate": 2.331840326889832e-05,
      "loss": 0.075,
      "step": 25500
    },
    {
      "epoch": 12.040539744269637,
      "grad_norm": 2.1872878074645996,
      "learning_rate": 2.3298758447273298e-05,
      "loss": 0.0706,
      "step": 25550
    },
    {
      "epoch": 12.064109363031053,
      "grad_norm": 2.315683603286743,
      "learning_rate": 2.3279113625648277e-05,
      "loss": 0.0708,
      "step": 25600
    },
    {
      "epoch": 12.08767898179247,
      "grad_norm": 3.038419485092163,
      "learning_rate": 2.3259468804023263e-05,
      "loss": 0.0707,
      "step": 25650
    },
    {
      "epoch": 12.111248600553886,
      "grad_norm": 2.7968716621398926,
      "learning_rate": 2.323982398239824e-05,
      "loss": 0.0693,
      "step": 25700
    },
    {
      "epoch": 12.134818219315303,
      "grad_norm": 2.0624215602874756,
      "learning_rate": 2.322017916077322e-05,
      "loss": 0.0767,
      "step": 25750
    },
    {
      "epoch": 12.158387838076719,
      "grad_norm": 2.0267419815063477,
      "learning_rate": 2.3200534339148203e-05,
      "loss": 0.0699,
      "step": 25800
    },
    {
      "epoch": 12.181957456838136,
      "grad_norm": 3.2881624698638916,
      "learning_rate": 2.3180889517523182e-05,
      "loss": 0.0655,
      "step": 25850
    },
    {
      "epoch": 12.205527075599552,
      "grad_norm": 1.8595550060272217,
      "learning_rate": 2.316124469589816e-05,
      "loss": 0.0658,
      "step": 25900
    },
    {
      "epoch": 12.229096694360969,
      "grad_norm": 2.2956864833831787,
      "learning_rate": 2.314159987427314e-05,
      "loss": 0.0753,
      "step": 25950
    },
    {
      "epoch": 12.252666313122385,
      "grad_norm": 3.652890205383301,
      "learning_rate": 2.3121955052648122e-05,
      "loss": 0.0771,
      "step": 26000
    },
    {
      "epoch": 12.276235931883802,
      "grad_norm": 3.2586510181427,
      "learning_rate": 2.31023102310231e-05,
      "loss": 0.0755,
      "step": 26050
    },
    {
      "epoch": 12.299805550645218,
      "grad_norm": 2.4033772945404053,
      "learning_rate": 2.3082665409398084e-05,
      "loss": 0.0718,
      "step": 26100
    },
    {
      "epoch": 12.323375169406635,
      "grad_norm": 1.7849395275115967,
      "learning_rate": 2.3063020587773066e-05,
      "loss": 0.0721,
      "step": 26150
    },
    {
      "epoch": 12.34694478816805,
      "grad_norm": 1.6696542501449585,
      "learning_rate": 2.3043375766148045e-05,
      "loss": 0.0799,
      "step": 26200
    },
    {
      "epoch": 12.370514406929468,
      "grad_norm": 3.407076835632324,
      "learning_rate": 2.3023730944523024e-05,
      "loss": 0.0713,
      "step": 26250
    },
    {
      "epoch": 12.394084025690885,
      "grad_norm": 1.8372009992599487,
      "learning_rate": 2.3004086122898007e-05,
      "loss": 0.0723,
      "step": 26300
    },
    {
      "epoch": 12.417653644452301,
      "grad_norm": 1.8533142805099487,
      "learning_rate": 2.2984441301272985e-05,
      "loss": 0.0713,
      "step": 26350
    },
    {
      "epoch": 12.441223263213718,
      "grad_norm": 1.4280765056610107,
      "learning_rate": 2.2964796479647964e-05,
      "loss": 0.0778,
      "step": 26400
    },
    {
      "epoch": 12.464792881975134,
      "grad_norm": 1.4616045951843262,
      "learning_rate": 2.2945544554455447e-05,
      "loss": 0.0756,
      "step": 26450
    },
    {
      "epoch": 12.488362500736551,
      "grad_norm": 2.33780837059021,
      "learning_rate": 2.2925899732830426e-05,
      "loss": 0.0748,
      "step": 26500
    },
    {
      "epoch": 12.511932119497967,
      "grad_norm": 2.5945706367492676,
      "learning_rate": 2.2906254911205405e-05,
      "loss": 0.0744,
      "step": 26550
    },
    {
      "epoch": 12.535501738259384,
      "grad_norm": 2.435713529586792,
      "learning_rate": 2.2886610089580388e-05,
      "loss": 0.0802,
      "step": 26600
    },
    {
      "epoch": 12.5590713570208,
      "grad_norm": 2.110917329788208,
      "learning_rate": 2.2866965267955367e-05,
      "loss": 0.0834,
      "step": 26650
    },
    {
      "epoch": 12.582640975782217,
      "grad_norm": 2.940336227416992,
      "learning_rate": 2.2847320446330346e-05,
      "loss": 0.0737,
      "step": 26700
    },
    {
      "epoch": 12.606210594543633,
      "grad_norm": 2.8951501846313477,
      "learning_rate": 2.2827675624705328e-05,
      "loss": 0.0777,
      "step": 26750
    },
    {
      "epoch": 12.62978021330505,
      "grad_norm": 1.6609687805175781,
      "learning_rate": 2.280803080308031e-05,
      "loss": 0.0856,
      "step": 26800
    },
    {
      "epoch": 12.653349832066466,
      "grad_norm": 1.30377197265625,
      "learning_rate": 2.278838598145529e-05,
      "loss": 0.0856,
      "step": 26850
    },
    {
      "epoch": 12.676919450827883,
      "grad_norm": 1.9152534008026123,
      "learning_rate": 2.276874115983027e-05,
      "loss": 0.0754,
      "step": 26900
    },
    {
      "epoch": 12.700489069589299,
      "grad_norm": 1.943624496459961,
      "learning_rate": 2.274909633820525e-05,
      "loss": 0.0845,
      "step": 26950
    },
    {
      "epoch": 12.724058688350716,
      "grad_norm": 2.182873487472534,
      "learning_rate": 2.272945151658023e-05,
      "loss": 0.0772,
      "step": 27000
    },
    {
      "epoch": 12.747628307112132,
      "grad_norm": 1.6026455163955688,
      "learning_rate": 2.270980669495521e-05,
      "loss": 0.0829,
      "step": 27050
    },
    {
      "epoch": 12.77119792587355,
      "grad_norm": 1.7950439453125,
      "learning_rate": 2.269016187333019e-05,
      "loss": 0.0771,
      "step": 27100
    },
    {
      "epoch": 12.794767544634965,
      "grad_norm": 1.038053274154663,
      "learning_rate": 2.267051705170517e-05,
      "loss": 0.0801,
      "step": 27150
    },
    {
      "epoch": 12.818337163396382,
      "grad_norm": 1.5150675773620605,
      "learning_rate": 2.2650872230080153e-05,
      "loss": 0.0757,
      "step": 27200
    },
    {
      "epoch": 12.841906782157798,
      "grad_norm": 2.2950539588928223,
      "learning_rate": 2.2631227408455132e-05,
      "loss": 0.0758,
      "step": 27250
    },
    {
      "epoch": 12.865476400919215,
      "grad_norm": 1.857843279838562,
      "learning_rate": 2.2611582586830114e-05,
      "loss": 0.0701,
      "step": 27300
    },
    {
      "epoch": 12.889046019680631,
      "grad_norm": 2.22076678276062,
      "learning_rate": 2.2591937765205093e-05,
      "loss": 0.0856,
      "step": 27350
    },
    {
      "epoch": 12.912615638442048,
      "grad_norm": 2.139019012451172,
      "learning_rate": 2.2572292943580072e-05,
      "loss": 0.077,
      "step": 27400
    },
    {
      "epoch": 12.936185257203464,
      "grad_norm": 1.1930826902389526,
      "learning_rate": 2.2552648121955054e-05,
      "loss": 0.0707,
      "step": 27450
    },
    {
      "epoch": 12.959754875964881,
      "grad_norm": 2.0253214836120605,
      "learning_rate": 2.2533003300330033e-05,
      "loss": 0.0835,
      "step": 27500
    },
    {
      "epoch": 12.983324494726297,
      "grad_norm": 3.52176570892334,
      "learning_rate": 2.2513358478705012e-05,
      "loss": 0.0752,
      "step": 27550
    },
    {
      "epoch": 13.006599493253196,
      "grad_norm": 2.3306076526641846,
      "learning_rate": 2.249371365707999e-05,
      "loss": 0.0725,
      "step": 27600
    },
    {
      "epoch": 13.030169112014613,
      "grad_norm": 4.163065433502197,
      "learning_rate": 2.2474068835454977e-05,
      "loss": 0.0619,
      "step": 27650
    },
    {
      "epoch": 13.053738730776029,
      "grad_norm": 2.0083577632904053,
      "learning_rate": 2.2454424013829956e-05,
      "loss": 0.0571,
      "step": 27700
    },
    {
      "epoch": 13.077308349537446,
      "grad_norm": 1.997465968132019,
      "learning_rate": 2.2434779192204935e-05,
      "loss": 0.0693,
      "step": 27750
    },
    {
      "epoch": 13.100877968298862,
      "grad_norm": 1.9270504713058472,
      "learning_rate": 2.2415134370579918e-05,
      "loss": 0.0673,
      "step": 27800
    },
    {
      "epoch": 13.12444758706028,
      "grad_norm": 3.0632996559143066,
      "learning_rate": 2.2395489548954897e-05,
      "loss": 0.0637,
      "step": 27850
    },
    {
      "epoch": 13.148017205821695,
      "grad_norm": 1.6959260702133179,
      "learning_rate": 2.2375844727329876e-05,
      "loss": 0.0617,
      "step": 27900
    },
    {
      "epoch": 13.171586824583112,
      "grad_norm": 2.233475685119629,
      "learning_rate": 2.2356199905704855e-05,
      "loss": 0.0619,
      "step": 27950
    },
    {
      "epoch": 13.195156443344528,
      "grad_norm": 2.0122106075286865,
      "learning_rate": 2.2336555084079837e-05,
      "loss": 0.0664,
      "step": 28000
    },
    {
      "epoch": 13.218726062105945,
      "grad_norm": 4.449855327606201,
      "learning_rate": 2.2316910262454816e-05,
      "loss": 0.0655,
      "step": 28050
    },
    {
      "epoch": 13.242295680867361,
      "grad_norm": 2.2726776599884033,
      "learning_rate": 2.22972654408298e-05,
      "loss": 0.069,
      "step": 28100
    },
    {
      "epoch": 13.265865299628778,
      "grad_norm": 1.3395625352859497,
      "learning_rate": 2.227762061920478e-05,
      "loss": 0.0636,
      "step": 28150
    },
    {
      "epoch": 13.289434918390196,
      "grad_norm": 1.5061213970184326,
      "learning_rate": 2.225797579757976e-05,
      "loss": 0.0657,
      "step": 28200
    },
    {
      "epoch": 13.313004537151611,
      "grad_norm": 2.0182547569274902,
      "learning_rate": 2.223833097595474e-05,
      "loss": 0.0698,
      "step": 28250
    },
    {
      "epoch": 13.336574155913029,
      "grad_norm": 1.5100116729736328,
      "learning_rate": 2.2218686154329718e-05,
      "loss": 0.0666,
      "step": 28300
    },
    {
      "epoch": 13.360143774674444,
      "grad_norm": 2.4953088760375977,
      "learning_rate": 2.21990413327047e-05,
      "loss": 0.0778,
      "step": 28350
    },
    {
      "epoch": 13.383713393435862,
      "grad_norm": 3.5140578746795654,
      "learning_rate": 2.217939651107968e-05,
      "loss": 0.0599,
      "step": 28400
    },
    {
      "epoch": 13.407283012197277,
      "grad_norm": 2.483144760131836,
      "learning_rate": 2.2159751689454658e-05,
      "loss": 0.0603,
      "step": 28450
    },
    {
      "epoch": 13.430852630958695,
      "grad_norm": 1.3652962446212769,
      "learning_rate": 2.214010686782964e-05,
      "loss": 0.0716,
      "step": 28500
    },
    {
      "epoch": 13.45442224972011,
      "grad_norm": 1.5030237436294556,
      "learning_rate": 2.2120462046204623e-05,
      "loss": 0.0749,
      "step": 28550
    },
    {
      "epoch": 13.477991868481528,
      "grad_norm": 2.8406338691711426,
      "learning_rate": 2.2100817224579602e-05,
      "loss": 0.0734,
      "step": 28600
    },
    {
      "epoch": 13.501561487242943,
      "grad_norm": 2.0196495056152344,
      "learning_rate": 2.2081172402954584e-05,
      "loss": 0.0712,
      "step": 28650
    },
    {
      "epoch": 13.52513110600436,
      "grad_norm": 1.9803727865219116,
      "learning_rate": 2.2061527581329563e-05,
      "loss": 0.0623,
      "step": 28700
    },
    {
      "epoch": 13.548700724765776,
      "grad_norm": 2.419214963912964,
      "learning_rate": 2.2041882759704542e-05,
      "loss": 0.0716,
      "step": 28750
    },
    {
      "epoch": 13.572270343527194,
      "grad_norm": 1.8405399322509766,
      "learning_rate": 2.202223793807952e-05,
      "loss": 0.0808,
      "step": 28800
    },
    {
      "epoch": 13.59583996228861,
      "grad_norm": 2.1608529090881348,
      "learning_rate": 2.2002986012887004e-05,
      "loss": 0.0693,
      "step": 28850
    },
    {
      "epoch": 13.619409581050027,
      "grad_norm": 2.656930685043335,
      "learning_rate": 2.1983341191261983e-05,
      "loss": 0.0734,
      "step": 28900
    },
    {
      "epoch": 13.642979199811442,
      "grad_norm": 2.401090145111084,
      "learning_rate": 2.1963696369636966e-05,
      "loss": 0.0683,
      "step": 28950
    },
    {
      "epoch": 13.66654881857286,
      "grad_norm": 3.3170998096466064,
      "learning_rate": 2.1944051548011945e-05,
      "loss": 0.0724,
      "step": 29000
    },
    {
      "epoch": 13.690118437334275,
      "grad_norm": 1.6723110675811768,
      "learning_rate": 2.1924406726386924e-05,
      "loss": 0.0791,
      "step": 29050
    },
    {
      "epoch": 13.713688056095693,
      "grad_norm": 2.032759428024292,
      "learning_rate": 2.1904761904761903e-05,
      "loss": 0.0715,
      "step": 29100
    },
    {
      "epoch": 13.737257674857108,
      "grad_norm": 2.219003438949585,
      "learning_rate": 2.1885117083136885e-05,
      "loss": 0.0711,
      "step": 29150
    },
    {
      "epoch": 13.760827293618526,
      "grad_norm": 2.0479702949523926,
      "learning_rate": 2.1865472261511867e-05,
      "loss": 0.0737,
      "step": 29200
    },
    {
      "epoch": 13.784396912379941,
      "grad_norm": 1.5912667512893677,
      "learning_rate": 2.1845827439886846e-05,
      "loss": 0.0723,
      "step": 29250
    },
    {
      "epoch": 13.807966531141359,
      "grad_norm": 2.523649215698242,
      "learning_rate": 2.182618261826183e-05,
      "loss": 0.0674,
      "step": 29300
    },
    {
      "epoch": 13.831536149902774,
      "grad_norm": 1.8641425371170044,
      "learning_rate": 2.1806537796636808e-05,
      "loss": 0.074,
      "step": 29350
    },
    {
      "epoch": 13.855105768664192,
      "grad_norm": 2.0689539909362793,
      "learning_rate": 2.1786892975011787e-05,
      "loss": 0.0739,
      "step": 29400
    },
    {
      "epoch": 13.87867538742561,
      "grad_norm": 2.1820404529571533,
      "learning_rate": 2.176764104981927e-05,
      "loss": 0.0828,
      "step": 29450
    },
    {
      "epoch": 13.902245006187025,
      "grad_norm": 1.9691967964172363,
      "learning_rate": 2.174799622819425e-05,
      "loss": 0.0719,
      "step": 29500
    },
    {
      "epoch": 13.925814624948442,
      "grad_norm": 1.3086237907409668,
      "learning_rate": 2.1728351406569228e-05,
      "loss": 0.0705,
      "step": 29550
    },
    {
      "epoch": 13.949384243709858,
      "grad_norm": 1.580777883529663,
      "learning_rate": 2.170870658494421e-05,
      "loss": 0.0705,
      "step": 29600
    },
    {
      "epoch": 13.972953862471275,
      "grad_norm": 1.1072410345077515,
      "learning_rate": 2.168906176331919e-05,
      "loss": 0.0692,
      "step": 29650
    },
    {
      "epoch": 13.99652348123269,
      "grad_norm": 2.522480010986328,
      "learning_rate": 2.1669416941694168e-05,
      "loss": 0.0735,
      "step": 29700
    },
    {
      "epoch": 14.01979847975959,
      "grad_norm": 1.8454563617706299,
      "learning_rate": 2.164977212006915e-05,
      "loss": 0.0591,
      "step": 29750
    },
    {
      "epoch": 14.043368098521006,
      "grad_norm": 2.0443291664123535,
      "learning_rate": 2.1630127298444133e-05,
      "loss": 0.0534,
      "step": 29800
    },
    {
      "epoch": 14.066937717282423,
      "grad_norm": 1.627810001373291,
      "learning_rate": 2.1610482476819112e-05,
      "loss": 0.0534,
      "step": 29850
    },
    {
      "epoch": 14.090507336043839,
      "grad_norm": 1.4697682857513428,
      "learning_rate": 2.159083765519409e-05,
      "loss": 0.0577,
      "step": 29900
    },
    {
      "epoch": 14.114076954805256,
      "grad_norm": 1.5521042346954346,
      "learning_rate": 2.1571192833569073e-05,
      "loss": 0.0515,
      "step": 29950
    },
    {
      "epoch": 14.137646573566673,
      "grad_norm": 2.0092334747314453,
      "learning_rate": 2.1551548011944052e-05,
      "loss": 0.0601,
      "step": 30000
    },
    {
      "epoch": 14.137646573566673,
      "eval_loss": 0.2469032108783722,
      "eval_runtime": 169.1832,
      "eval_samples_per_second": 171.967,
      "eval_steps_per_second": 42.995,
      "step": 30000
    },
    {
      "epoch": 14.161216192328089,
      "grad_norm": 1.2243211269378662,
      "learning_rate": 2.153190319031903e-05,
      "loss": 0.0572,
      "step": 30050
    },
    {
      "epoch": 14.184785811089506,
      "grad_norm": 1.9956830739974976,
      "learning_rate": 2.1512258368694014e-05,
      "loss": 0.0541,
      "step": 30100
    },
    {
      "epoch": 14.208355429850922,
      "grad_norm": 0.8385252356529236,
      "learning_rate": 2.1492613547068992e-05,
      "loss": 0.0521,
      "step": 30150
    },
    {
      "epoch": 14.23192504861234,
      "grad_norm": 2.275871515274048,
      "learning_rate": 2.147296872544397e-05,
      "loss": 0.0625,
      "step": 30200
    },
    {
      "epoch": 14.255494667373755,
      "grad_norm": 2.06443452835083,
      "learning_rate": 2.1453323903818954e-05,
      "loss": 0.0603,
      "step": 30250
    },
    {
      "epoch": 14.279064286135172,
      "grad_norm": 1.7109785079956055,
      "learning_rate": 2.1433679082193936e-05,
      "loss": 0.0571,
      "step": 30300
    },
    {
      "epoch": 14.302633904896588,
      "grad_norm": 0.7900126576423645,
      "learning_rate": 2.1414034260568915e-05,
      "loss": 0.0647,
      "step": 30350
    },
    {
      "epoch": 14.326203523658005,
      "grad_norm": 1.8690065145492554,
      "learning_rate": 2.1394389438943894e-05,
      "loss": 0.067,
      "step": 30400
    },
    {
      "epoch": 14.349773142419421,
      "grad_norm": 1.6567492485046387,
      "learning_rate": 2.1374744617318877e-05,
      "loss": 0.0605,
      "step": 30450
    },
    {
      "epoch": 14.373342761180838,
      "grad_norm": 1.671358346939087,
      "learning_rate": 2.1355099795693856e-05,
      "loss": 0.0658,
      "step": 30500
    },
    {
      "epoch": 14.396912379942254,
      "grad_norm": 1.340235710144043,
      "learning_rate": 2.1335454974068835e-05,
      "loss": 0.0702,
      "step": 30550
    },
    {
      "epoch": 14.420481998703671,
      "grad_norm": 2.2152671813964844,
      "learning_rate": 2.1315810152443817e-05,
      "loss": 0.0671,
      "step": 30600
    },
    {
      "epoch": 14.444051617465087,
      "grad_norm": 1.5634492635726929,
      "learning_rate": 2.1296165330818796e-05,
      "loss": 0.0652,
      "step": 30650
    },
    {
      "epoch": 14.467621236226504,
      "grad_norm": 1.811607837677002,
      "learning_rate": 2.127652050919378e-05,
      "loss": 0.0703,
      "step": 30700
    },
    {
      "epoch": 14.49119085498792,
      "grad_norm": 2.0217654705047607,
      "learning_rate": 2.1256875687568757e-05,
      "loss": 0.0677,
      "step": 30750
    },
    {
      "epoch": 14.514760473749337,
      "grad_norm": 1.3917919397354126,
      "learning_rate": 2.123723086594374e-05,
      "loss": 0.0692,
      "step": 30800
    },
    {
      "epoch": 14.538330092510753,
      "grad_norm": 2.2305409908294678,
      "learning_rate": 2.121758604431872e-05,
      "loss": 0.0656,
      "step": 30850
    },
    {
      "epoch": 14.56189971127217,
      "grad_norm": 2.2612783908843994,
      "learning_rate": 2.1197941222693698e-05,
      "loss": 0.0595,
      "step": 30900
    },
    {
      "epoch": 14.585469330033586,
      "grad_norm": 1.6614785194396973,
      "learning_rate": 2.117829640106868e-05,
      "loss": 0.076,
      "step": 30950
    },
    {
      "epoch": 14.609038948795003,
      "grad_norm": 1.9319143295288086,
      "learning_rate": 2.115865157944366e-05,
      "loss": 0.0645,
      "step": 31000
    },
    {
      "epoch": 14.632608567556419,
      "grad_norm": 1.8448723554611206,
      "learning_rate": 2.1139006757818638e-05,
      "loss": 0.0713,
      "step": 31050
    },
    {
      "epoch": 14.656178186317836,
      "grad_norm": 2.4791829586029053,
      "learning_rate": 2.1119361936193617e-05,
      "loss": 0.0633,
      "step": 31100
    },
    {
      "epoch": 14.679747805079252,
      "grad_norm": 6.4475417137146,
      "learning_rate": 2.1099717114568603e-05,
      "loss": 0.0635,
      "step": 31150
    },
    {
      "epoch": 14.70331742384067,
      "grad_norm": 2.2712154388427734,
      "learning_rate": 2.1080072292943582e-05,
      "loss": 0.0693,
      "step": 31200
    },
    {
      "epoch": 14.726887042602087,
      "grad_norm": 2.173983573913574,
      "learning_rate": 2.106042747131856e-05,
      "loss": 0.0722,
      "step": 31250
    },
    {
      "epoch": 14.750456661363502,
      "grad_norm": 3.0865423679351807,
      "learning_rate": 2.1040782649693543e-05,
      "loss": 0.0639,
      "step": 31300
    },
    {
      "epoch": 14.77402628012492,
      "grad_norm": 1.7627928256988525,
      "learning_rate": 2.1021137828068522e-05,
      "loss": 0.0698,
      "step": 31350
    },
    {
      "epoch": 14.797595898886335,
      "grad_norm": 2.1232566833496094,
      "learning_rate": 2.10014930064435e-05,
      "loss": 0.0658,
      "step": 31400
    },
    {
      "epoch": 14.821165517647753,
      "grad_norm": 2.1676971912384033,
      "learning_rate": 2.098184818481848e-05,
      "loss": 0.0669,
      "step": 31450
    },
    {
      "epoch": 14.844735136409168,
      "grad_norm": 2.9679653644561768,
      "learning_rate": 2.0962596259625963e-05,
      "loss": 0.0684,
      "step": 31500
    },
    {
      "epoch": 14.868304755170586,
      "grad_norm": 6.555601596832275,
      "learning_rate": 2.0942951438000942e-05,
      "loss": 0.0672,
      "step": 31550
    },
    {
      "epoch": 14.891874373932001,
      "grad_norm": 1.2236876487731934,
      "learning_rate": 2.0923306616375925e-05,
      "loss": 0.0705,
      "step": 31600
    },
    {
      "epoch": 14.915443992693419,
      "grad_norm": 3.0867130756378174,
      "learning_rate": 2.0903661794750904e-05,
      "loss": 0.0655,
      "step": 31650
    },
    {
      "epoch": 14.939013611454834,
      "grad_norm": 1.781619906425476,
      "learning_rate": 2.0884016973125883e-05,
      "loss": 0.0632,
      "step": 31700
    },
    {
      "epoch": 14.962583230216252,
      "grad_norm": 2.4520256519317627,
      "learning_rate": 2.0864372151500865e-05,
      "loss": 0.064,
      "step": 31750
    },
    {
      "epoch": 14.986152848977667,
      "grad_norm": 1.1588702201843262,
      "learning_rate": 2.0844727329875847e-05,
      "loss": 0.0708,
      "step": 31800
    },
    {
      "epoch": 15.009427847504567,
      "grad_norm": 2.0350096225738525,
      "learning_rate": 2.0825082508250826e-05,
      "loss": 0.0545,
      "step": 31850
    },
    {
      "epoch": 15.032997466265984,
      "grad_norm": 0.6925041675567627,
      "learning_rate": 2.0805437686625805e-05,
      "loss": 0.0544,
      "step": 31900
    },
    {
      "epoch": 15.0565670850274,
      "grad_norm": 1.335410714149475,
      "learning_rate": 2.0785792865000788e-05,
      "loss": 0.0507,
      "step": 31950
    },
    {
      "epoch": 15.080136703788817,
      "grad_norm": 2.0003576278686523,
      "learning_rate": 2.0766148043375767e-05,
      "loss": 0.0541,
      "step": 32000
    },
    {
      "epoch": 15.103706322550233,
      "grad_norm": 1.538203477859497,
      "learning_rate": 2.0746503221750746e-05,
      "loss": 0.056,
      "step": 32050
    },
    {
      "epoch": 15.12727594131165,
      "grad_norm": 2.1168622970581055,
      "learning_rate": 2.0726858400125728e-05,
      "loss": 0.0526,
      "step": 32100
    },
    {
      "epoch": 15.150845560073066,
      "grad_norm": 2.2773919105529785,
      "learning_rate": 2.0707213578500707e-05,
      "loss": 0.0593,
      "step": 32150
    },
    {
      "epoch": 15.174415178834483,
      "grad_norm": 2.190957546234131,
      "learning_rate": 2.0687568756875686e-05,
      "loss": 0.0495,
      "step": 32200
    },
    {
      "epoch": 15.197984797595899,
      "grad_norm": 1.4199811220169067,
      "learning_rate": 2.066792393525067e-05,
      "loss": 0.0552,
      "step": 32250
    },
    {
      "epoch": 15.221554416357316,
      "grad_norm": 2.4098758697509766,
      "learning_rate": 2.064827911362565e-05,
      "loss": 0.0494,
      "step": 32300
    },
    {
      "epoch": 15.245124035118732,
      "grad_norm": 2.137576103210449,
      "learning_rate": 2.062863429200063e-05,
      "loss": 0.0561,
      "step": 32350
    },
    {
      "epoch": 15.268693653880149,
      "grad_norm": 2.1071178913116455,
      "learning_rate": 2.060898947037561e-05,
      "loss": 0.0652,
      "step": 32400
    },
    {
      "epoch": 15.292263272641565,
      "grad_norm": 1.979694128036499,
      "learning_rate": 2.058934464875059e-05,
      "loss": 0.0531,
      "step": 32450
    },
    {
      "epoch": 15.315832891402982,
      "grad_norm": 2.010307788848877,
      "learning_rate": 2.056969982712557e-05,
      "loss": 0.0577,
      "step": 32500
    },
    {
      "epoch": 15.339402510164398,
      "grad_norm": 2.5112247467041016,
      "learning_rate": 2.055005500550055e-05,
      "loss": 0.0644,
      "step": 32550
    },
    {
      "epoch": 15.362972128925815,
      "grad_norm": 1.5705915689468384,
      "learning_rate": 2.0530410183875528e-05,
      "loss": 0.0573,
      "step": 32600
    },
    {
      "epoch": 15.38654174768723,
      "grad_norm": 1.4013748168945312,
      "learning_rate": 2.051076536225051e-05,
      "loss": 0.0633,
      "step": 32650
    },
    {
      "epoch": 15.410111366448648,
      "grad_norm": 1.7422791719436646,
      "learning_rate": 2.0491120540625493e-05,
      "loss": 0.0575,
      "step": 32700
    },
    {
      "epoch": 15.433680985210064,
      "grad_norm": 1.7992230653762817,
      "learning_rate": 2.0471475719000472e-05,
      "loss": 0.0595,
      "step": 32750
    },
    {
      "epoch": 15.457250603971481,
      "grad_norm": 1.3503865003585815,
      "learning_rate": 2.0451830897375454e-05,
      "loss": 0.0594,
      "step": 32800
    },
    {
      "epoch": 15.480820222732897,
      "grad_norm": 1.8458906412124634,
      "learning_rate": 2.0432186075750433e-05,
      "loss": 0.0629,
      "step": 32850
    },
    {
      "epoch": 15.504389841494314,
      "grad_norm": 2.822770833969116,
      "learning_rate": 2.0412541254125412e-05,
      "loss": 0.0605,
      "step": 32900
    },
    {
      "epoch": 15.52795946025573,
      "grad_norm": 2.8281939029693604,
      "learning_rate": 2.0392896432500395e-05,
      "loss": 0.0585,
      "step": 32950
    },
    {
      "epoch": 15.551529079017147,
      "grad_norm": 1.94772469997406,
      "learning_rate": 2.0373251610875374e-05,
      "loss": 0.0601,
      "step": 33000
    },
    {
      "epoch": 15.575098697778564,
      "grad_norm": 1.5771797895431519,
      "learning_rate": 2.0353606789250353e-05,
      "loss": 0.071,
      "step": 33050
    },
    {
      "epoch": 15.59866831653998,
      "grad_norm": 2.1692028045654297,
      "learning_rate": 2.033396196762533e-05,
      "loss": 0.0573,
      "step": 33100
    },
    {
      "epoch": 15.622237935301396,
      "grad_norm": 1.97735595703125,
      "learning_rate": 2.0314317146000317e-05,
      "loss": 0.0595,
      "step": 33150
    },
    {
      "epoch": 15.645807554062813,
      "grad_norm": 1.426501989364624,
      "learning_rate": 2.0294672324375296e-05,
      "loss": 0.0635,
      "step": 33200
    },
    {
      "epoch": 15.66937717282423,
      "grad_norm": 1.8593939542770386,
      "learning_rate": 2.0275027502750275e-05,
      "loss": 0.0615,
      "step": 33250
    },
    {
      "epoch": 15.692946791585646,
      "grad_norm": 2.507883310317993,
      "learning_rate": 2.0255382681125258e-05,
      "loss": 0.067,
      "step": 33300
    },
    {
      "epoch": 15.716516410347063,
      "grad_norm": 4.182543754577637,
      "learning_rate": 2.0235737859500237e-05,
      "loss": 0.0573,
      "step": 33350
    },
    {
      "epoch": 15.740086029108479,
      "grad_norm": 1.4103446006774902,
      "learning_rate": 2.0216093037875216e-05,
      "loss": 0.0606,
      "step": 33400
    },
    {
      "epoch": 15.763655647869896,
      "grad_norm": 2.086851119995117,
      "learning_rate": 2.0196448216250195e-05,
      "loss": 0.0646,
      "step": 33450
    },
    {
      "epoch": 15.787225266631312,
      "grad_norm": 2.483027696609497,
      "learning_rate": 2.0176803394625177e-05,
      "loss": 0.0702,
      "step": 33500
    },
    {
      "epoch": 15.81079488539273,
      "grad_norm": 1.5842112302780151,
      "learning_rate": 2.0157158573000156e-05,
      "loss": 0.064,
      "step": 33550
    },
    {
      "epoch": 15.834364504154145,
      "grad_norm": 2.011039972305298,
      "learning_rate": 2.013751375137514e-05,
      "loss": 0.0624,
      "step": 33600
    },
    {
      "epoch": 15.857934122915562,
      "grad_norm": 1.8188481330871582,
      "learning_rate": 2.011786892975012e-05,
      "loss": 0.0612,
      "step": 33650
    },
    {
      "epoch": 15.881503741676978,
      "grad_norm": 3.398745536804199,
      "learning_rate": 2.00982241081251e-05,
      "loss": 0.0676,
      "step": 33700
    },
    {
      "epoch": 15.905073360438395,
      "grad_norm": 3.0249691009521484,
      "learning_rate": 2.007857928650008e-05,
      "loss": 0.0662,
      "step": 33750
    },
    {
      "epoch": 15.928642979199811,
      "grad_norm": 2.5416147708892822,
      "learning_rate": 2.0058934464875058e-05,
      "loss": 0.0534,
      "step": 33800
    },
    {
      "epoch": 15.952212597961228,
      "grad_norm": 2.270805835723877,
      "learning_rate": 2.003968253968254e-05,
      "loss": 0.0637,
      "step": 33850
    },
    {
      "epoch": 15.975782216722644,
      "grad_norm": 2.0239219665527344,
      "learning_rate": 2.002003771805752e-05,
      "loss": 0.0679,
      "step": 33900
    },
    {
      "epoch": 15.999351835484061,
      "grad_norm": 2.4908528327941895,
      "learning_rate": 2.0000392896432502e-05,
      "loss": 0.0677,
      "step": 33950
    },
    {
      "epoch": 16.02262683401096,
      "grad_norm": 0.7528629302978516,
      "learning_rate": 1.998074807480748e-05,
      "loss": 0.0462,
      "step": 34000
    },
    {
      "epoch": 16.046196452772378,
      "grad_norm": 1.8270049095153809,
      "learning_rate": 1.996110325318246e-05,
      "loss": 0.0497,
      "step": 34050
    },
    {
      "epoch": 16.06976607153379,
      "grad_norm": 2.0620670318603516,
      "learning_rate": 1.9941458431557443e-05,
      "loss": 0.0425,
      "step": 34100
    },
    {
      "epoch": 16.09333569029521,
      "grad_norm": 2.2605650424957275,
      "learning_rate": 1.992181360993242e-05,
      "loss": 0.0592,
      "step": 34150
    },
    {
      "epoch": 16.116905309056627,
      "grad_norm": 0.7187871336936951,
      "learning_rate": 1.99021687883074e-05,
      "loss": 0.0465,
      "step": 34200
    },
    {
      "epoch": 16.140474927818044,
      "grad_norm": 1.8710988759994507,
      "learning_rate": 1.9882523966682383e-05,
      "loss": 0.0465,
      "step": 34250
    },
    {
      "epoch": 16.164044546579458,
      "grad_norm": 1.719231128692627,
      "learning_rate": 1.9862879145057365e-05,
      "loss": 0.0522,
      "step": 34300
    },
    {
      "epoch": 16.187614165340875,
      "grad_norm": 1.7978832721710205,
      "learning_rate": 1.9843234323432344e-05,
      "loss": 0.0472,
      "step": 34350
    },
    {
      "epoch": 16.211183784102293,
      "grad_norm": 1.2123581171035767,
      "learning_rate": 1.9823589501807323e-05,
      "loss": 0.0552,
      "step": 34400
    },
    {
      "epoch": 16.23475340286371,
      "grad_norm": 1.6219549179077148,
      "learning_rate": 1.9803944680182306e-05,
      "loss": 0.0534,
      "step": 34450
    },
    {
      "epoch": 16.258323021625124,
      "grad_norm": 1.83018159866333,
      "learning_rate": 1.9784299858557285e-05,
      "loss": 0.0533,
      "step": 34500
    },
    {
      "epoch": 16.28189264038654,
      "grad_norm": 1.5932519435882568,
      "learning_rate": 1.9764655036932264e-05,
      "loss": 0.0577,
      "step": 34550
    },
    {
      "epoch": 16.30546225914796,
      "grad_norm": 2.82877516746521,
      "learning_rate": 1.9745010215307243e-05,
      "loss": 0.0561,
      "step": 34600
    },
    {
      "epoch": 16.329031877909376,
      "grad_norm": 1.7735759019851685,
      "learning_rate": 1.9725365393682225e-05,
      "loss": 0.057,
      "step": 34650
    },
    {
      "epoch": 16.35260149667079,
      "grad_norm": 4.801385879516602,
      "learning_rate": 1.9705720572057207e-05,
      "loss": 0.0589,
      "step": 34700
    },
    {
      "epoch": 16.376171115432207,
      "grad_norm": 2.2209134101867676,
      "learning_rate": 1.9686075750432186e-05,
      "loss": 0.0578,
      "step": 34750
    },
    {
      "epoch": 16.399740734193625,
      "grad_norm": 2.576892375946045,
      "learning_rate": 1.966643092880717e-05,
      "loss": 0.0511,
      "step": 34800
    },
    {
      "epoch": 16.423310352955042,
      "grad_norm": 1.2792104482650757,
      "learning_rate": 1.9646786107182148e-05,
      "loss": 0.0502,
      "step": 34850
    },
    {
      "epoch": 16.446879971716456,
      "grad_norm": 1.7283493280410767,
      "learning_rate": 1.962753418198963e-05,
      "loss": 0.0603,
      "step": 34900
    },
    {
      "epoch": 16.470449590477873,
      "grad_norm": 1.5405930280685425,
      "learning_rate": 1.960788936036461e-05,
      "loss": 0.061,
      "step": 34950
    },
    {
      "epoch": 16.49401920923929,
      "grad_norm": 2.5554897785186768,
      "learning_rate": 1.958824453873959e-05,
      "loss": 0.0606,
      "step": 35000
    },
    {
      "epoch": 16.49401920923929,
      "eval_loss": 0.24800144135951996,
      "eval_runtime": 168.6831,
      "eval_samples_per_second": 172.477,
      "eval_steps_per_second": 43.122,
      "step": 35000
    },
    {
      "epoch": 16.517588828000708,
      "grad_norm": 2.152705430984497,
      "learning_rate": 1.9568599717114568e-05,
      "loss": 0.0489,
      "step": 35050
    },
    {
      "epoch": 16.541158446762125,
      "grad_norm": 1.9045591354370117,
      "learning_rate": 1.954895489548955e-05,
      "loss": 0.0545,
      "step": 35100
    },
    {
      "epoch": 16.56472806552354,
      "grad_norm": 1.4611809253692627,
      "learning_rate": 1.952931007386453e-05,
      "loss": 0.055,
      "step": 35150
    },
    {
      "epoch": 16.588297684284957,
      "grad_norm": 1.8792468309402466,
      "learning_rate": 1.9509665252239508e-05,
      "loss": 0.0577,
      "step": 35200
    },
    {
      "epoch": 16.611867303046374,
      "grad_norm": 1.7897980213165283,
      "learning_rate": 1.949002043061449e-05,
      "loss": 0.0521,
      "step": 35250
    },
    {
      "epoch": 16.63543692180779,
      "grad_norm": 2.7153007984161377,
      "learning_rate": 1.9470375608989473e-05,
      "loss": 0.0626,
      "step": 35300
    },
    {
      "epoch": 16.659006540569205,
      "grad_norm": 1.114316701889038,
      "learning_rate": 1.9450730787364452e-05,
      "loss": 0.0596,
      "step": 35350
    },
    {
      "epoch": 16.682576159330623,
      "grad_norm": 2.558426856994629,
      "learning_rate": 1.943108596573943e-05,
      "loss": 0.0584,
      "step": 35400
    },
    {
      "epoch": 16.70614577809204,
      "grad_norm": 1.957410454750061,
      "learning_rate": 1.9411441144114413e-05,
      "loss": 0.0604,
      "step": 35450
    },
    {
      "epoch": 16.729715396853457,
      "grad_norm": 1.2288516759872437,
      "learning_rate": 1.9391796322489392e-05,
      "loss": 0.0558,
      "step": 35500
    },
    {
      "epoch": 16.75328501561487,
      "grad_norm": 1.6941676139831543,
      "learning_rate": 1.937215150086437e-05,
      "loss": 0.0532,
      "step": 35550
    },
    {
      "epoch": 16.77685463437629,
      "grad_norm": 1.1747692823410034,
      "learning_rate": 1.9352506679239354e-05,
      "loss": 0.0604,
      "step": 35600
    },
    {
      "epoch": 16.800424253137706,
      "grad_norm": 2.1386289596557617,
      "learning_rate": 1.9332861857614333e-05,
      "loss": 0.0596,
      "step": 35650
    },
    {
      "epoch": 16.823993871899123,
      "grad_norm": 2.629098653793335,
      "learning_rate": 1.931321703598931e-05,
      "loss": 0.0676,
      "step": 35700
    },
    {
      "epoch": 16.847563490660537,
      "grad_norm": 2.775158643722534,
      "learning_rate": 1.9293572214364294e-05,
      "loss": 0.0621,
      "step": 35750
    },
    {
      "epoch": 16.871133109421955,
      "grad_norm": 2.1643166542053223,
      "learning_rate": 1.9273927392739276e-05,
      "loss": 0.0586,
      "step": 35800
    },
    {
      "epoch": 16.894702728183372,
      "grad_norm": 1.5254887342453003,
      "learning_rate": 1.9254282571114255e-05,
      "loss": 0.0597,
      "step": 35850
    },
    {
      "epoch": 16.91827234694479,
      "grad_norm": 2.391526937484741,
      "learning_rate": 1.9234637749489234e-05,
      "loss": 0.0609,
      "step": 35900
    },
    {
      "epoch": 16.941841965706203,
      "grad_norm": 1.3990774154663086,
      "learning_rate": 1.9214992927864217e-05,
      "loss": 0.0637,
      "step": 35950
    },
    {
      "epoch": 16.96541158446762,
      "grad_norm": 1.8917690515518188,
      "learning_rate": 1.9195348106239196e-05,
      "loss": 0.0638,
      "step": 36000
    },
    {
      "epoch": 16.988981203229038,
      "grad_norm": 1.1893905401229858,
      "learning_rate": 1.9175703284614175e-05,
      "loss": 0.0571,
      "step": 36050
    },
    {
      "epoch": 17.012256201755935,
      "grad_norm": 2.3415098190307617,
      "learning_rate": 1.9156058462989157e-05,
      "loss": 0.0548,
      "step": 36100
    },
    {
      "epoch": 17.035825820517353,
      "grad_norm": 1.9223192930221558,
      "learning_rate": 1.9136413641364136e-05,
      "loss": 0.0451,
      "step": 36150
    },
    {
      "epoch": 17.05939543927877,
      "grad_norm": 1.844771146774292,
      "learning_rate": 1.911676881973912e-05,
      "loss": 0.0444,
      "step": 36200
    },
    {
      "epoch": 17.082965058040188,
      "grad_norm": 1.2347795963287354,
      "learning_rate": 1.9097123998114098e-05,
      "loss": 0.0455,
      "step": 36250
    },
    {
      "epoch": 17.1065346768016,
      "grad_norm": 1.6087515354156494,
      "learning_rate": 1.907747917648908e-05,
      "loss": 0.0553,
      "step": 36300
    },
    {
      "epoch": 17.13010429556302,
      "grad_norm": 1.722839593887329,
      "learning_rate": 1.905783435486406e-05,
      "loss": 0.051,
      "step": 36350
    },
    {
      "epoch": 17.153673914324436,
      "grad_norm": 1.3934539556503296,
      "learning_rate": 1.9038189533239038e-05,
      "loss": 0.0469,
      "step": 36400
    },
    {
      "epoch": 17.177243533085853,
      "grad_norm": 2.0877749919891357,
      "learning_rate": 1.901854471161402e-05,
      "loss": 0.0492,
      "step": 36450
    },
    {
      "epoch": 17.200813151847267,
      "grad_norm": 1.4505406618118286,
      "learning_rate": 1.8998899889989e-05,
      "loss": 0.0464,
      "step": 36500
    },
    {
      "epoch": 17.224382770608685,
      "grad_norm": 2.119464874267578,
      "learning_rate": 1.8979255068363978e-05,
      "loss": 0.0508,
      "step": 36550
    },
    {
      "epoch": 17.247952389370102,
      "grad_norm": 1.5783730745315552,
      "learning_rate": 1.8959610246738957e-05,
      "loss": 0.051,
      "step": 36600
    },
    {
      "epoch": 17.27152200813152,
      "grad_norm": 1.2342486381530762,
      "learning_rate": 1.8939965425113943e-05,
      "loss": 0.0504,
      "step": 36650
    },
    {
      "epoch": 17.295091626892933,
      "grad_norm": 2.3535592555999756,
      "learning_rate": 1.8920320603488922e-05,
      "loss": 0.0504,
      "step": 36700
    },
    {
      "epoch": 17.31866124565435,
      "grad_norm": 1.4970391988754272,
      "learning_rate": 1.89006757818639e-05,
      "loss": 0.0527,
      "step": 36750
    },
    {
      "epoch": 17.342230864415768,
      "grad_norm": 1.9293144941329956,
      "learning_rate": 1.8881030960238883e-05,
      "loss": 0.0496,
      "step": 36800
    },
    {
      "epoch": 17.365800483177185,
      "grad_norm": 2.9291493892669678,
      "learning_rate": 1.8861386138613862e-05,
      "loss": 0.0604,
      "step": 36850
    },
    {
      "epoch": 17.389370101938603,
      "grad_norm": 1.4651459455490112,
      "learning_rate": 1.884174131698884e-05,
      "loss": 0.0612,
      "step": 36900
    },
    {
      "epoch": 17.412939720700017,
      "grad_norm": 1.485892653465271,
      "learning_rate": 1.882209649536382e-05,
      "loss": 0.0518,
      "step": 36950
    },
    {
      "epoch": 17.436509339461434,
      "grad_norm": 3.432135820388794,
      "learning_rate": 1.8802451673738803e-05,
      "loss": 0.0484,
      "step": 37000
    },
    {
      "epoch": 17.46007895822285,
      "grad_norm": 2.3429250717163086,
      "learning_rate": 1.8782806852113782e-05,
      "loss": 0.0521,
      "step": 37050
    },
    {
      "epoch": 17.48364857698427,
      "grad_norm": 2.290722370147705,
      "learning_rate": 1.8763162030488764e-05,
      "loss": 0.0482,
      "step": 37100
    },
    {
      "epoch": 17.507218195745683,
      "grad_norm": 2.2158565521240234,
      "learning_rate": 1.8743517208863747e-05,
      "loss": 0.0572,
      "step": 37150
    },
    {
      "epoch": 17.5307878145071,
      "grad_norm": 1.6427574157714844,
      "learning_rate": 1.8723872387238726e-05,
      "loss": 0.0495,
      "step": 37200
    },
    {
      "epoch": 17.554357433268517,
      "grad_norm": 1.3352668285369873,
      "learning_rate": 1.8704227565613705e-05,
      "loss": 0.0469,
      "step": 37250
    },
    {
      "epoch": 17.577927052029935,
      "grad_norm": 1.22696852684021,
      "learning_rate": 1.8684582743988684e-05,
      "loss": 0.0525,
      "step": 37300
    },
    {
      "epoch": 17.60149667079135,
      "grad_norm": 2.1191046237945557,
      "learning_rate": 1.8664937922363666e-05,
      "loss": 0.0474,
      "step": 37350
    },
    {
      "epoch": 17.625066289552766,
      "grad_norm": 6.929384231567383,
      "learning_rate": 1.8645293100738645e-05,
      "loss": 0.0494,
      "step": 37400
    },
    {
      "epoch": 17.648635908314183,
      "grad_norm": 2.2941155433654785,
      "learning_rate": 1.8625648279113624e-05,
      "loss": 0.0551,
      "step": 37450
    },
    {
      "epoch": 17.6722055270756,
      "grad_norm": 2.136302947998047,
      "learning_rate": 1.8606003457488606e-05,
      "loss": 0.0507,
      "step": 37500
    },
    {
      "epoch": 17.695775145837015,
      "grad_norm": 1.8433083295822144,
      "learning_rate": 1.858635863586359e-05,
      "loss": 0.0535,
      "step": 37550
    },
    {
      "epoch": 17.719344764598432,
      "grad_norm": 1.2749398946762085,
      "learning_rate": 1.8566713814238568e-05,
      "loss": 0.0575,
      "step": 37600
    },
    {
      "epoch": 17.74291438335985,
      "grad_norm": 1.2194231748580933,
      "learning_rate": 1.854706899261355e-05,
      "loss": 0.0569,
      "step": 37650
    },
    {
      "epoch": 17.766484002121267,
      "grad_norm": 1.7094519138336182,
      "learning_rate": 1.852742417098853e-05,
      "loss": 0.053,
      "step": 37700
    },
    {
      "epoch": 17.79005362088268,
      "grad_norm": 1.6223914623260498,
      "learning_rate": 1.8507779349363508e-05,
      "loss": 0.0556,
      "step": 37750
    },
    {
      "epoch": 17.813623239644098,
      "grad_norm": 1.9360899925231934,
      "learning_rate": 1.8488134527738487e-05,
      "loss": 0.0561,
      "step": 37800
    },
    {
      "epoch": 17.837192858405515,
      "grad_norm": 1.6458455324172974,
      "learning_rate": 1.846848970611347e-05,
      "loss": 0.0619,
      "step": 37850
    },
    {
      "epoch": 17.860762477166933,
      "grad_norm": 2.913945436477661,
      "learning_rate": 1.844884488448845e-05,
      "loss": 0.0523,
      "step": 37900
    },
    {
      "epoch": 17.884332095928347,
      "grad_norm": 2.734929084777832,
      "learning_rate": 1.8429200062863427e-05,
      "loss": 0.0517,
      "step": 37950
    },
    {
      "epoch": 17.907901714689764,
      "grad_norm": 2.8626582622528076,
      "learning_rate": 1.8409555241238413e-05,
      "loss": 0.0582,
      "step": 38000
    },
    {
      "epoch": 17.93147133345118,
      "grad_norm": 2.251193046569824,
      "learning_rate": 1.8389910419613392e-05,
      "loss": 0.0577,
      "step": 38050
    },
    {
      "epoch": 17.9550409522126,
      "grad_norm": 1.2099615335464478,
      "learning_rate": 1.837026559798837e-05,
      "loss": 0.0586,
      "step": 38100
    },
    {
      "epoch": 17.978610570974013,
      "grad_norm": 1.3439651727676392,
      "learning_rate": 1.835062077636335e-05,
      "loss": 0.06,
      "step": 38150
    },
    {
      "epoch": 18.001885569500914,
      "grad_norm": 1.8944423198699951,
      "learning_rate": 1.8330975954738333e-05,
      "loss": 0.0525,
      "step": 38200
    },
    {
      "epoch": 18.02545518826233,
      "grad_norm": 1.9031260013580322,
      "learning_rate": 1.831133113311331e-05,
      "loss": 0.0409,
      "step": 38250
    },
    {
      "epoch": 18.049024807023745,
      "grad_norm": 1.0563918352127075,
      "learning_rate": 1.829168631148829e-05,
      "loss": 0.0443,
      "step": 38300
    },
    {
      "epoch": 18.072594425785162,
      "grad_norm": 1.4480037689208984,
      "learning_rate": 1.8272041489863273e-05,
      "loss": 0.0412,
      "step": 38350
    },
    {
      "epoch": 18.09616404454658,
      "grad_norm": 1.7772204875946045,
      "learning_rate": 1.8252396668238252e-05,
      "loss": 0.045,
      "step": 38400
    },
    {
      "epoch": 18.119733663307997,
      "grad_norm": 0.915127694606781,
      "learning_rate": 1.8232751846613234e-05,
      "loss": 0.0474,
      "step": 38450
    },
    {
      "epoch": 18.14330328206941,
      "grad_norm": 1.2620089054107666,
      "learning_rate": 1.8213107024988213e-05,
      "loss": 0.0401,
      "step": 38500
    },
    {
      "epoch": 18.16687290083083,
      "grad_norm": 1.7058695554733276,
      "learning_rate": 1.8193462203363196e-05,
      "loss": 0.0488,
      "step": 38550
    },
    {
      "epoch": 18.190442519592246,
      "grad_norm": 1.955370306968689,
      "learning_rate": 1.8173817381738175e-05,
      "loss": 0.0469,
      "step": 38600
    },
    {
      "epoch": 18.214012138353663,
      "grad_norm": 1.8946685791015625,
      "learning_rate": 1.8154172560113154e-05,
      "loss": 0.0509,
      "step": 38650
    },
    {
      "epoch": 18.23758175711508,
      "grad_norm": 2.569127082824707,
      "learning_rate": 1.8134527738488136e-05,
      "loss": 0.0469,
      "step": 38700
    },
    {
      "epoch": 18.261151375876494,
      "grad_norm": 1.7561720609664917,
      "learning_rate": 1.8114882916863115e-05,
      "loss": 0.0521,
      "step": 38750
    },
    {
      "epoch": 18.28472099463791,
      "grad_norm": 1.3750215768814087,
      "learning_rate": 1.8095238095238094e-05,
      "loss": 0.0472,
      "step": 38800
    },
    {
      "epoch": 18.30829061339933,
      "grad_norm": 1.6637789011001587,
      "learning_rate": 1.8075593273613076e-05,
      "loss": 0.0465,
      "step": 38850
    },
    {
      "epoch": 18.331860232160746,
      "grad_norm": 2.8928186893463135,
      "learning_rate": 1.805594845198806e-05,
      "loss": 0.0492,
      "step": 38900
    },
    {
      "epoch": 18.35542985092216,
      "grad_norm": 1.2481369972229004,
      "learning_rate": 1.8036303630363038e-05,
      "loss": 0.0493,
      "step": 38950
    },
    {
      "epoch": 18.378999469683578,
      "grad_norm": 0.9627656936645508,
      "learning_rate": 1.8016658808738017e-05,
      "loss": 0.0566,
      "step": 39000
    },
    {
      "epoch": 18.402569088444995,
      "grad_norm": 0.8107165098190308,
      "learning_rate": 1.7997013987113e-05,
      "loss": 0.0486,
      "step": 39050
    },
    {
      "epoch": 18.426138707206412,
      "grad_norm": 1.017141342163086,
      "learning_rate": 1.7977369165487978e-05,
      "loss": 0.0517,
      "step": 39100
    },
    {
      "epoch": 18.449708325967826,
      "grad_norm": 2.6374104022979736,
      "learning_rate": 1.7957724343862957e-05,
      "loss": 0.0417,
      "step": 39150
    },
    {
      "epoch": 18.473277944729244,
      "grad_norm": 1.830755591392517,
      "learning_rate": 1.793807952223794e-05,
      "loss": 0.0527,
      "step": 39200
    },
    {
      "epoch": 18.49684756349066,
      "grad_norm": 0.6322799324989319,
      "learning_rate": 1.791843470061292e-05,
      "loss": 0.0497,
      "step": 39250
    },
    {
      "epoch": 18.52041718225208,
      "grad_norm": 1.4640212059020996,
      "learning_rate": 1.7898789878987897e-05,
      "loss": 0.0431,
      "step": 39300
    },
    {
      "epoch": 18.543986801013492,
      "grad_norm": 1.845713496208191,
      "learning_rate": 1.787914505736288e-05,
      "loss": 0.0554,
      "step": 39350
    },
    {
      "epoch": 18.56755641977491,
      "grad_norm": 0.6973305344581604,
      "learning_rate": 1.7859500235737862e-05,
      "loss": 0.0521,
      "step": 39400
    },
    {
      "epoch": 18.591126038536327,
      "grad_norm": 3.121795892715454,
      "learning_rate": 1.783985541411284e-05,
      "loss": 0.0515,
      "step": 39450
    },
    {
      "epoch": 18.614695657297744,
      "grad_norm": 1.3635351657867432,
      "learning_rate": 1.782021059248782e-05,
      "loss": 0.0504,
      "step": 39500
    },
    {
      "epoch": 18.63826527605916,
      "grad_norm": 1.8721518516540527,
      "learning_rate": 1.7800565770862803e-05,
      "loss": 0.0524,
      "step": 39550
    },
    {
      "epoch": 18.661834894820576,
      "grad_norm": 1.9710332155227661,
      "learning_rate": 1.778092094923778e-05,
      "loss": 0.0549,
      "step": 39600
    },
    {
      "epoch": 18.685404513581993,
      "grad_norm": 0.692185640335083,
      "learning_rate": 1.776127612761276e-05,
      "loss": 0.0526,
      "step": 39650
    },
    {
      "epoch": 18.70897413234341,
      "grad_norm": 1.5729341506958008,
      "learning_rate": 1.774163130598774e-05,
      "loss": 0.05,
      "step": 39700
    },
    {
      "epoch": 18.732543751104824,
      "grad_norm": 2.548231840133667,
      "learning_rate": 1.7721986484362722e-05,
      "loss": 0.0583,
      "step": 39750
    },
    {
      "epoch": 18.75611336986624,
      "grad_norm": 2.4970595836639404,
      "learning_rate": 1.7702341662737704e-05,
      "loss": 0.051,
      "step": 39800
    },
    {
      "epoch": 18.77968298862766,
      "grad_norm": 1.8166872262954712,
      "learning_rate": 1.7682696841112683e-05,
      "loss": 0.0527,
      "step": 39850
    },
    {
      "epoch": 18.803252607389076,
      "grad_norm": 1.2618423700332642,
      "learning_rate": 1.7663052019487666e-05,
      "loss": 0.0573,
      "step": 39900
    },
    {
      "epoch": 18.82682222615049,
      "grad_norm": 1.9645678997039795,
      "learning_rate": 1.7643407197862645e-05,
      "loss": 0.0521,
      "step": 39950
    },
    {
      "epoch": 18.850391844911908,
      "grad_norm": 2.781019687652588,
      "learning_rate": 1.7623762376237624e-05,
      "loss": 0.0533,
      "step": 40000
    },
    {
      "epoch": 18.850391844911908,
      "eval_loss": 0.25146374106407166,
      "eval_runtime": 169.9528,
      "eval_samples_per_second": 171.189,
      "eval_steps_per_second": 42.8,
      "step": 40000
    },
    {
      "epoch": 18.873961463673325,
      "grad_norm": 1.7410223484039307,
      "learning_rate": 1.7604117554612603e-05,
      "loss": 0.0494,
      "step": 40050
    },
    {
      "epoch": 18.897531082434742,
      "grad_norm": 1.8919063806533813,
      "learning_rate": 1.7584472732987585e-05,
      "loss": 0.0592,
      "step": 40100
    },
    {
      "epoch": 18.92110070119616,
      "grad_norm": 1.526118516921997,
      "learning_rate": 1.7564827911362564e-05,
      "loss": 0.0453,
      "step": 40150
    },
    {
      "epoch": 18.944670319957574,
      "grad_norm": 1.4522255659103394,
      "learning_rate": 1.7545183089737543e-05,
      "loss": 0.0514,
      "step": 40200
    },
    {
      "epoch": 18.96823993871899,
      "grad_norm": 1.1158405542373657,
      "learning_rate": 1.752553826811253e-05,
      "loss": 0.0473,
      "step": 40250
    },
    {
      "epoch": 18.99180955748041,
      "grad_norm": 1.6249018907546997,
      "learning_rate": 1.7505893446487508e-05,
      "loss": 0.0506,
      "step": 40300
    },
    {
      "epoch": 19.015084556007306,
      "grad_norm": 1.4422929286956787,
      "learning_rate": 1.7486248624862487e-05,
      "loss": 0.0442,
      "step": 40350
    },
    {
      "epoch": 19.038654174768723,
      "grad_norm": 1.8190398216247559,
      "learning_rate": 1.746660380323747e-05,
      "loss": 0.0441,
      "step": 40400
    },
    {
      "epoch": 19.06222379353014,
      "grad_norm": 0.9475623965263367,
      "learning_rate": 1.7446958981612448e-05,
      "loss": 0.0404,
      "step": 40450
    },
    {
      "epoch": 19.085793412291554,
      "grad_norm": 1.3897374868392944,
      "learning_rate": 1.7427314159987427e-05,
      "loss": 0.0385,
      "step": 40500
    },
    {
      "epoch": 19.109363031052972,
      "grad_norm": 1.1120264530181885,
      "learning_rate": 1.7407669338362406e-05,
      "loss": 0.0439,
      "step": 40550
    },
    {
      "epoch": 19.13293264981439,
      "grad_norm": 2.0423600673675537,
      "learning_rate": 1.738802451673739e-05,
      "loss": 0.0447,
      "step": 40600
    },
    {
      "epoch": 19.156502268575807,
      "grad_norm": 1.6924744844436646,
      "learning_rate": 1.7368379695112368e-05,
      "loss": 0.0428,
      "step": 40650
    },
    {
      "epoch": 19.180071887337224,
      "grad_norm": 1.7821370363235474,
      "learning_rate": 1.734873487348735e-05,
      "loss": 0.044,
      "step": 40700
    },
    {
      "epoch": 19.203641506098638,
      "grad_norm": 2.2165651321411133,
      "learning_rate": 1.7329090051862332e-05,
      "loss": 0.0416,
      "step": 40750
    },
    {
      "epoch": 19.227211124860055,
      "grad_norm": 1.849232792854309,
      "learning_rate": 1.730944523023731e-05,
      "loss": 0.0401,
      "step": 40800
    },
    {
      "epoch": 19.250780743621473,
      "grad_norm": 1.0960569381713867,
      "learning_rate": 1.728980040861229e-05,
      "loss": 0.0413,
      "step": 40850
    },
    {
      "epoch": 19.27435036238289,
      "grad_norm": 1.6317238807678223,
      "learning_rate": 1.727015558698727e-05,
      "loss": 0.0474,
      "step": 40900
    },
    {
      "epoch": 19.297919981144304,
      "grad_norm": 1.783454418182373,
      "learning_rate": 1.725051076536225e-05,
      "loss": 0.0493,
      "step": 40950
    },
    {
      "epoch": 19.32148959990572,
      "grad_norm": 0.7343422770500183,
      "learning_rate": 1.723086594373723e-05,
      "loss": 0.0511,
      "step": 41000
    },
    {
      "epoch": 19.34505921866714,
      "grad_norm": 2.0042858123779297,
      "learning_rate": 1.721122112211221e-05,
      "loss": 0.0423,
      "step": 41050
    },
    {
      "epoch": 19.368628837428556,
      "grad_norm": 1.5944429636001587,
      "learning_rate": 1.7191576300487192e-05,
      "loss": 0.0438,
      "step": 41100
    },
    {
      "epoch": 19.39219845618997,
      "grad_norm": 2.3620598316192627,
      "learning_rate": 1.7171931478862174e-05,
      "loss": 0.0456,
      "step": 41150
    },
    {
      "epoch": 19.415768074951387,
      "grad_norm": 2.1466403007507324,
      "learning_rate": 1.7152286657237153e-05,
      "loss": 0.0493,
      "step": 41200
    },
    {
      "epoch": 19.439337693712805,
      "grad_norm": 1.590977668762207,
      "learning_rate": 1.7132641835612132e-05,
      "loss": 0.0513,
      "step": 41250
    },
    {
      "epoch": 19.462907312474222,
      "grad_norm": 1.7550644874572754,
      "learning_rate": 1.7112997013987115e-05,
      "loss": 0.0452,
      "step": 41300
    },
    {
      "epoch": 19.486476931235636,
      "grad_norm": 1.8532265424728394,
      "learning_rate": 1.7093352192362094e-05,
      "loss": 0.0499,
      "step": 41350
    },
    {
      "epoch": 19.510046549997053,
      "grad_norm": 3.5978379249572754,
      "learning_rate": 1.7073707370737073e-05,
      "loss": 0.0402,
      "step": 41400
    },
    {
      "epoch": 19.53361616875847,
      "grad_norm": 1.3873523473739624,
      "learning_rate": 1.7054062549112055e-05,
      "loss": 0.0465,
      "step": 41450
    },
    {
      "epoch": 19.557185787519888,
      "grad_norm": 1.3705021142959595,
      "learning_rate": 1.7034417727487034e-05,
      "loss": 0.0413,
      "step": 41500
    },
    {
      "epoch": 19.580755406281302,
      "grad_norm": 1.377471685409546,
      "learning_rate": 1.7014772905862013e-05,
      "loss": 0.0488,
      "step": 41550
    },
    {
      "epoch": 19.60432502504272,
      "grad_norm": 1.1599719524383545,
      "learning_rate": 1.6995520980669496e-05,
      "loss": 0.0427,
      "step": 41600
    },
    {
      "epoch": 19.627894643804137,
      "grad_norm": 1.4143729209899902,
      "learning_rate": 1.6975876159044475e-05,
      "loss": 0.0476,
      "step": 41650
    },
    {
      "epoch": 19.651464262565554,
      "grad_norm": 1.752387285232544,
      "learning_rate": 1.6956231337419454e-05,
      "loss": 0.0498,
      "step": 41700
    },
    {
      "epoch": 19.675033881326968,
      "grad_norm": 1.842256784439087,
      "learning_rate": 1.6936586515794437e-05,
      "loss": 0.0462,
      "step": 41750
    },
    {
      "epoch": 19.698603500088385,
      "grad_norm": 1.4762204885482788,
      "learning_rate": 1.691694169416942e-05,
      "loss": 0.0452,
      "step": 41800
    },
    {
      "epoch": 19.722173118849803,
      "grad_norm": 1.5268675088882446,
      "learning_rate": 1.6897296872544398e-05,
      "loss": 0.0478,
      "step": 41850
    },
    {
      "epoch": 19.74574273761122,
      "grad_norm": 1.9915531873703003,
      "learning_rate": 1.687765205091938e-05,
      "loss": 0.0532,
      "step": 41900
    },
    {
      "epoch": 19.769312356372637,
      "grad_norm": 1.747208595275879,
      "learning_rate": 1.685800722929436e-05,
      "loss": 0.0519,
      "step": 41950
    },
    {
      "epoch": 19.79288197513405,
      "grad_norm": 1.7322090864181519,
      "learning_rate": 1.6838362407669338e-05,
      "loss": 0.0477,
      "step": 42000
    },
    {
      "epoch": 19.81645159389547,
      "grad_norm": 1.41934335231781,
      "learning_rate": 1.6818717586044317e-05,
      "loss": 0.0537,
      "step": 42050
    },
    {
      "epoch": 19.840021212656886,
      "grad_norm": 1.1504496335983276,
      "learning_rate": 1.67990727644193e-05,
      "loss": 0.0519,
      "step": 42100
    },
    {
      "epoch": 19.863590831418303,
      "grad_norm": 1.6698583364486694,
      "learning_rate": 1.677942794279428e-05,
      "loss": 0.0536,
      "step": 42150
    },
    {
      "epoch": 19.887160450179717,
      "grad_norm": 1.551295280456543,
      "learning_rate": 1.6759783121169258e-05,
      "loss": 0.0503,
      "step": 42200
    },
    {
      "epoch": 19.910730068941135,
      "grad_norm": 1.1587804555892944,
      "learning_rate": 1.6740138299544243e-05,
      "loss": 0.0475,
      "step": 42250
    },
    {
      "epoch": 19.934299687702552,
      "grad_norm": 2.2545902729034424,
      "learning_rate": 1.6720493477919222e-05,
      "loss": 0.0457,
      "step": 42300
    },
    {
      "epoch": 19.95786930646397,
      "grad_norm": 2.310986280441284,
      "learning_rate": 1.67008486562942e-05,
      "loss": 0.0461,
      "step": 42350
    },
    {
      "epoch": 19.981438925225383,
      "grad_norm": 1.8672131299972534,
      "learning_rate": 1.668120383466918e-05,
      "loss": 0.0518,
      "step": 42400
    },
    {
      "epoch": 20.004713923752284,
      "grad_norm": 2.028771162033081,
      "learning_rate": 1.6661559013044163e-05,
      "loss": 0.0454,
      "step": 42450
    },
    {
      "epoch": 20.0282835425137,
      "grad_norm": 1.4379243850708008,
      "learning_rate": 1.6641914191419142e-05,
      "loss": 0.0383,
      "step": 42500
    },
    {
      "epoch": 20.051853161275115,
      "grad_norm": 1.864031434059143,
      "learning_rate": 1.6622662266226625e-05,
      "loss": 0.0416,
      "step": 42550
    },
    {
      "epoch": 20.075422780036533,
      "grad_norm": 1.2797328233718872,
      "learning_rate": 1.6603017444601604e-05,
      "loss": 0.0451,
      "step": 42600
    },
    {
      "epoch": 20.09899239879795,
      "grad_norm": 2.8067476749420166,
      "learning_rate": 1.6583372622976583e-05,
      "loss": 0.0456,
      "step": 42650
    },
    {
      "epoch": 20.122562017559368,
      "grad_norm": 2.5325818061828613,
      "learning_rate": 1.6563727801351565e-05,
      "loss": 0.0445,
      "step": 42700
    },
    {
      "epoch": 20.14613163632078,
      "grad_norm": 1.0252395868301392,
      "learning_rate": 1.6544082979726544e-05,
      "loss": 0.0381,
      "step": 42750
    },
    {
      "epoch": 20.1697012550822,
      "grad_norm": 1.3574508428573608,
      "learning_rate": 1.6524438158101523e-05,
      "loss": 0.0438,
      "step": 42800
    },
    {
      "epoch": 20.193270873843616,
      "grad_norm": 1.2216824293136597,
      "learning_rate": 1.6504793336476502e-05,
      "loss": 0.0398,
      "step": 42850
    },
    {
      "epoch": 20.216840492605034,
      "grad_norm": 1.3006831407546997,
      "learning_rate": 1.6485148514851488e-05,
      "loss": 0.0432,
      "step": 42900
    },
    {
      "epoch": 20.240410111366447,
      "grad_norm": 1.0372411012649536,
      "learning_rate": 1.6465503693226467e-05,
      "loss": 0.0406,
      "step": 42950
    },
    {
      "epoch": 20.263979730127865,
      "grad_norm": 2.6403379440307617,
      "learning_rate": 1.6445858871601446e-05,
      "loss": 0.0459,
      "step": 43000
    },
    {
      "epoch": 20.287549348889282,
      "grad_norm": 2.223121166229248,
      "learning_rate": 1.6426214049976428e-05,
      "loss": 0.0426,
      "step": 43050
    },
    {
      "epoch": 20.3111189676507,
      "grad_norm": 1.5344808101654053,
      "learning_rate": 1.6406569228351407e-05,
      "loss": 0.0487,
      "step": 43100
    },
    {
      "epoch": 20.334688586412113,
      "grad_norm": 1.9355673789978027,
      "learning_rate": 1.6386924406726386e-05,
      "loss": 0.0428,
      "step": 43150
    },
    {
      "epoch": 20.35825820517353,
      "grad_norm": 1.8425039052963257,
      "learning_rate": 1.6367279585101365e-05,
      "loss": 0.0488,
      "step": 43200
    },
    {
      "epoch": 20.381827823934948,
      "grad_norm": 2.168114185333252,
      "learning_rate": 1.6347634763476348e-05,
      "loss": 0.0392,
      "step": 43250
    },
    {
      "epoch": 20.405397442696366,
      "grad_norm": 1.5624350309371948,
      "learning_rate": 1.6327989941851327e-05,
      "loss": 0.0415,
      "step": 43300
    },
    {
      "epoch": 20.42896706145778,
      "grad_norm": 1.290467619895935,
      "learning_rate": 1.630834512022631e-05,
      "loss": 0.0472,
      "step": 43350
    },
    {
      "epoch": 20.452536680219197,
      "grad_norm": 1.3528211116790771,
      "learning_rate": 1.628870029860129e-05,
      "loss": 0.0452,
      "step": 43400
    },
    {
      "epoch": 20.476106298980614,
      "grad_norm": 1.6105592250823975,
      "learning_rate": 1.626905547697627e-05,
      "loss": 0.0426,
      "step": 43450
    },
    {
      "epoch": 20.49967591774203,
      "grad_norm": 1.5074416399002075,
      "learning_rate": 1.624941065535125e-05,
      "loss": 0.049,
      "step": 43500
    },
    {
      "epoch": 20.523245536503445,
      "grad_norm": 2.6920688152313232,
      "learning_rate": 1.622976583372623e-05,
      "loss": 0.0442,
      "step": 43550
    },
    {
      "epoch": 20.546815155264863,
      "grad_norm": 1.2991541624069214,
      "learning_rate": 1.621012101210121e-05,
      "loss": 0.0452,
      "step": 43600
    },
    {
      "epoch": 20.57038477402628,
      "grad_norm": 1.355735182762146,
      "learning_rate": 1.619047619047619e-05,
      "loss": 0.0461,
      "step": 43650
    },
    {
      "epoch": 20.593954392787698,
      "grad_norm": 2.3893208503723145,
      "learning_rate": 1.617083136885117e-05,
      "loss": 0.0432,
      "step": 43700
    },
    {
      "epoch": 20.617524011549115,
      "grad_norm": 1.3564894199371338,
      "learning_rate": 1.615118654722615e-05,
      "loss": 0.0386,
      "step": 43750
    },
    {
      "epoch": 20.64109363031053,
      "grad_norm": 1.0827847719192505,
      "learning_rate": 1.6131541725601133e-05,
      "loss": 0.0493,
      "step": 43800
    },
    {
      "epoch": 20.664663249071946,
      "grad_norm": 1.9275667667388916,
      "learning_rate": 1.6111896903976112e-05,
      "loss": 0.0473,
      "step": 43850
    },
    {
      "epoch": 20.688232867833364,
      "grad_norm": 0.6324502229690552,
      "learning_rate": 1.6092252082351095e-05,
      "loss": 0.046,
      "step": 43900
    },
    {
      "epoch": 20.71180248659478,
      "grad_norm": 1.4174916744232178,
      "learning_rate": 1.6072607260726074e-05,
      "loss": 0.0432,
      "step": 43950
    },
    {
      "epoch": 20.735372105356195,
      "grad_norm": 2.516275644302368,
      "learning_rate": 1.6052962439101053e-05,
      "loss": 0.0528,
      "step": 44000
    },
    {
      "epoch": 20.758941724117612,
      "grad_norm": 1.0812885761260986,
      "learning_rate": 1.6033317617476032e-05,
      "loss": 0.0426,
      "step": 44050
    },
    {
      "epoch": 20.78251134287903,
      "grad_norm": 1.424685001373291,
      "learning_rate": 1.6013672795851014e-05,
      "loss": 0.0461,
      "step": 44100
    },
    {
      "epoch": 20.806080961640447,
      "grad_norm": 1.4297910928726196,
      "learning_rate": 1.5994420870658494e-05,
      "loss": 0.0487,
      "step": 44150
    },
    {
      "epoch": 20.82965058040186,
      "grad_norm": 2.3553144931793213,
      "learning_rate": 1.5974776049033476e-05,
      "loss": 0.0404,
      "step": 44200
    },
    {
      "epoch": 20.853220199163278,
      "grad_norm": 3.5270957946777344,
      "learning_rate": 1.5955131227408455e-05,
      "loss": 0.0431,
      "step": 44250
    },
    {
      "epoch": 20.876789817924696,
      "grad_norm": 1.948843240737915,
      "learning_rate": 1.5935486405783434e-05,
      "loss": 0.0495,
      "step": 44300
    },
    {
      "epoch": 20.900359436686113,
      "grad_norm": 1.4241825342178345,
      "learning_rate": 1.5915841584158413e-05,
      "loss": 0.0429,
      "step": 44350
    },
    {
      "epoch": 20.923929055447527,
      "grad_norm": 2.3927018642425537,
      "learning_rate": 1.58961967625334e-05,
      "loss": 0.0513,
      "step": 44400
    },
    {
      "epoch": 20.947498674208944,
      "grad_norm": 1.8296433687210083,
      "learning_rate": 1.5876551940908378e-05,
      "loss": 0.049,
      "step": 44450
    },
    {
      "epoch": 20.97106829297036,
      "grad_norm": 0.7597060203552246,
      "learning_rate": 1.5856907119283357e-05,
      "loss": 0.05,
      "step": 44500
    },
    {
      "epoch": 20.99463791173178,
      "grad_norm": 1.8584263324737549,
      "learning_rate": 1.583726229765834e-05,
      "loss": 0.047,
      "step": 44550
    },
    {
      "epoch": 21.017912910258676,
      "grad_norm": 1.021321415901184,
      "learning_rate": 1.5817617476033318e-05,
      "loss": 0.0396,
      "step": 44600
    },
    {
      "epoch": 21.041482529020094,
      "grad_norm": 0.8038949966430664,
      "learning_rate": 1.5797972654408297e-05,
      "loss": 0.0398,
      "step": 44650
    },
    {
      "epoch": 21.06505214778151,
      "grad_norm": 0.9907049536705017,
      "learning_rate": 1.577832783278328e-05,
      "loss": 0.0355,
      "step": 44700
    },
    {
      "epoch": 21.088621766542925,
      "grad_norm": 1.0699964761734009,
      "learning_rate": 1.575868301115826e-05,
      "loss": 0.0374,
      "step": 44750
    },
    {
      "epoch": 21.112191385304342,
      "grad_norm": 2.1220905780792236,
      "learning_rate": 1.5739038189533238e-05,
      "loss": 0.0405,
      "step": 44800
    },
    {
      "epoch": 21.13576100406576,
      "grad_norm": 1.4682961702346802,
      "learning_rate": 1.571939336790822e-05,
      "loss": 0.0375,
      "step": 44850
    },
    {
      "epoch": 21.159330622827177,
      "grad_norm": 1.6994831562042236,
      "learning_rate": 1.5699748546283202e-05,
      "loss": 0.042,
      "step": 44900
    },
    {
      "epoch": 21.18290024158859,
      "grad_norm": 1.1973017454147339,
      "learning_rate": 1.568010372465818e-05,
      "loss": 0.0398,
      "step": 44950
    },
    {
      "epoch": 21.20646986035001,
      "grad_norm": 0.6943427324295044,
      "learning_rate": 1.566045890303316e-05,
      "loss": 0.0376,
      "step": 45000
    },
    {
      "epoch": 21.20646986035001,
      "eval_loss": 0.260189414024353,
      "eval_runtime": 169.0855,
      "eval_samples_per_second": 172.067,
      "eval_steps_per_second": 43.02,
      "step": 45000
    },
    {
      "epoch": 21.230039479111426,
      "grad_norm": 0.6161876916885376,
      "learning_rate": 1.5640814081408143e-05,
      "loss": 0.0387,
      "step": 45050
    },
    {
      "epoch": 21.253609097872843,
      "grad_norm": 1.2312577962875366,
      "learning_rate": 1.5621169259783122e-05,
      "loss": 0.0357,
      "step": 45100
    },
    {
      "epoch": 21.277178716634257,
      "grad_norm": 2.5953118801116943,
      "learning_rate": 1.56015244381581e-05,
      "loss": 0.0422,
      "step": 45150
    },
    {
      "epoch": 21.300748335395674,
      "grad_norm": 0.7354553937911987,
      "learning_rate": 1.558187961653308e-05,
      "loss": 0.041,
      "step": 45200
    },
    {
      "epoch": 21.32431795415709,
      "grad_norm": 1.0748766660690308,
      "learning_rate": 1.5562234794908062e-05,
      "loss": 0.0387,
      "step": 45250
    },
    {
      "epoch": 21.34788757291851,
      "grad_norm": 1.6378167867660522,
      "learning_rate": 1.5542589973283045e-05,
      "loss": 0.0427,
      "step": 45300
    },
    {
      "epoch": 21.371457191679923,
      "grad_norm": 1.574714183807373,
      "learning_rate": 1.5522945151658024e-05,
      "loss": 0.0408,
      "step": 45350
    },
    {
      "epoch": 21.39502681044134,
      "grad_norm": 1.9701769351959229,
      "learning_rate": 1.5503300330033006e-05,
      "loss": 0.0426,
      "step": 45400
    },
    {
      "epoch": 21.418596429202758,
      "grad_norm": 1.6513797044754028,
      "learning_rate": 1.5483655508407985e-05,
      "loss": 0.0382,
      "step": 45450
    },
    {
      "epoch": 21.442166047964175,
      "grad_norm": 1.7235337495803833,
      "learning_rate": 1.5464010686782964e-05,
      "loss": 0.0483,
      "step": 45500
    },
    {
      "epoch": 21.46573566672559,
      "grad_norm": 0.9246681332588196,
      "learning_rate": 1.5444365865157943e-05,
      "loss": 0.0368,
      "step": 45550
    },
    {
      "epoch": 21.489305285487006,
      "grad_norm": 3.6498184204101562,
      "learning_rate": 1.5424721043532925e-05,
      "loss": 0.0407,
      "step": 45600
    },
    {
      "epoch": 21.512874904248424,
      "grad_norm": 1.6782569885253906,
      "learning_rate": 1.5405076221907904e-05,
      "loss": 0.0439,
      "step": 45650
    },
    {
      "epoch": 21.53644452300984,
      "grad_norm": 1.4816454648971558,
      "learning_rate": 1.5385431400282883e-05,
      "loss": 0.0421,
      "step": 45700
    },
    {
      "epoch": 21.56001414177126,
      "grad_norm": 1.3580158948898315,
      "learning_rate": 1.536578657865787e-05,
      "loss": 0.0455,
      "step": 45750
    },
    {
      "epoch": 21.583583760532672,
      "grad_norm": 1.497481346130371,
      "learning_rate": 1.5346141757032848e-05,
      "loss": 0.0426,
      "step": 45800
    },
    {
      "epoch": 21.60715337929409,
      "grad_norm": 1.51191246509552,
      "learning_rate": 1.5326496935407827e-05,
      "loss": 0.0467,
      "step": 45850
    },
    {
      "epoch": 21.630722998055507,
      "grad_norm": 1.1822878122329712,
      "learning_rate": 1.5306852113782806e-05,
      "loss": 0.0468,
      "step": 45900
    },
    {
      "epoch": 21.654292616816925,
      "grad_norm": 1.7335307598114014,
      "learning_rate": 1.528720729215779e-05,
      "loss": 0.0484,
      "step": 45950
    },
    {
      "epoch": 21.67786223557834,
      "grad_norm": 1.5107407569885254,
      "learning_rate": 1.5267562470532767e-05,
      "loss": 0.0391,
      "step": 46000
    },
    {
      "epoch": 21.701431854339756,
      "grad_norm": 1.827491283416748,
      "learning_rate": 1.5247917648907748e-05,
      "loss": 0.0421,
      "step": 46050
    },
    {
      "epoch": 21.725001473101173,
      "grad_norm": 0.6612588763237,
      "learning_rate": 1.5228272827282727e-05,
      "loss": 0.0456,
      "step": 46100
    },
    {
      "epoch": 21.74857109186259,
      "grad_norm": 1.8277132511138916,
      "learning_rate": 1.5208628005657708e-05,
      "loss": 0.0476,
      "step": 46150
    },
    {
      "epoch": 21.772140710624004,
      "grad_norm": 1.389266014099121,
      "learning_rate": 1.518898318403269e-05,
      "loss": 0.0441,
      "step": 46200
    },
    {
      "epoch": 21.79571032938542,
      "grad_norm": 1.251495122909546,
      "learning_rate": 1.516933836240767e-05,
      "loss": 0.0397,
      "step": 46250
    },
    {
      "epoch": 21.81927994814684,
      "grad_norm": 1.750279188156128,
      "learning_rate": 1.5149693540782652e-05,
      "loss": 0.046,
      "step": 46300
    },
    {
      "epoch": 21.842849566908257,
      "grad_norm": 1.0411005020141602,
      "learning_rate": 1.513004871915763e-05,
      "loss": 0.046,
      "step": 46350
    },
    {
      "epoch": 21.86641918566967,
      "grad_norm": 0.7947952151298523,
      "learning_rate": 1.5110403897532611e-05,
      "loss": 0.0466,
      "step": 46400
    },
    {
      "epoch": 21.889988804431088,
      "grad_norm": 1.4666411876678467,
      "learning_rate": 1.509075907590759e-05,
      "loss": 0.0435,
      "step": 46450
    },
    {
      "epoch": 21.913558423192505,
      "grad_norm": 1.190801978111267,
      "learning_rate": 1.5071114254282571e-05,
      "loss": 0.0521,
      "step": 46500
    },
    {
      "epoch": 21.937128041953923,
      "grad_norm": 2.014005184173584,
      "learning_rate": 1.5051469432657552e-05,
      "loss": 0.0474,
      "step": 46550
    },
    {
      "epoch": 21.960697660715336,
      "grad_norm": 3.708712577819824,
      "learning_rate": 1.503182461103253e-05,
      "loss": 0.0461,
      "step": 46600
    },
    {
      "epoch": 21.984267279476754,
      "grad_norm": 2.7436881065368652,
      "learning_rate": 1.5012179789407515e-05,
      "loss": 0.0476,
      "step": 46650
    },
    {
      "epoch": 22.007542278003655,
      "grad_norm": 1.267066240310669,
      "learning_rate": 1.4992534967782492e-05,
      "loss": 0.0404,
      "step": 46700
    },
    {
      "epoch": 22.03111189676507,
      "grad_norm": 2.289111852645874,
      "learning_rate": 1.4972890146157474e-05,
      "loss": 0.0364,
      "step": 46750
    },
    {
      "epoch": 22.054681515526486,
      "grad_norm": 1.3388376235961914,
      "learning_rate": 1.4953245324532453e-05,
      "loss": 0.0377,
      "step": 46800
    },
    {
      "epoch": 22.078251134287903,
      "grad_norm": 1.1360636949539185,
      "learning_rate": 1.4933600502907434e-05,
      "loss": 0.0358,
      "step": 46850
    },
    {
      "epoch": 22.10182075304932,
      "grad_norm": 1.55062735080719,
      "learning_rate": 1.4913955681282415e-05,
      "loss": 0.038,
      "step": 46900
    },
    {
      "epoch": 22.125390371810735,
      "grad_norm": 1.4053261280059814,
      "learning_rate": 1.4894310859657394e-05,
      "loss": 0.038,
      "step": 46950
    },
    {
      "epoch": 22.148959990572152,
      "grad_norm": 1.7989203929901123,
      "learning_rate": 1.4874666038032376e-05,
      "loss": 0.036,
      "step": 47000
    },
    {
      "epoch": 22.17252960933357,
      "grad_norm": 1.9345831871032715,
      "learning_rate": 1.4855021216407355e-05,
      "loss": 0.0358,
      "step": 47050
    },
    {
      "epoch": 22.196099228094987,
      "grad_norm": 2.179077386856079,
      "learning_rate": 1.4835376394782336e-05,
      "loss": 0.0373,
      "step": 47100
    },
    {
      "epoch": 22.2196688468564,
      "grad_norm": 1.9311140775680542,
      "learning_rate": 1.4815731573157316e-05,
      "loss": 0.0368,
      "step": 47150
    },
    {
      "epoch": 22.243238465617818,
      "grad_norm": 2.1251659393310547,
      "learning_rate": 1.4796086751532297e-05,
      "loss": 0.044,
      "step": 47200
    },
    {
      "epoch": 22.266808084379235,
      "grad_norm": 1.1541563272476196,
      "learning_rate": 1.4776441929907278e-05,
      "loss": 0.0409,
      "step": 47250
    },
    {
      "epoch": 22.290377703140653,
      "grad_norm": 1.4092272520065308,
      "learning_rate": 1.4756797108282257e-05,
      "loss": 0.0405,
      "step": 47300
    },
    {
      "epoch": 22.313947321902067,
      "grad_norm": 1.4738951921463013,
      "learning_rate": 1.4737152286657237e-05,
      "loss": 0.0401,
      "step": 47350
    },
    {
      "epoch": 22.337516940663484,
      "grad_norm": 0.8274522423744202,
      "learning_rate": 1.4717507465032216e-05,
      "loss": 0.0388,
      "step": 47400
    },
    {
      "epoch": 22.3610865594249,
      "grad_norm": 1.4022668600082397,
      "learning_rate": 1.4697862643407199e-05,
      "loss": 0.038,
      "step": 47450
    },
    {
      "epoch": 22.38465617818632,
      "grad_norm": 1.3701430559158325,
      "learning_rate": 1.467821782178218e-05,
      "loss": 0.0448,
      "step": 47500
    },
    {
      "epoch": 22.408225796947736,
      "grad_norm": 1.7102488279342651,
      "learning_rate": 1.4658573000157159e-05,
      "loss": 0.0397,
      "step": 47550
    },
    {
      "epoch": 22.43179541570915,
      "grad_norm": 0.7402945756912231,
      "learning_rate": 1.463892817853214e-05,
      "loss": 0.0392,
      "step": 47600
    },
    {
      "epoch": 22.455365034470567,
      "grad_norm": 1.3272870779037476,
      "learning_rate": 1.461928335690712e-05,
      "loss": 0.0436,
      "step": 47650
    },
    {
      "epoch": 22.478934653231985,
      "grad_norm": 1.7634259462356567,
      "learning_rate": 1.45996385352821e-05,
      "loss": 0.0436,
      "step": 47700
    },
    {
      "epoch": 22.502504271993402,
      "grad_norm": 0.1948896199464798,
      "learning_rate": 1.4579993713657081e-05,
      "loss": 0.0357,
      "step": 47750
    },
    {
      "epoch": 22.526073890754816,
      "grad_norm": 1.8283579349517822,
      "learning_rate": 1.456034889203206e-05,
      "loss": 0.0373,
      "step": 47800
    },
    {
      "epoch": 22.549643509516233,
      "grad_norm": 2.1284234523773193,
      "learning_rate": 1.4540704070407041e-05,
      "loss": 0.0382,
      "step": 47850
    },
    {
      "epoch": 22.57321312827765,
      "grad_norm": 1.2101819515228271,
      "learning_rate": 1.4521059248782022e-05,
      "loss": 0.0422,
      "step": 47900
    },
    {
      "epoch": 22.596782747039068,
      "grad_norm": 1.8192251920700073,
      "learning_rate": 1.4501414427157002e-05,
      "loss": 0.0364,
      "step": 47950
    },
    {
      "epoch": 22.620352365800482,
      "grad_norm": 1.3736392259597778,
      "learning_rate": 1.4481769605531981e-05,
      "loss": 0.0424,
      "step": 48000
    },
    {
      "epoch": 22.6439219845619,
      "grad_norm": 1.2142072916030884,
      "learning_rate": 1.4462124783906962e-05,
      "loss": 0.0444,
      "step": 48050
    },
    {
      "epoch": 22.667491603323317,
      "grad_norm": 0.9571799039840698,
      "learning_rate": 1.4442479962281944e-05,
      "loss": 0.0377,
      "step": 48100
    },
    {
      "epoch": 22.691061222084734,
      "grad_norm": 1.910054326057434,
      "learning_rate": 1.4422835140656923e-05,
      "loss": 0.0396,
      "step": 48150
    },
    {
      "epoch": 22.714630840846148,
      "grad_norm": 1.2914685010910034,
      "learning_rate": 1.4403190319031904e-05,
      "loss": 0.0398,
      "step": 48200
    },
    {
      "epoch": 22.738200459607565,
      "grad_norm": 0.8758055567741394,
      "learning_rate": 1.4383545497406883e-05,
      "loss": 0.0401,
      "step": 48250
    },
    {
      "epoch": 22.761770078368983,
      "grad_norm": 1.1249866485595703,
      "learning_rate": 1.4363900675781864e-05,
      "loss": 0.0416,
      "step": 48300
    },
    {
      "epoch": 22.7853396971304,
      "grad_norm": 1.9443506002426147,
      "learning_rate": 1.4344255854156844e-05,
      "loss": 0.048,
      "step": 48350
    },
    {
      "epoch": 22.808909315891814,
      "grad_norm": 1.3666431903839111,
      "learning_rate": 1.4324611032531825e-05,
      "loss": 0.0464,
      "step": 48400
    },
    {
      "epoch": 22.83247893465323,
      "grad_norm": 1.8503597974777222,
      "learning_rate": 1.4304966210906806e-05,
      "loss": 0.0436,
      "step": 48450
    },
    {
      "epoch": 22.85604855341465,
      "grad_norm": 1.1387690305709839,
      "learning_rate": 1.4285321389281785e-05,
      "loss": 0.0413,
      "step": 48500
    },
    {
      "epoch": 22.879618172176066,
      "grad_norm": 2.9250311851501465,
      "learning_rate": 1.4265676567656767e-05,
      "loss": 0.0475,
      "step": 48550
    },
    {
      "epoch": 22.90318779093748,
      "grad_norm": 1.9373222589492798,
      "learning_rate": 1.4246031746031746e-05,
      "loss": 0.0468,
      "step": 48600
    },
    {
      "epoch": 22.926757409698897,
      "grad_norm": 2.4974730014801025,
      "learning_rate": 1.4226386924406727e-05,
      "loss": 0.0444,
      "step": 48650
    },
    {
      "epoch": 22.950327028460315,
      "grad_norm": 0.8849188089370728,
      "learning_rate": 1.4206742102781708e-05,
      "loss": 0.0455,
      "step": 48700
    },
    {
      "epoch": 22.973896647221732,
      "grad_norm": 2.0347213745117188,
      "learning_rate": 1.4187097281156687e-05,
      "loss": 0.0445,
      "step": 48750
    },
    {
      "epoch": 22.99746626598315,
      "grad_norm": 1.6004348993301392,
      "learning_rate": 1.4167452459531669e-05,
      "loss": 0.0468,
      "step": 48800
    },
    {
      "epoch": 23.020741264510047,
      "grad_norm": 0.8675183057785034,
      "learning_rate": 1.4147807637906648e-05,
      "loss": 0.0307,
      "step": 48850
    },
    {
      "epoch": 23.044310883271464,
      "grad_norm": 1.1650700569152832,
      "learning_rate": 1.4128162816281629e-05,
      "loss": 0.0347,
      "step": 48900
    },
    {
      "epoch": 23.067880502032878,
      "grad_norm": 1.880763053894043,
      "learning_rate": 1.4108517994656608e-05,
      "loss": 0.0416,
      "step": 48950
    },
    {
      "epoch": 23.091450120794295,
      "grad_norm": 1.0582722425460815,
      "learning_rate": 1.408887317303159e-05,
      "loss": 0.0379,
      "step": 49000
    },
    {
      "epoch": 23.115019739555713,
      "grad_norm": 0.5881329774856567,
      "learning_rate": 1.406922835140657e-05,
      "loss": 0.0308,
      "step": 49050
    },
    {
      "epoch": 23.13858935831713,
      "grad_norm": 1.8892401456832886,
      "learning_rate": 1.404958352978155e-05,
      "loss": 0.0388,
      "step": 49100
    },
    {
      "epoch": 23.162158977078544,
      "grad_norm": 1.4547014236450195,
      "learning_rate": 1.402993870815653e-05,
      "loss": 0.0344,
      "step": 49150
    },
    {
      "epoch": 23.18572859583996,
      "grad_norm": 1.3333466053009033,
      "learning_rate": 1.401029388653151e-05,
      "loss": 0.0371,
      "step": 49200
    },
    {
      "epoch": 23.20929821460138,
      "grad_norm": 1.0674389600753784,
      "learning_rate": 1.3990649064906492e-05,
      "loss": 0.0376,
      "step": 49250
    },
    {
      "epoch": 23.232867833362796,
      "grad_norm": 0.74062180519104,
      "learning_rate": 1.3971004243281472e-05,
      "loss": 0.0375,
      "step": 49300
    },
    {
      "epoch": 23.25643745212421,
      "grad_norm": 1.2130650281906128,
      "learning_rate": 1.3951359421656451e-05,
      "loss": 0.0382,
      "step": 49350
    },
    {
      "epoch": 23.280007070885627,
      "grad_norm": 1.796654224395752,
      "learning_rate": 1.3931714600031432e-05,
      "loss": 0.0404,
      "step": 49400
    },
    {
      "epoch": 23.303576689647045,
      "grad_norm": 1.032279372215271,
      "learning_rate": 1.3912069778406413e-05,
      "loss": 0.0333,
      "step": 49450
    },
    {
      "epoch": 23.327146308408462,
      "grad_norm": 1.8277053833007812,
      "learning_rate": 1.3892424956781393e-05,
      "loss": 0.0363,
      "step": 49500
    },
    {
      "epoch": 23.35071592716988,
      "grad_norm": 1.9547076225280762,
      "learning_rate": 1.3872780135156372e-05,
      "loss": 0.0341,
      "step": 49550
    },
    {
      "epoch": 23.374285545931293,
      "grad_norm": 2.0450267791748047,
      "learning_rate": 1.3853135313531353e-05,
      "loss": 0.0366,
      "step": 49600
    },
    {
      "epoch": 23.39785516469271,
      "grad_norm": 1.0979262590408325,
      "learning_rate": 1.3833490491906334e-05,
      "loss": 0.0374,
      "step": 49650
    },
    {
      "epoch": 23.42142478345413,
      "grad_norm": 1.2350554466247559,
      "learning_rate": 1.3813845670281315e-05,
      "loss": 0.0387,
      "step": 49700
    },
    {
      "epoch": 23.444994402215546,
      "grad_norm": 1.378882884979248,
      "learning_rate": 1.3794200848656295e-05,
      "loss": 0.035,
      "step": 49750
    },
    {
      "epoch": 23.46856402097696,
      "grad_norm": 1.9442421197891235,
      "learning_rate": 1.3774556027031274e-05,
      "loss": 0.0351,
      "step": 49800
    },
    {
      "epoch": 23.492133639738377,
      "grad_norm": 0.7377168536186218,
      "learning_rate": 1.3754911205406255e-05,
      "loss": 0.0345,
      "step": 49850
    },
    {
      "epoch": 23.515703258499794,
      "grad_norm": 1.869111180305481,
      "learning_rate": 1.3735266383781236e-05,
      "loss": 0.0388,
      "step": 49900
    },
    {
      "epoch": 23.53927287726121,
      "grad_norm": 1.5262941122055054,
      "learning_rate": 1.3715621562156216e-05,
      "loss": 0.0402,
      "step": 49950
    },
    {
      "epoch": 23.562842496022625,
      "grad_norm": 1.6875512599945068,
      "learning_rate": 1.3695976740531197e-05,
      "loss": 0.0394,
      "step": 50000
    },
    {
      "epoch": 23.562842496022625,
      "eval_loss": 0.26001837849617004,
      "eval_runtime": 169.0372,
      "eval_samples_per_second": 172.116,
      "eval_steps_per_second": 43.032,
      "step": 50000
    },
    {
      "epoch": 23.586412114784043,
      "grad_norm": 0.8228974342346191,
      "learning_rate": 1.3676331918906176e-05,
      "loss": 0.0335,
      "step": 50050
    },
    {
      "epoch": 23.60998173354546,
      "grad_norm": 2.0722291469573975,
      "learning_rate": 1.3656687097281157e-05,
      "loss": 0.0437,
      "step": 50100
    },
    {
      "epoch": 23.633551352306878,
      "grad_norm": 1.4793362617492676,
      "learning_rate": 1.3637042275656137e-05,
      "loss": 0.04,
      "step": 50150
    },
    {
      "epoch": 23.65712097106829,
      "grad_norm": 1.3822650909423828,
      "learning_rate": 1.3617397454031118e-05,
      "loss": 0.0399,
      "step": 50200
    },
    {
      "epoch": 23.68069058982971,
      "grad_norm": 1.3611146211624146,
      "learning_rate": 1.3597752632406099e-05,
      "loss": 0.0447,
      "step": 50250
    },
    {
      "epoch": 23.704260208591126,
      "grad_norm": 1.2259219884872437,
      "learning_rate": 1.3578107810781078e-05,
      "loss": 0.0379,
      "step": 50300
    },
    {
      "epoch": 23.727829827352544,
      "grad_norm": 1.1544780731201172,
      "learning_rate": 1.3558462989156058e-05,
      "loss": 0.0386,
      "step": 50350
    },
    {
      "epoch": 23.751399446113957,
      "grad_norm": 1.2205290794372559,
      "learning_rate": 1.3538818167531039e-05,
      "loss": 0.0382,
      "step": 50400
    },
    {
      "epoch": 23.774969064875375,
      "grad_norm": 0.3385803699493408,
      "learning_rate": 1.351917334590602e-05,
      "loss": 0.0355,
      "step": 50450
    },
    {
      "epoch": 23.798538683636792,
      "grad_norm": 1.6968806982040405,
      "learning_rate": 1.3499528524281e-05,
      "loss": 0.0384,
      "step": 50500
    },
    {
      "epoch": 23.82210830239821,
      "grad_norm": 1.5387078523635864,
      "learning_rate": 1.347988370265598e-05,
      "loss": 0.0452,
      "step": 50550
    },
    {
      "epoch": 23.845677921159627,
      "grad_norm": 1.4819468259811401,
      "learning_rate": 1.346063177746346e-05,
      "loss": 0.0422,
      "step": 50600
    },
    {
      "epoch": 23.86924753992104,
      "grad_norm": 1.3753430843353271,
      "learning_rate": 1.3440986955838441e-05,
      "loss": 0.0406,
      "step": 50650
    },
    {
      "epoch": 23.89281715868246,
      "grad_norm": 1.638904094696045,
      "learning_rate": 1.342134213421342e-05,
      "loss": 0.0407,
      "step": 50700
    },
    {
      "epoch": 23.916386777443876,
      "grad_norm": 1.7768359184265137,
      "learning_rate": 1.3401697312588401e-05,
      "loss": 0.0422,
      "step": 50750
    },
    {
      "epoch": 23.939956396205293,
      "grad_norm": 2.413559675216675,
      "learning_rate": 1.3382052490963383e-05,
      "loss": 0.0405,
      "step": 50800
    },
    {
      "epoch": 23.963526014966707,
      "grad_norm": 0.7148990035057068,
      "learning_rate": 1.3362407669338362e-05,
      "loss": 0.0449,
      "step": 50850
    },
    {
      "epoch": 23.987095633728124,
      "grad_norm": 1.4910378456115723,
      "learning_rate": 1.3342762847713343e-05,
      "loss": 0.0462,
      "step": 50900
    },
    {
      "epoch": 24.01037063225502,
      "grad_norm": 1.4488767385482788,
      "learning_rate": 1.3323118026088322e-05,
      "loss": 0.0308,
      "step": 50950
    },
    {
      "epoch": 24.03394025101644,
      "grad_norm": 0.9253305792808533,
      "learning_rate": 1.3303473204463305e-05,
      "loss": 0.0283,
      "step": 51000
    },
    {
      "epoch": 24.057509869777856,
      "grad_norm": 0.630499005317688,
      "learning_rate": 1.3283828382838285e-05,
      "loss": 0.0332,
      "step": 51050
    },
    {
      "epoch": 24.081079488539274,
      "grad_norm": 0.904491126537323,
      "learning_rate": 1.3264183561213264e-05,
      "loss": 0.0329,
      "step": 51100
    },
    {
      "epoch": 24.104649107300688,
      "grad_norm": 3.7505853176116943,
      "learning_rate": 1.3244538739588245e-05,
      "loss": 0.0347,
      "step": 51150
    },
    {
      "epoch": 24.128218726062105,
      "grad_norm": 0.3097330331802368,
      "learning_rate": 1.3224893917963224e-05,
      "loss": 0.0356,
      "step": 51200
    },
    {
      "epoch": 24.151788344823522,
      "grad_norm": 2.461804151535034,
      "learning_rate": 1.3205249096338206e-05,
      "loss": 0.0329,
      "step": 51250
    },
    {
      "epoch": 24.17535796358494,
      "grad_norm": 1.5699692964553833,
      "learning_rate": 1.3185604274713185e-05,
      "loss": 0.0302,
      "step": 51300
    },
    {
      "epoch": 24.198927582346357,
      "grad_norm": 1.503682017326355,
      "learning_rate": 1.3165959453088166e-05,
      "loss": 0.0329,
      "step": 51350
    },
    {
      "epoch": 24.22249720110777,
      "grad_norm": 0.894737958908081,
      "learning_rate": 1.3146314631463147e-05,
      "loss": 0.036,
      "step": 51400
    },
    {
      "epoch": 24.24606681986919,
      "grad_norm": 1.6896933317184448,
      "learning_rate": 1.3126669809838127e-05,
      "loss": 0.0343,
      "step": 51450
    },
    {
      "epoch": 24.269636438630606,
      "grad_norm": 1.27558171749115,
      "learning_rate": 1.3107024988213108e-05,
      "loss": 0.0344,
      "step": 51500
    },
    {
      "epoch": 24.293206057392023,
      "grad_norm": 1.7048587799072266,
      "learning_rate": 1.3087380166588087e-05,
      "loss": 0.035,
      "step": 51550
    },
    {
      "epoch": 24.316775676153437,
      "grad_norm": 1.671334147453308,
      "learning_rate": 1.3067735344963068e-05,
      "loss": 0.0397,
      "step": 51600
    },
    {
      "epoch": 24.340345294914854,
      "grad_norm": 1.3771541118621826,
      "learning_rate": 1.3048090523338048e-05,
      "loss": 0.0362,
      "step": 51650
    },
    {
      "epoch": 24.363914913676272,
      "grad_norm": 1.563813328742981,
      "learning_rate": 1.3028445701713029e-05,
      "loss": 0.0309,
      "step": 51700
    },
    {
      "epoch": 24.38748453243769,
      "grad_norm": 1.6104671955108643,
      "learning_rate": 1.300880088008801e-05,
      "loss": 0.0324,
      "step": 51750
    },
    {
      "epoch": 24.411054151199103,
      "grad_norm": 1.625111699104309,
      "learning_rate": 1.2989156058462989e-05,
      "loss": 0.0386,
      "step": 51800
    },
    {
      "epoch": 24.43462376996052,
      "grad_norm": 2.1318747997283936,
      "learning_rate": 1.296951123683797e-05,
      "loss": 0.0379,
      "step": 51850
    },
    {
      "epoch": 24.458193388721938,
      "grad_norm": 1.6895102262496948,
      "learning_rate": 1.294986641521295e-05,
      "loss": 0.039,
      "step": 51900
    },
    {
      "epoch": 24.481763007483355,
      "grad_norm": 1.0495132207870483,
      "learning_rate": 1.293022159358793e-05,
      "loss": 0.0411,
      "step": 51950
    },
    {
      "epoch": 24.50533262624477,
      "grad_norm": 1.464483380317688,
      "learning_rate": 1.2910576771962912e-05,
      "loss": 0.0431,
      "step": 52000
    },
    {
      "epoch": 24.528902245006186,
      "grad_norm": 1.9751315116882324,
      "learning_rate": 1.289093195033789e-05,
      "loss": 0.0349,
      "step": 52050
    },
    {
      "epoch": 24.552471863767604,
      "grad_norm": 0.8953289985656738,
      "learning_rate": 1.2871287128712871e-05,
      "loss": 0.0387,
      "step": 52100
    },
    {
      "epoch": 24.57604148252902,
      "grad_norm": 0.8745422959327698,
      "learning_rate": 1.2851642307087852e-05,
      "loss": 0.0365,
      "step": 52150
    },
    {
      "epoch": 24.599611101290435,
      "grad_norm": 1.9837309122085571,
      "learning_rate": 1.2831997485462833e-05,
      "loss": 0.0383,
      "step": 52200
    },
    {
      "epoch": 24.623180720051852,
      "grad_norm": 0.6648041009902954,
      "learning_rate": 1.2812352663837813e-05,
      "loss": 0.0385,
      "step": 52250
    },
    {
      "epoch": 24.64675033881327,
      "grad_norm": 2.090045213699341,
      "learning_rate": 1.2793100738645295e-05,
      "loss": 0.0399,
      "step": 52300
    },
    {
      "epoch": 24.670319957574687,
      "grad_norm": 1.2457534074783325,
      "learning_rate": 1.2773455917020274e-05,
      "loss": 0.0363,
      "step": 52350
    },
    {
      "epoch": 24.6938895763361,
      "grad_norm": 1.5212512016296387,
      "learning_rate": 1.2753811095395254e-05,
      "loss": 0.0394,
      "step": 52400
    },
    {
      "epoch": 24.71745919509752,
      "grad_norm": 1.501922369003296,
      "learning_rate": 1.2734166273770235e-05,
      "loss": 0.0386,
      "step": 52450
    },
    {
      "epoch": 24.741028813858936,
      "grad_norm": 1.3644353151321411,
      "learning_rate": 1.2714521452145214e-05,
      "loss": 0.0354,
      "step": 52500
    },
    {
      "epoch": 24.764598432620353,
      "grad_norm": 0.8736855387687683,
      "learning_rate": 1.2694876630520196e-05,
      "loss": 0.036,
      "step": 52550
    },
    {
      "epoch": 24.78816805138177,
      "grad_norm": 1.4204262495040894,
      "learning_rate": 1.2675231808895175e-05,
      "loss": 0.0439,
      "step": 52600
    },
    {
      "epoch": 24.811737670143184,
      "grad_norm": 1.3570513725280762,
      "learning_rate": 1.2655586987270156e-05,
      "loss": 0.0399,
      "step": 52650
    },
    {
      "epoch": 24.835307288904602,
      "grad_norm": 1.6248748302459717,
      "learning_rate": 1.2635942165645135e-05,
      "loss": 0.0359,
      "step": 52700
    },
    {
      "epoch": 24.85887690766602,
      "grad_norm": 1.4655731916427612,
      "learning_rate": 1.2616297344020117e-05,
      "loss": 0.0405,
      "step": 52750
    },
    {
      "epoch": 24.882446526427437,
      "grad_norm": 1.9079172611236572,
      "learning_rate": 1.2596652522395098e-05,
      "loss": 0.0385,
      "step": 52800
    },
    {
      "epoch": 24.90601614518885,
      "grad_norm": 1.5662158727645874,
      "learning_rate": 1.2577007700770077e-05,
      "loss": 0.0383,
      "step": 52850
    },
    {
      "epoch": 24.929585763950268,
      "grad_norm": 1.6500329971313477,
      "learning_rate": 1.2557362879145058e-05,
      "loss": 0.0444,
      "step": 52900
    },
    {
      "epoch": 24.953155382711685,
      "grad_norm": 0.424242228269577,
      "learning_rate": 1.2537718057520037e-05,
      "loss": 0.0398,
      "step": 52950
    },
    {
      "epoch": 24.976725001473103,
      "grad_norm": 2.2597811222076416,
      "learning_rate": 1.2518073235895019e-05,
      "loss": 0.0345,
      "step": 53000
    },
    {
      "epoch": 25.0,
      "grad_norm": 1.126697063446045,
      "learning_rate": 1.2498428414269998e-05,
      "loss": 0.0411,
      "step": 53050
    },
    {
      "epoch": 25.023569618761417,
      "grad_norm": 0.9424963593482971,
      "learning_rate": 1.2478783592644979e-05,
      "loss": 0.0293,
      "step": 53100
    },
    {
      "epoch": 25.047139237522835,
      "grad_norm": 3.707092046737671,
      "learning_rate": 1.245913877101996e-05,
      "loss": 0.0325,
      "step": 53150
    },
    {
      "epoch": 25.07070885628425,
      "grad_norm": 1.266019582748413,
      "learning_rate": 1.243949394939494e-05,
      "loss": 0.0283,
      "step": 53200
    },
    {
      "epoch": 25.094278475045666,
      "grad_norm": 1.2080824375152588,
      "learning_rate": 1.2419849127769921e-05,
      "loss": 0.0279,
      "step": 53250
    },
    {
      "epoch": 25.117848093807083,
      "grad_norm": 0.774435818195343,
      "learning_rate": 1.24002043061449e-05,
      "loss": 0.03,
      "step": 53300
    },
    {
      "epoch": 25.1414177125685,
      "grad_norm": 1.386210560798645,
      "learning_rate": 1.238055948451988e-05,
      "loss": 0.032,
      "step": 53350
    },
    {
      "epoch": 25.164987331329915,
      "grad_norm": 1.1516118049621582,
      "learning_rate": 1.2360914662894861e-05,
      "loss": 0.0319,
      "step": 53400
    },
    {
      "epoch": 25.188556950091332,
      "grad_norm": 1.4762718677520752,
      "learning_rate": 1.2341269841269842e-05,
      "loss": 0.0353,
      "step": 53450
    },
    {
      "epoch": 25.21212656885275,
      "grad_norm": 1.4317833185195923,
      "learning_rate": 1.2321625019644823e-05,
      "loss": 0.0369,
      "step": 53500
    },
    {
      "epoch": 25.235696187614167,
      "grad_norm": 2.2917377948760986,
      "learning_rate": 1.2301980198019802e-05,
      "loss": 0.036,
      "step": 53550
    },
    {
      "epoch": 25.25926580637558,
      "grad_norm": 4.544863224029541,
      "learning_rate": 1.2282335376394782e-05,
      "loss": 0.0288,
      "step": 53600
    },
    {
      "epoch": 25.282835425136998,
      "grad_norm": 1.1274795532226562,
      "learning_rate": 1.2262690554769763e-05,
      "loss": 0.037,
      "step": 53650
    },
    {
      "epoch": 25.306405043898415,
      "grad_norm": 1.4667587280273438,
      "learning_rate": 1.2243045733144744e-05,
      "loss": 0.033,
      "step": 53700
    },
    {
      "epoch": 25.329974662659833,
      "grad_norm": 1.232116937637329,
      "learning_rate": 1.2223400911519724e-05,
      "loss": 0.0311,
      "step": 53750
    },
    {
      "epoch": 25.353544281421247,
      "grad_norm": 1.0001568794250488,
      "learning_rate": 1.2203756089894703e-05,
      "loss": 0.0417,
      "step": 53800
    },
    {
      "epoch": 25.377113900182664,
      "grad_norm": 1.5304522514343262,
      "learning_rate": 1.2184111268269684e-05,
      "loss": 0.0341,
      "step": 53850
    },
    {
      "epoch": 25.40068351894408,
      "grad_norm": 1.4145374298095703,
      "learning_rate": 1.2164466446644665e-05,
      "loss": 0.0369,
      "step": 53900
    },
    {
      "epoch": 25.4242531377055,
      "grad_norm": 1.5610005855560303,
      "learning_rate": 1.2144821625019645e-05,
      "loss": 0.0357,
      "step": 53950
    },
    {
      "epoch": 25.447822756466913,
      "grad_norm": 1.593166470527649,
      "learning_rate": 1.2125176803394626e-05,
      "loss": 0.0381,
      "step": 54000
    },
    {
      "epoch": 25.47139237522833,
      "grad_norm": 1.9019159078598022,
      "learning_rate": 1.2105531981769605e-05,
      "loss": 0.0339,
      "step": 54050
    },
    {
      "epoch": 25.494961993989747,
      "grad_norm": 1.3689156770706177,
      "learning_rate": 1.2085887160144587e-05,
      "loss": 0.0375,
      "step": 54100
    },
    {
      "epoch": 25.518531612751165,
      "grad_norm": 2.3784680366516113,
      "learning_rate": 1.2066242338519566e-05,
      "loss": 0.0376,
      "step": 54150
    },
    {
      "epoch": 25.54210123151258,
      "grad_norm": 0.929806113243103,
      "learning_rate": 1.2046597516894547e-05,
      "loss": 0.0342,
      "step": 54200
    },
    {
      "epoch": 25.565670850273996,
      "grad_norm": 1.4704393148422241,
      "learning_rate": 1.2026952695269526e-05,
      "loss": 0.0356,
      "step": 54250
    },
    {
      "epoch": 25.589240469035413,
      "grad_norm": 2.740290641784668,
      "learning_rate": 1.2007307873644507e-05,
      "loss": 0.0366,
      "step": 54300
    },
    {
      "epoch": 25.61281008779683,
      "grad_norm": 1.2510807514190674,
      "learning_rate": 1.198766305201949e-05,
      "loss": 0.0336,
      "step": 54350
    },
    {
      "epoch": 25.636379706558245,
      "grad_norm": 1.3378078937530518,
      "learning_rate": 1.1968018230394468e-05,
      "loss": 0.039,
      "step": 54400
    },
    {
      "epoch": 25.659949325319662,
      "grad_norm": 1.6304160356521606,
      "learning_rate": 1.1948373408769449e-05,
      "loss": 0.0313,
      "step": 54450
    },
    {
      "epoch": 25.68351894408108,
      "grad_norm": 1.5844955444335938,
      "learning_rate": 1.1928728587144428e-05,
      "loss": 0.0418,
      "step": 54500
    },
    {
      "epoch": 25.707088562842497,
      "grad_norm": 0.1959112286567688,
      "learning_rate": 1.190908376551941e-05,
      "loss": 0.037,
      "step": 54550
    },
    {
      "epoch": 25.730658181603914,
      "grad_norm": 1.749189019203186,
      "learning_rate": 1.1889438943894391e-05,
      "loss": 0.0369,
      "step": 54600
    },
    {
      "epoch": 25.754227800365328,
      "grad_norm": 1.3092132806777954,
      "learning_rate": 1.186979412226937e-05,
      "loss": 0.0371,
      "step": 54650
    },
    {
      "epoch": 25.777797419126745,
      "grad_norm": 0.9898368120193481,
      "learning_rate": 1.185014930064435e-05,
      "loss": 0.0355,
      "step": 54700
    },
    {
      "epoch": 25.801367037888163,
      "grad_norm": 1.5181679725646973,
      "learning_rate": 1.183050447901933e-05,
      "loss": 0.0306,
      "step": 54750
    },
    {
      "epoch": 25.82493665664958,
      "grad_norm": 1.5032836198806763,
      "learning_rate": 1.1810859657394312e-05,
      "loss": 0.0428,
      "step": 54800
    },
    {
      "epoch": 25.848506275410994,
      "grad_norm": 1.886471152305603,
      "learning_rate": 1.1791214835769291e-05,
      "loss": 0.0385,
      "step": 54850
    },
    {
      "epoch": 25.87207589417241,
      "grad_norm": 0.6602370142936707,
      "learning_rate": 1.1771570014144272e-05,
      "loss": 0.0338,
      "step": 54900
    },
    {
      "epoch": 25.89564551293383,
      "grad_norm": 0.5176483988761902,
      "learning_rate": 1.1751925192519252e-05,
      "loss": 0.0383,
      "step": 54950
    },
    {
      "epoch": 25.919215131695246,
      "grad_norm": 1.4837626218795776,
      "learning_rate": 1.1732280370894233e-05,
      "loss": 0.0407,
      "step": 55000
    },
    {
      "epoch": 25.919215131695246,
      "eval_loss": 0.2631624639034271,
      "eval_runtime": 169.3952,
      "eval_samples_per_second": 171.752,
      "eval_steps_per_second": 42.941,
      "step": 55000
    },
    {
      "epoch": 25.95456955983737,
      "grad_norm": 1.8032766580581665,
      "learning_rate": 1.1712635549269214e-05,
      "loss": 0.0414,
      "step": 55050
    },
    {
      "epoch": 25.978139178598788,
      "grad_norm": 1.5908043384552002,
      "learning_rate": 1.1692990727644193e-05,
      "loss": 0.0362,
      "step": 55100
    },
    {
      "epoch": 26.001885569500914,
      "grad_norm": 1.655976414680481,
      "learning_rate": 1.1673345906019173e-05,
      "loss": 0.0386,
      "step": 55150
    },
    {
      "epoch": 26.02545518826233,
      "grad_norm": 1.8255419731140137,
      "learning_rate": 1.1653701084394154e-05,
      "loss": 0.0284,
      "step": 55200
    },
    {
      "epoch": 26.049024807023745,
      "grad_norm": 0.8576280474662781,
      "learning_rate": 1.1634056262769135e-05,
      "loss": 0.0319,
      "step": 55250
    },
    {
      "epoch": 26.072594425785162,
      "grad_norm": 1.1177407503128052,
      "learning_rate": 1.1614411441144115e-05,
      "loss": 0.028,
      "step": 55300
    },
    {
      "epoch": 26.09616404454658,
      "grad_norm": 1.1980443000793457,
      "learning_rate": 1.1595159515951595e-05,
      "loss": 0.0327,
      "step": 55350
    },
    {
      "epoch": 26.119733663307997,
      "grad_norm": 0.5200539231300354,
      "learning_rate": 1.1575514694326576e-05,
      "loss": 0.0312,
      "step": 55400
    },
    {
      "epoch": 26.14330328206941,
      "grad_norm": 0.7260087132453918,
      "learning_rate": 1.1555869872701556e-05,
      "loss": 0.0327,
      "step": 55450
    },
    {
      "epoch": 26.16687290083083,
      "grad_norm": 0.9934223890304565,
      "learning_rate": 1.1536225051076537e-05,
      "loss": 0.0323,
      "step": 55500
    },
    {
      "epoch": 26.190442519592246,
      "grad_norm": 1.9312514066696167,
      "learning_rate": 1.1516580229451516e-05,
      "loss": 0.0302,
      "step": 55550
    },
    {
      "epoch": 26.214012138353663,
      "grad_norm": 0.5952000617980957,
      "learning_rate": 1.1496935407826497e-05,
      "loss": 0.0365,
      "step": 55600
    },
    {
      "epoch": 26.23758175711508,
      "grad_norm": 1.2103720903396606,
      "learning_rate": 1.1477290586201478e-05,
      "loss": 0.0316,
      "step": 55650
    },
    {
      "epoch": 26.261151375876494,
      "grad_norm": 1.4560346603393555,
      "learning_rate": 1.1457645764576458e-05,
      "loss": 0.0293,
      "step": 55700
    },
    {
      "epoch": 26.28472099463791,
      "grad_norm": 0.9888887405395508,
      "learning_rate": 1.1438000942951439e-05,
      "loss": 0.0319,
      "step": 55750
    },
    {
      "epoch": 26.30829061339933,
      "grad_norm": 4.343812465667725,
      "learning_rate": 1.1418356121326418e-05,
      "loss": 0.033,
      "step": 55800
    },
    {
      "epoch": 26.331860232160746,
      "grad_norm": 1.050270676612854,
      "learning_rate": 1.13987112997014e-05,
      "loss": 0.0308,
      "step": 55850
    },
    {
      "epoch": 26.35542985092216,
      "grad_norm": 1.3647723197937012,
      "learning_rate": 1.137906647807638e-05,
      "loss": 0.0344,
      "step": 55900
    },
    {
      "epoch": 26.378999469683578,
      "grad_norm": 1.8177074193954468,
      "learning_rate": 1.135942165645136e-05,
      "loss": 0.0338,
      "step": 55950
    },
    {
      "epoch": 26.402569088444995,
      "grad_norm": 0.9616228342056274,
      "learning_rate": 1.1339776834826339e-05,
      "loss": 0.0319,
      "step": 56000
    },
    {
      "epoch": 26.426138707206412,
      "grad_norm": 1.1560779809951782,
      "learning_rate": 1.132013201320132e-05,
      "loss": 0.034,
      "step": 56050
    },
    {
      "epoch": 26.449708325967826,
      "grad_norm": 1.4671992063522339,
      "learning_rate": 1.1300487191576302e-05,
      "loss": 0.0338,
      "step": 56100
    },
    {
      "epoch": 26.473277944729244,
      "grad_norm": 0.5254855751991272,
      "learning_rate": 1.1280842369951281e-05,
      "loss": 0.0316,
      "step": 56150
    },
    {
      "epoch": 26.49684756349066,
      "grad_norm": 1.540418028831482,
      "learning_rate": 1.1261197548326262e-05,
      "loss": 0.0313,
      "step": 56200
    },
    {
      "epoch": 26.52041718225208,
      "grad_norm": 0.9259846806526184,
      "learning_rate": 1.124155272670124e-05,
      "loss": 0.0434,
      "step": 56250
    },
    {
      "epoch": 26.543986801013492,
      "grad_norm": 1.3076351881027222,
      "learning_rate": 1.1221907905076223e-05,
      "loss": 0.0321,
      "step": 56300
    },
    {
      "epoch": 26.56755641977491,
      "grad_norm": 1.0886927843093872,
      "learning_rate": 1.1202263083451204e-05,
      "loss": 0.0353,
      "step": 56350
    },
    {
      "epoch": 26.591126038536327,
      "grad_norm": 2.1797847747802734,
      "learning_rate": 1.1182618261826183e-05,
      "loss": 0.0341,
      "step": 56400
    },
    {
      "epoch": 26.614695657297744,
      "grad_norm": 3.472109794616699,
      "learning_rate": 1.1162973440201163e-05,
      "loss": 0.0345,
      "step": 56450
    },
    {
      "epoch": 26.63826527605916,
      "grad_norm": 1.4019469022750854,
      "learning_rate": 1.1143328618576142e-05,
      "loss": 0.0388,
      "step": 56500
    },
    {
      "epoch": 26.661834894820576,
      "grad_norm": 1.380563497543335,
      "learning_rate": 1.1123683796951125e-05,
      "loss": 0.0358,
      "step": 56550
    },
    {
      "epoch": 26.685404513581993,
      "grad_norm": 1.7603956460952759,
      "learning_rate": 1.1104038975326104e-05,
      "loss": 0.0369,
      "step": 56600
    },
    {
      "epoch": 26.70897413234341,
      "grad_norm": 1.7187702655792236,
      "learning_rate": 1.1084394153701084e-05,
      "loss": 0.0419,
      "step": 56650
    },
    {
      "epoch": 26.732543751104824,
      "grad_norm": 1.3223893642425537,
      "learning_rate": 1.1064749332076065e-05,
      "loss": 0.0377,
      "step": 56700
    },
    {
      "epoch": 26.75611336986624,
      "grad_norm": 0.8648626208305359,
      "learning_rate": 1.1045104510451046e-05,
      "loss": 0.0336,
      "step": 56750
    },
    {
      "epoch": 26.77968298862766,
      "grad_norm": 0.49406877160072327,
      "learning_rate": 1.1025459688826027e-05,
      "loss": 0.0366,
      "step": 56800
    },
    {
      "epoch": 26.803252607389076,
      "grad_norm": 0.9806467294692993,
      "learning_rate": 1.1005814867201006e-05,
      "loss": 0.0301,
      "step": 56850
    },
    {
      "epoch": 26.82682222615049,
      "grad_norm": 2.248433828353882,
      "learning_rate": 1.0986170045575986e-05,
      "loss": 0.0384,
      "step": 56900
    },
    {
      "epoch": 26.850391844911908,
      "grad_norm": 0.9742823839187622,
      "learning_rate": 1.0966525223950967e-05,
      "loss": 0.0361,
      "step": 56950
    },
    {
      "epoch": 26.873961463673325,
      "grad_norm": 2.2997868061065674,
      "learning_rate": 1.0946880402325948e-05,
      "loss": 0.0368,
      "step": 57000
    },
    {
      "epoch": 26.897531082434742,
      "grad_norm": 1.997118353843689,
      "learning_rate": 1.0927235580700928e-05,
      "loss": 0.0337,
      "step": 57050
    },
    {
      "epoch": 26.92110070119616,
      "grad_norm": 0.715728223323822,
      "learning_rate": 1.0907590759075907e-05,
      "loss": 0.0385,
      "step": 57100
    },
    {
      "epoch": 26.944670319957574,
      "grad_norm": 1.386444091796875,
      "learning_rate": 1.0887945937450888e-05,
      "loss": 0.036,
      "step": 57150
    },
    {
      "epoch": 26.96823993871899,
      "grad_norm": 1.456841230392456,
      "learning_rate": 1.0868301115825869e-05,
      "loss": 0.0373,
      "step": 57200
    },
    {
      "epoch": 26.99180955748041,
      "grad_norm": 1.2565875053405762,
      "learning_rate": 1.084865629420085e-05,
      "loss": 0.0336,
      "step": 57250
    },
    {
      "epoch": 27.015084556007306,
      "grad_norm": 0.9866294264793396,
      "learning_rate": 1.082901147257583e-05,
      "loss": 0.0288,
      "step": 57300
    },
    {
      "epoch": 27.038654174768723,
      "grad_norm": 1.489958643913269,
      "learning_rate": 1.0809366650950809e-05,
      "loss": 0.0301,
      "step": 57350
    },
    {
      "epoch": 27.06222379353014,
      "grad_norm": 1.2344532012939453,
      "learning_rate": 1.078972182932579e-05,
      "loss": 0.0278,
      "step": 57400
    },
    {
      "epoch": 27.085793412291554,
      "grad_norm": 0.7097308039665222,
      "learning_rate": 1.077007700770077e-05,
      "loss": 0.0242,
      "step": 57450
    },
    {
      "epoch": 27.109363031052972,
      "grad_norm": 1.4414951801300049,
      "learning_rate": 1.0750432186075751e-05,
      "loss": 0.0301,
      "step": 57500
    },
    {
      "epoch": 27.13293264981439,
      "grad_norm": 0.5590474605560303,
      "learning_rate": 1.0730787364450732e-05,
      "loss": 0.0301,
      "step": 57550
    },
    {
      "epoch": 27.156502268575807,
      "grad_norm": 0.5780234932899475,
      "learning_rate": 1.071114254282571e-05,
      "loss": 0.0311,
      "step": 57600
    },
    {
      "epoch": 27.180071887337224,
      "grad_norm": 1.5860008001327515,
      "learning_rate": 1.0691497721200693e-05,
      "loss": 0.0283,
      "step": 57650
    },
    {
      "epoch": 27.203641506098638,
      "grad_norm": 1.137101173400879,
      "learning_rate": 1.0671852899575672e-05,
      "loss": 0.03,
      "step": 57700
    },
    {
      "epoch": 27.227211124860055,
      "grad_norm": 0.8839505910873413,
      "learning_rate": 1.0652208077950653e-05,
      "loss": 0.0305,
      "step": 57750
    },
    {
      "epoch": 27.250780743621473,
      "grad_norm": 2.9170053005218506,
      "learning_rate": 1.0632563256325632e-05,
      "loss": 0.0282,
      "step": 57800
    },
    {
      "epoch": 27.27435036238289,
      "grad_norm": 1.5810500383377075,
      "learning_rate": 1.0612918434700613e-05,
      "loss": 0.0344,
      "step": 57850
    },
    {
      "epoch": 27.297919981144304,
      "grad_norm": 0.9316717386245728,
      "learning_rate": 1.0593273613075595e-05,
      "loss": 0.031,
      "step": 57900
    },
    {
      "epoch": 27.32148959990572,
      "grad_norm": 1.2200965881347656,
      "learning_rate": 1.0573628791450574e-05,
      "loss": 0.0324,
      "step": 57950
    },
    {
      "epoch": 27.34505921866714,
      "grad_norm": 0.9165388941764832,
      "learning_rate": 1.0553983969825555e-05,
      "loss": 0.0353,
      "step": 58000
    },
    {
      "epoch": 27.368628837428556,
      "grad_norm": 1.612657904624939,
      "learning_rate": 1.0534339148200534e-05,
      "loss": 0.037,
      "step": 58050
    },
    {
      "epoch": 27.39219845618997,
      "grad_norm": 1.0488290786743164,
      "learning_rate": 1.0514694326575516e-05,
      "loss": 0.0334,
      "step": 58100
    },
    {
      "epoch": 27.415768074951387,
      "grad_norm": 2.1158761978149414,
      "learning_rate": 1.0495049504950495e-05,
      "loss": 0.0376,
      "step": 58150
    },
    {
      "epoch": 27.439337693712805,
      "grad_norm": 1.845584750175476,
      "learning_rate": 1.0475404683325476e-05,
      "loss": 0.0301,
      "step": 58200
    },
    {
      "epoch": 27.462907312474222,
      "grad_norm": 1.8584954738616943,
      "learning_rate": 1.0455759861700456e-05,
      "loss": 0.0344,
      "step": 58250
    },
    {
      "epoch": 27.486476931235636,
      "grad_norm": 1.6454228162765503,
      "learning_rate": 1.0436115040075435e-05,
      "loss": 0.0356,
      "step": 58300
    },
    {
      "epoch": 27.510046549997053,
      "grad_norm": 1.524294137954712,
      "learning_rate": 1.0416470218450418e-05,
      "loss": 0.0359,
      "step": 58350
    },
    {
      "epoch": 27.53361616875847,
      "grad_norm": 1.1822577714920044,
      "learning_rate": 1.0396825396825397e-05,
      "loss": 0.0293,
      "step": 58400
    },
    {
      "epoch": 27.557185787519888,
      "grad_norm": 2.1649863719940186,
      "learning_rate": 1.0377180575200377e-05,
      "loss": 0.0333,
      "step": 58450
    },
    {
      "epoch": 27.580755406281302,
      "grad_norm": 1.7689273357391357,
      "learning_rate": 1.0357535753575358e-05,
      "loss": 0.0324,
      "step": 58500
    },
    {
      "epoch": 27.60432502504272,
      "grad_norm": 1.2435507774353027,
      "learning_rate": 1.0337890931950339e-05,
      "loss": 0.0331,
      "step": 58550
    },
    {
      "epoch": 27.627894643804137,
      "grad_norm": 3.1550066471099854,
      "learning_rate": 1.031824611032532e-05,
      "loss": 0.0292,
      "step": 58600
    },
    {
      "epoch": 27.651464262565554,
      "grad_norm": 0.3026275634765625,
      "learning_rate": 1.0298601288700298e-05,
      "loss": 0.031,
      "step": 58650
    },
    {
      "epoch": 27.675033881326968,
      "grad_norm": 1.5297733545303345,
      "learning_rate": 1.0278956467075279e-05,
      "loss": 0.0378,
      "step": 58700
    },
    {
      "epoch": 27.698603500088385,
      "grad_norm": 0.9688240885734558,
      "learning_rate": 1.0259311645450258e-05,
      "loss": 0.037,
      "step": 58750
    },
    {
      "epoch": 27.722173118849803,
      "grad_norm": 1.09540593624115,
      "learning_rate": 1.023966682382524e-05,
      "loss": 0.0352,
      "step": 58800
    },
    {
      "epoch": 27.74574273761122,
      "grad_norm": 1.1679222583770752,
      "learning_rate": 1.0220022002200221e-05,
      "loss": 0.0385,
      "step": 58850
    },
    {
      "epoch": 27.769312356372637,
      "grad_norm": 2.426522970199585,
      "learning_rate": 1.02003771805752e-05,
      "loss": 0.0329,
      "step": 58900
    },
    {
      "epoch": 27.79288197513405,
      "grad_norm": 1.0763657093048096,
      "learning_rate": 1.0180732358950181e-05,
      "loss": 0.0282,
      "step": 58950
    },
    {
      "epoch": 27.81645159389547,
      "grad_norm": 2.0076870918273926,
      "learning_rate": 1.0161087537325162e-05,
      "loss": 0.0354,
      "step": 59000
    },
    {
      "epoch": 27.840021212656886,
      "grad_norm": 1.6223740577697754,
      "learning_rate": 1.0141442715700142e-05,
      "loss": 0.0341,
      "step": 59050
    },
    {
      "epoch": 27.863590831418303,
      "grad_norm": 1.1955788135528564,
      "learning_rate": 1.0121797894075123e-05,
      "loss": 0.0404,
      "step": 59100
    },
    {
      "epoch": 27.887160450179717,
      "grad_norm": 1.7389640808105469,
      "learning_rate": 1.0102153072450102e-05,
      "loss": 0.0358,
      "step": 59150
    },
    {
      "epoch": 27.910730068941135,
      "grad_norm": 1.396214246749878,
      "learning_rate": 1.0082508250825083e-05,
      "loss": 0.0366,
      "step": 59200
    },
    {
      "epoch": 27.934299687702552,
      "grad_norm": 1.7968906164169312,
      "learning_rate": 1.0063256325632564e-05,
      "loss": 0.0347,
      "step": 59250
    },
    {
      "epoch": 27.95786930646397,
      "grad_norm": 1.7153980731964111,
      "learning_rate": 1.0043611504007545e-05,
      "loss": 0.0364,
      "step": 59300
    },
    {
      "epoch": 27.981438925225383,
      "grad_norm": 1.1381335258483887,
      "learning_rate": 1.0023966682382524e-05,
      "loss": 0.0367,
      "step": 59350
    },
    {
      "epoch": 28.004713923752284,
      "grad_norm": 0.9805035591125488,
      "learning_rate": 1.0004321860757504e-05,
      "loss": 0.0358,
      "step": 59400
    },
    {
      "epoch": 28.0282835425137,
      "grad_norm": 1.3969006538391113,
      "learning_rate": 9.984677039132485e-06,
      "loss": 0.0236,
      "step": 59450
    },
    {
      "epoch": 28.051853161275115,
      "grad_norm": 0.9101859331130981,
      "learning_rate": 9.965032217507466e-06,
      "loss": 0.0261,
      "step": 59500
    },
    {
      "epoch": 28.075422780036533,
      "grad_norm": 0.8985884785652161,
      "learning_rate": 9.945387395882445e-06,
      "loss": 0.0251,
      "step": 59550
    },
    {
      "epoch": 28.09899239879795,
      "grad_norm": 1.9132826328277588,
      "learning_rate": 9.925742574257425e-06,
      "loss": 0.0227,
      "step": 59600
    },
    {
      "epoch": 28.122562017559368,
      "grad_norm": 1.2154430150985718,
      "learning_rate": 9.906097752632408e-06,
      "loss": 0.0287,
      "step": 59650
    },
    {
      "epoch": 28.14613163632078,
      "grad_norm": 0.8226194381713867,
      "learning_rate": 9.886452931007387e-06,
      "loss": 0.0293,
      "step": 59700
    },
    {
      "epoch": 28.1697012550822,
      "grad_norm": 1.2648977041244507,
      "learning_rate": 9.866808109382367e-06,
      "loss": 0.0268,
      "step": 59750
    },
    {
      "epoch": 28.193270873843616,
      "grad_norm": 1.974181056022644,
      "learning_rate": 9.847163287757346e-06,
      "loss": 0.0294,
      "step": 59800
    },
    {
      "epoch": 28.216840492605034,
      "grad_norm": 1.5769785642623901,
      "learning_rate": 9.827518466132327e-06,
      "loss": 0.0318,
      "step": 59850
    },
    {
      "epoch": 28.240410111366447,
      "grad_norm": 0.7971946001052856,
      "learning_rate": 9.807873644507308e-06,
      "loss": 0.0294,
      "step": 59900
    },
    {
      "epoch": 28.263979730127865,
      "grad_norm": 1.4903451204299927,
      "learning_rate": 9.788228822882288e-06,
      "loss": 0.027,
      "step": 59950
    },
    {
      "epoch": 28.287549348889282,
      "grad_norm": 1.807135820388794,
      "learning_rate": 9.768584001257269e-06,
      "loss": 0.0336,
      "step": 60000
    },
    {
      "epoch": 28.287549348889282,
      "eval_loss": 0.2659206986427307,
      "eval_runtime": 165.2937,
      "eval_samples_per_second": 176.014,
      "eval_steps_per_second": 44.007,
      "step": 60000
    },
    {
      "epoch": 28.3111189676507,
      "grad_norm": 0.9125618934631348,
      "learning_rate": 9.748939179632248e-06,
      "loss": 0.035,
      "step": 60050
    },
    {
      "epoch": 28.334688586412113,
      "grad_norm": 1.8362839221954346,
      "learning_rate": 9.72929435800723e-06,
      "loss": 0.033,
      "step": 60100
    },
    {
      "epoch": 28.35825820517353,
      "grad_norm": 1.318579912185669,
      "learning_rate": 9.70964953638221e-06,
      "loss": 0.0322,
      "step": 60150
    },
    {
      "epoch": 28.381827823934948,
      "grad_norm": 2.9404213428497314,
      "learning_rate": 9.69000471475719e-06,
      "loss": 0.0304,
      "step": 60200
    },
    {
      "epoch": 28.405397442696366,
      "grad_norm": 0.5179895162582397,
      "learning_rate": 9.670359893132171e-06,
      "loss": 0.0292,
      "step": 60250
    },
    {
      "epoch": 28.42896706145778,
      "grad_norm": 0.7380061149597168,
      "learning_rate": 9.65071507150715e-06,
      "loss": 0.0287,
      "step": 60300
    },
    {
      "epoch": 28.452536680219197,
      "grad_norm": 1.4897048473358154,
      "learning_rate": 9.631070249882132e-06,
      "loss": 0.0329,
      "step": 60350
    },
    {
      "epoch": 28.476106298980614,
      "grad_norm": 0.5303643941879272,
      "learning_rate": 9.611425428257111e-06,
      "loss": 0.0327,
      "step": 60400
    },
    {
      "epoch": 28.49967591774203,
      "grad_norm": 1.1031520366668701,
      "learning_rate": 9.591780606632092e-06,
      "loss": 0.0344,
      "step": 60450
    },
    {
      "epoch": 28.523245536503445,
      "grad_norm": 1.7170038223266602,
      "learning_rate": 9.572135785007071e-06,
      "loss": 0.0322,
      "step": 60500
    },
    {
      "epoch": 28.546815155264863,
      "grad_norm": 1.749816656112671,
      "learning_rate": 9.552490963382053e-06,
      "loss": 0.0348,
      "step": 60550
    },
    {
      "epoch": 28.57038477402628,
      "grad_norm": 0.9798631072044373,
      "learning_rate": 9.532846141757034e-06,
      "loss": 0.0303,
      "step": 60600
    },
    {
      "epoch": 28.593954392787698,
      "grad_norm": 1.8098692893981934,
      "learning_rate": 9.513201320132013e-06,
      "loss": 0.0338,
      "step": 60650
    },
    {
      "epoch": 28.617524011549115,
      "grad_norm": 2.03981614112854,
      "learning_rate": 9.493556498506994e-06,
      "loss": 0.0374,
      "step": 60700
    },
    {
      "epoch": 28.64109363031053,
      "grad_norm": 1.0901907682418823,
      "learning_rate": 9.473911676881973e-06,
      "loss": 0.0298,
      "step": 60750
    },
    {
      "epoch": 28.664663249071946,
      "grad_norm": 2.0278780460357666,
      "learning_rate": 9.454266855256955e-06,
      "loss": 0.0279,
      "step": 60800
    },
    {
      "epoch": 28.688232867833364,
      "grad_norm": 2.9129717350006104,
      "learning_rate": 9.434622033631936e-06,
      "loss": 0.0363,
      "step": 60850
    },
    {
      "epoch": 28.71180248659478,
      "grad_norm": 1.4923914670944214,
      "learning_rate": 9.414977212006915e-06,
      "loss": 0.0297,
      "step": 60900
    },
    {
      "epoch": 28.735372105356195,
      "grad_norm": 2.2262165546417236,
      "learning_rate": 9.395332390381895e-06,
      "loss": 0.032,
      "step": 60950
    },
    {
      "epoch": 28.758941724117612,
      "grad_norm": 1.3439642190933228,
      "learning_rate": 9.375687568756876e-06,
      "loss": 0.037,
      "step": 61000
    },
    {
      "epoch": 28.78251134287903,
      "grad_norm": 0.9601325392723083,
      "learning_rate": 9.356042747131857e-06,
      "loss": 0.0336,
      "step": 61050
    },
    {
      "epoch": 28.806080961640447,
      "grad_norm": 0.9717387557029724,
      "learning_rate": 9.336397925506836e-06,
      "loss": 0.0318,
      "step": 61100
    },
    {
      "epoch": 28.82965058040186,
      "grad_norm": 1.746664047241211,
      "learning_rate": 9.316753103881816e-06,
      "loss": 0.0336,
      "step": 61150
    },
    {
      "epoch": 28.853220199163278,
      "grad_norm": 1.6154351234436035,
      "learning_rate": 9.297108282256797e-06,
      "loss": 0.0332,
      "step": 61200
    },
    {
      "epoch": 28.876789817924696,
      "grad_norm": 1.9587492942810059,
      "learning_rate": 9.277463460631778e-06,
      "loss": 0.0331,
      "step": 61250
    },
    {
      "epoch": 28.900359436686113,
      "grad_norm": 1.106900691986084,
      "learning_rate": 9.257818639006759e-06,
      "loss": 0.0311,
      "step": 61300
    },
    {
      "epoch": 28.923929055447527,
      "grad_norm": 1.021830439567566,
      "learning_rate": 9.238173817381738e-06,
      "loss": 0.0357,
      "step": 61350
    },
    {
      "epoch": 28.947498674208944,
      "grad_norm": 1.3296558856964111,
      "learning_rate": 9.218528995756718e-06,
      "loss": 0.0359,
      "step": 61400
    },
    {
      "epoch": 28.97106829297036,
      "grad_norm": 1.0993231534957886,
      "learning_rate": 9.1988841741317e-06,
      "loss": 0.0308,
      "step": 61450
    },
    {
      "epoch": 28.99463791173178,
      "grad_norm": 1.4834465980529785,
      "learning_rate": 9.17923935250668e-06,
      "loss": 0.0388,
      "step": 61500
    },
    {
      "epoch": 29.017912910258676,
      "grad_norm": 0.6822028756141663,
      "learning_rate": 9.15959453088166e-06,
      "loss": 0.0349,
      "step": 61550
    },
    {
      "epoch": 29.041482529020094,
      "grad_norm": 1.2168731689453125,
      "learning_rate": 9.13994970925664e-06,
      "loss": 0.0245,
      "step": 61600
    },
    {
      "epoch": 29.06505214778151,
      "grad_norm": 0.8555282354354858,
      "learning_rate": 9.12030488763162e-06,
      "loss": 0.0265,
      "step": 61650
    },
    {
      "epoch": 29.088621766542925,
      "grad_norm": 1.1673071384429932,
      "learning_rate": 9.1006600660066e-06,
      "loss": 0.0282,
      "step": 61700
    },
    {
      "epoch": 29.112191385304342,
      "grad_norm": 1.6651926040649414,
      "learning_rate": 9.081015244381581e-06,
      "loss": 0.0283,
      "step": 61750
    },
    {
      "epoch": 29.13576100406576,
      "grad_norm": 0.9010726809501648,
      "learning_rate": 9.061370422756562e-06,
      "loss": 0.0275,
      "step": 61800
    },
    {
      "epoch": 29.159330622827177,
      "grad_norm": 0.7584050297737122,
      "learning_rate": 9.041725601131541e-06,
      "loss": 0.0252,
      "step": 61850
    },
    {
      "epoch": 29.18290024158859,
      "grad_norm": 1.2843317985534668,
      "learning_rate": 9.022080779506523e-06,
      "loss": 0.0229,
      "step": 61900
    },
    {
      "epoch": 29.20646986035001,
      "grad_norm": 1.2868425846099854,
      "learning_rate": 9.002435957881502e-06,
      "loss": 0.0299,
      "step": 61950
    },
    {
      "epoch": 29.230039479111426,
      "grad_norm": 0.5788112282752991,
      "learning_rate": 8.982791136256483e-06,
      "loss": 0.0283,
      "step": 62000
    },
    {
      "epoch": 29.253609097872843,
      "grad_norm": 0.6175509095191956,
      "learning_rate": 8.963146314631464e-06,
      "loss": 0.026,
      "step": 62050
    },
    {
      "epoch": 29.277178716634257,
      "grad_norm": 2.1469874382019043,
      "learning_rate": 8.943501493006443e-06,
      "loss": 0.0284,
      "step": 62100
    },
    {
      "epoch": 29.300748335395674,
      "grad_norm": 1.2766846418380737,
      "learning_rate": 8.923856671381425e-06,
      "loss": 0.0284,
      "step": 62150
    },
    {
      "epoch": 29.32431795415709,
      "grad_norm": 1.1142854690551758,
      "learning_rate": 8.904211849756404e-06,
      "loss": 0.0266,
      "step": 62200
    },
    {
      "epoch": 29.34788757291851,
      "grad_norm": 1.4643669128417969,
      "learning_rate": 8.884567028131385e-06,
      "loss": 0.0277,
      "step": 62250
    },
    {
      "epoch": 29.371457191679923,
      "grad_norm": 0.3364775478839874,
      "learning_rate": 8.864922206506364e-06,
      "loss": 0.0278,
      "step": 62300
    },
    {
      "epoch": 29.39502681044134,
      "grad_norm": 1.2799029350280762,
      "learning_rate": 8.845277384881346e-06,
      "loss": 0.0303,
      "step": 62350
    },
    {
      "epoch": 29.418596429202758,
      "grad_norm": 2.44569993019104,
      "learning_rate": 8.825632563256327e-06,
      "loss": 0.0276,
      "step": 62400
    },
    {
      "epoch": 29.442166047964175,
      "grad_norm": 1.005496859550476,
      "learning_rate": 8.805987741631306e-06,
      "loss": 0.0331,
      "step": 62450
    },
    {
      "epoch": 29.46573566672559,
      "grad_norm": 0.590807318687439,
      "learning_rate": 8.786342920006287e-06,
      "loss": 0.0298,
      "step": 62500
    },
    {
      "epoch": 29.489305285487006,
      "grad_norm": 1.9364567995071411,
      "learning_rate": 8.766698098381266e-06,
      "loss": 0.0287,
      "step": 62550
    },
    {
      "epoch": 29.512874904248424,
      "grad_norm": 1.669671893119812,
      "learning_rate": 8.747053276756248e-06,
      "loss": 0.0267,
      "step": 62600
    },
    {
      "epoch": 29.53644452300984,
      "grad_norm": 1.4152973890304565,
      "learning_rate": 8.727408455131227e-06,
      "loss": 0.0371,
      "step": 62650
    },
    {
      "epoch": 29.56001414177126,
      "grad_norm": 1.1400086879730225,
      "learning_rate": 8.707763633506208e-06,
      "loss": 0.0308,
      "step": 62700
    },
    {
      "epoch": 29.583583760532672,
      "grad_norm": 0.686471164226532,
      "learning_rate": 8.688118811881188e-06,
      "loss": 0.0325,
      "step": 62750
    },
    {
      "epoch": 29.60715337929409,
      "grad_norm": 1.4236444234848022,
      "learning_rate": 8.668473990256169e-06,
      "loss": 0.03,
      "step": 62800
    },
    {
      "epoch": 29.630722998055507,
      "grad_norm": 0.6464881300926208,
      "learning_rate": 8.64882916863115e-06,
      "loss": 0.0369,
      "step": 62850
    },
    {
      "epoch": 29.654292616816925,
      "grad_norm": 1.4861934185028076,
      "learning_rate": 8.629184347006129e-06,
      "loss": 0.0338,
      "step": 62900
    },
    {
      "epoch": 29.67786223557834,
      "grad_norm": 1.8266569375991821,
      "learning_rate": 8.60953952538111e-06,
      "loss": 0.0313,
      "step": 62950
    },
    {
      "epoch": 29.701431854339756,
      "grad_norm": 1.1330498456954956,
      "learning_rate": 8.58989470375609e-06,
      "loss": 0.0306,
      "step": 63000
    },
    {
      "epoch": 29.725001473101173,
      "grad_norm": 1.4041168689727783,
      "learning_rate": 8.57024988213107e-06,
      "loss": 0.0286,
      "step": 63050
    },
    {
      "epoch": 29.74857109186259,
      "grad_norm": 1.853751540184021,
      "learning_rate": 8.550605060506051e-06,
      "loss": 0.0327,
      "step": 63100
    },
    {
      "epoch": 29.772140710624004,
      "grad_norm": 1.636978030204773,
      "learning_rate": 8.53096023888103e-06,
      "loss": 0.0348,
      "step": 63150
    },
    {
      "epoch": 29.79571032938542,
      "grad_norm": 1.1099073886871338,
      "learning_rate": 8.511315417256011e-06,
      "loss": 0.0358,
      "step": 63200
    },
    {
      "epoch": 29.81927994814684,
      "grad_norm": 3.8827435970306396,
      "learning_rate": 8.491670595630992e-06,
      "loss": 0.0299,
      "step": 63250
    },
    {
      "epoch": 29.842849566908257,
      "grad_norm": 2.693324565887451,
      "learning_rate": 8.472025774005972e-06,
      "loss": 0.0315,
      "step": 63300
    },
    {
      "epoch": 29.86641918566967,
      "grad_norm": 1.1883275508880615,
      "learning_rate": 8.452380952380953e-06,
      "loss": 0.0349,
      "step": 63350
    },
    {
      "epoch": 29.889988804431088,
      "grad_norm": 1.8717767000198364,
      "learning_rate": 8.432736130755932e-06,
      "loss": 0.0298,
      "step": 63400
    },
    {
      "epoch": 29.913558423192505,
      "grad_norm": 0.48260679841041565,
      "learning_rate": 8.413091309130913e-06,
      "loss": 0.0313,
      "step": 63450
    },
    {
      "epoch": 29.937128041953923,
      "grad_norm": 0.7357680797576904,
      "learning_rate": 8.393839383938394e-06,
      "loss": 0.0353,
      "step": 63500
    },
    {
      "epoch": 29.960697660715336,
      "grad_norm": 1.1581897735595703,
      "learning_rate": 8.374194562313375e-06,
      "loss": 0.0336,
      "step": 63550
    },
    {
      "epoch": 29.984267279476754,
      "grad_norm": 2.9130959510803223,
      "learning_rate": 8.354549740688354e-06,
      "loss": 0.0321,
      "step": 63600
    },
    {
      "epoch": 30.007542278003655,
      "grad_norm": 1.1097666025161743,
      "learning_rate": 8.334904919063336e-06,
      "loss": 0.0317,
      "step": 63650
    },
    {
      "epoch": 30.03111189676507,
      "grad_norm": 1.0063352584838867,
      "learning_rate": 8.315260097438315e-06,
      "loss": 0.029,
      "step": 63700
    },
    {
      "epoch": 30.054681515526486,
      "grad_norm": 1.3387041091918945,
      "learning_rate": 8.295615275813296e-06,
      "loss": 0.0277,
      "step": 63750
    },
    {
      "epoch": 30.078251134287903,
      "grad_norm": 0.624051570892334,
      "learning_rate": 8.275970454188277e-06,
      "loss": 0.0269,
      "step": 63800
    },
    {
      "epoch": 30.10182075304932,
      "grad_norm": 1.0087482929229736,
      "learning_rate": 8.256325632563256e-06,
      "loss": 0.0248,
      "step": 63850
    },
    {
      "epoch": 30.125390371810735,
      "grad_norm": 0.6717721223831177,
      "learning_rate": 8.236680810938238e-06,
      "loss": 0.0234,
      "step": 63900
    },
    {
      "epoch": 30.148959990572152,
      "grad_norm": 0.4778923988342285,
      "learning_rate": 8.217035989313217e-06,
      "loss": 0.0271,
      "step": 63950
    },
    {
      "epoch": 30.17252960933357,
      "grad_norm": 0.451797217130661,
      "learning_rate": 8.197391167688198e-06,
      "loss": 0.0255,
      "step": 64000
    },
    {
      "epoch": 30.196099228094987,
      "grad_norm": 1.6681160926818848,
      "learning_rate": 8.177746346063177e-06,
      "loss": 0.0274,
      "step": 64050
    },
    {
      "epoch": 30.2196688468564,
      "grad_norm": 1.0547391176223755,
      "learning_rate": 8.158101524438159e-06,
      "loss": 0.0322,
      "step": 64100
    },
    {
      "epoch": 30.243238465617818,
      "grad_norm": 1.6087467670440674,
      "learning_rate": 8.13845670281314e-06,
      "loss": 0.0266,
      "step": 64150
    },
    {
      "epoch": 30.266808084379235,
      "grad_norm": 1.7586674690246582,
      "learning_rate": 8.118811881188119e-06,
      "loss": 0.0263,
      "step": 64200
    },
    {
      "epoch": 30.290377703140653,
      "grad_norm": 0.5939701199531555,
      "learning_rate": 8.0991670595631e-06,
      "loss": 0.0277,
      "step": 64250
    },
    {
      "epoch": 30.313947321902067,
      "grad_norm": 0.5168440341949463,
      "learning_rate": 8.079522237938078e-06,
      "loss": 0.0308,
      "step": 64300
    },
    {
      "epoch": 30.337516940663484,
      "grad_norm": 1.3314661979675293,
      "learning_rate": 8.05987741631306e-06,
      "loss": 0.0233,
      "step": 64350
    },
    {
      "epoch": 30.3610865594249,
      "grad_norm": 0.41785261034965515,
      "learning_rate": 8.040232594688041e-06,
      "loss": 0.0272,
      "step": 64400
    },
    {
      "epoch": 30.38465617818632,
      "grad_norm": 1.2678223848342896,
      "learning_rate": 8.02058777306302e-06,
      "loss": 0.0291,
      "step": 64450
    },
    {
      "epoch": 30.408225796947736,
      "grad_norm": 2.4668326377868652,
      "learning_rate": 8.000942951438001e-06,
      "loss": 0.0321,
      "step": 64500
    },
    {
      "epoch": 30.43179541570915,
      "grad_norm": 2.107285499572754,
      "learning_rate": 7.981298129812982e-06,
      "loss": 0.0294,
      "step": 64550
    },
    {
      "epoch": 30.455365034470567,
      "grad_norm": 2.325831890106201,
      "learning_rate": 7.961653308187963e-06,
      "loss": 0.0315,
      "step": 64600
    },
    {
      "epoch": 30.478934653231985,
      "grad_norm": 2.1924145221710205,
      "learning_rate": 7.942008486562942e-06,
      "loss": 0.0283,
      "step": 64650
    },
    {
      "epoch": 30.502504271993402,
      "grad_norm": 0.9484047889709473,
      "learning_rate": 7.922363664937922e-06,
      "loss": 0.0287,
      "step": 64700
    },
    {
      "epoch": 30.526073890754816,
      "grad_norm": 0.699944019317627,
      "learning_rate": 7.902718843312903e-06,
      "loss": 0.0317,
      "step": 64750
    },
    {
      "epoch": 30.549643509516233,
      "grad_norm": 1.0361651182174683,
      "learning_rate": 7.883074021687884e-06,
      "loss": 0.0296,
      "step": 64800
    },
    {
      "epoch": 30.57321312827765,
      "grad_norm": 0.6265474557876587,
      "learning_rate": 7.863429200062864e-06,
      "loss": 0.032,
      "step": 64850
    },
    {
      "epoch": 30.596782747039068,
      "grad_norm": 1.2754360437393188,
      "learning_rate": 7.843784378437843e-06,
      "loss": 0.0282,
      "step": 64900
    },
    {
      "epoch": 30.620352365800482,
      "grad_norm": 1.7791812419891357,
      "learning_rate": 7.824139556812824e-06,
      "loss": 0.0326,
      "step": 64950
    },
    {
      "epoch": 30.6439219845619,
      "grad_norm": 1.0590966939926147,
      "learning_rate": 7.804494735187805e-06,
      "loss": 0.029,
      "step": 65000
    },
    {
      "epoch": 30.6439219845619,
      "eval_loss": 0.2685844600200653,
      "eval_runtime": 185.7908,
      "eval_samples_per_second": 156.595,
      "eval_steps_per_second": 39.152,
      "step": 65000
    },
    {
      "epoch": 30.667491603323317,
      "grad_norm": 1.6960614919662476,
      "learning_rate": 7.784849913562785e-06,
      "loss": 0.033,
      "step": 65050
    },
    {
      "epoch": 30.691061222084734,
      "grad_norm": 1.2280875444412231,
      "learning_rate": 7.765205091937766e-06,
      "loss": 0.0302,
      "step": 65100
    },
    {
      "epoch": 30.714630840846148,
      "grad_norm": 1.3475093841552734,
      "learning_rate": 7.745560270312745e-06,
      "loss": 0.0334,
      "step": 65150
    },
    {
      "epoch": 30.738200459607565,
      "grad_norm": 1.54008948802948,
      "learning_rate": 7.725915448687726e-06,
      "loss": 0.0322,
      "step": 65200
    },
    {
      "epoch": 30.761770078368983,
      "grad_norm": 1.3292044401168823,
      "learning_rate": 7.706270627062706e-06,
      "loss": 0.0334,
      "step": 65250
    },
    {
      "epoch": 30.7853396971304,
      "grad_norm": 1.161000370979309,
      "learning_rate": 7.686625805437687e-06,
      "loss": 0.0312,
      "step": 65300
    },
    {
      "epoch": 30.808909315891814,
      "grad_norm": 1.0694904327392578,
      "learning_rate": 7.666980983812668e-06,
      "loss": 0.0324,
      "step": 65350
    },
    {
      "epoch": 30.83247893465323,
      "grad_norm": 0.7585427761077881,
      "learning_rate": 7.647336162187647e-06,
      "loss": 0.0296,
      "step": 65400
    },
    {
      "epoch": 30.85604855341465,
      "grad_norm": 0.4660758674144745,
      "learning_rate": 7.627691340562628e-06,
      "loss": 0.0264,
      "step": 65450
    },
    {
      "epoch": 30.879618172176066,
      "grad_norm": 1.4668360948562622,
      "learning_rate": 7.608046518937609e-06,
      "loss": 0.0279,
      "step": 65500
    },
    {
      "epoch": 30.90318779093748,
      "grad_norm": 1.6257703304290771,
      "learning_rate": 7.588401697312589e-06,
      "loss": 0.0344,
      "step": 65550
    },
    {
      "epoch": 30.926757409698897,
      "grad_norm": 7.0167717933654785,
      "learning_rate": 7.568756875687569e-06,
      "loss": 0.0294,
      "step": 65600
    },
    {
      "epoch": 30.950327028460315,
      "grad_norm": 1.1138496398925781,
      "learning_rate": 7.5491120540625485e-06,
      "loss": 0.029,
      "step": 65650
    },
    {
      "epoch": 30.973896647221732,
      "grad_norm": 1.5996969938278198,
      "learning_rate": 7.529860128870031e-06,
      "loss": 0.0312,
      "step": 65700
    },
    {
      "epoch": 30.99746626598315,
      "grad_norm": 1.4041591882705688,
      "learning_rate": 7.5102153072450105e-06,
      "loss": 0.0291,
      "step": 65750
    },
    {
      "epoch": 31.020741264510047,
      "grad_norm": 1.1836175918579102,
      "learning_rate": 7.490570485619991e-06,
      "loss": 0.0243,
      "step": 65800
    },
    {
      "epoch": 31.044310883271464,
      "grad_norm": 1.5265262126922607,
      "learning_rate": 7.470925663994971e-06,
      "loss": 0.0223,
      "step": 65850
    },
    {
      "epoch": 31.067880502032878,
      "grad_norm": 1.1857560873031616,
      "learning_rate": 7.451280842369952e-06,
      "loss": 0.0278,
      "step": 65900
    },
    {
      "epoch": 31.091450120794295,
      "grad_norm": 0.7104184031486511,
      "learning_rate": 7.4316360207449315e-06,
      "loss": 0.0258,
      "step": 65950
    },
    {
      "epoch": 31.115019739555713,
      "grad_norm": 2.1065163612365723,
      "learning_rate": 7.411991199119912e-06,
      "loss": 0.0246,
      "step": 66000
    },
    {
      "epoch": 31.13858935831713,
      "grad_norm": 0.5503594875335693,
      "learning_rate": 7.392346377494893e-06,
      "loss": 0.0268,
      "step": 66050
    },
    {
      "epoch": 31.162158977078544,
      "grad_norm": 0.6924808621406555,
      "learning_rate": 7.372701555869873e-06,
      "loss": 0.027,
      "step": 66100
    },
    {
      "epoch": 31.18572859583996,
      "grad_norm": 2.1518895626068115,
      "learning_rate": 7.3530567342448534e-06,
      "loss": 0.0309,
      "step": 66150
    },
    {
      "epoch": 31.20929821460138,
      "grad_norm": 1.8238049745559692,
      "learning_rate": 7.333411912619833e-06,
      "loss": 0.0288,
      "step": 66200
    },
    {
      "epoch": 31.232867833362796,
      "grad_norm": 1.2465204000473022,
      "learning_rate": 7.313767090994814e-06,
      "loss": 0.0255,
      "step": 66250
    },
    {
      "epoch": 31.25643745212421,
      "grad_norm": 2.056440830230713,
      "learning_rate": 7.294122269369795e-06,
      "loss": 0.0287,
      "step": 66300
    },
    {
      "epoch": 31.280007070885627,
      "grad_norm": 0.3807273209095001,
      "learning_rate": 7.2744774477447745e-06,
      "loss": 0.0246,
      "step": 66350
    },
    {
      "epoch": 31.303576689647045,
      "grad_norm": 1.4598006010055542,
      "learning_rate": 7.254832626119755e-06,
      "loss": 0.0281,
      "step": 66400
    },
    {
      "epoch": 31.327146308408462,
      "grad_norm": 1.4300241470336914,
      "learning_rate": 7.235187804494735e-06,
      "loss": 0.0288,
      "step": 66450
    },
    {
      "epoch": 31.35071592716988,
      "grad_norm": 1.7597672939300537,
      "learning_rate": 7.215542982869716e-06,
      "loss": 0.0263,
      "step": 66500
    },
    {
      "epoch": 31.374285545931293,
      "grad_norm": 0.38958755135536194,
      "learning_rate": 7.1958981612446955e-06,
      "loss": 0.024,
      "step": 66550
    },
    {
      "epoch": 31.39785516469271,
      "grad_norm": 1.6107853651046753,
      "learning_rate": 7.176253339619677e-06,
      "loss": 0.03,
      "step": 66600
    },
    {
      "epoch": 31.42142478345413,
      "grad_norm": 0.908169150352478,
      "learning_rate": 7.156608517994657e-06,
      "loss": 0.0275,
      "step": 66650
    },
    {
      "epoch": 31.444994402215546,
      "grad_norm": 1.669784665107727,
      "learning_rate": 7.136963696369638e-06,
      "loss": 0.0245,
      "step": 66700
    },
    {
      "epoch": 31.46856402097696,
      "grad_norm": 0.9482927322387695,
      "learning_rate": 7.1173188747446174e-06,
      "loss": 0.0285,
      "step": 66750
    },
    {
      "epoch": 31.492133639738377,
      "grad_norm": 1.5911481380462646,
      "learning_rate": 7.097674053119597e-06,
      "loss": 0.0278,
      "step": 66800
    },
    {
      "epoch": 31.515703258499794,
      "grad_norm": 1.3927643299102783,
      "learning_rate": 7.078029231494578e-06,
      "loss": 0.0273,
      "step": 66850
    },
    {
      "epoch": 31.53927287726121,
      "grad_norm": 1.5338459014892578,
      "learning_rate": 7.058384409869559e-06,
      "loss": 0.0278,
      "step": 66900
    },
    {
      "epoch": 31.562842496022625,
      "grad_norm": 0.6983182430267334,
      "learning_rate": 7.038739588244539e-06,
      "loss": 0.0266,
      "step": 66950
    },
    {
      "epoch": 31.586412114784043,
      "grad_norm": 1.5588388442993164,
      "learning_rate": 7.019094766619519e-06,
      "loss": 0.0319,
      "step": 67000
    },
    {
      "epoch": 31.60998173354546,
      "grad_norm": 1.7154418230056763,
      "learning_rate": 6.9994499449945e-06,
      "loss": 0.0316,
      "step": 67050
    },
    {
      "epoch": 31.633551352306878,
      "grad_norm": 0.9463013410568237,
      "learning_rate": 6.97980512336948e-06,
      "loss": 0.0286,
      "step": 67100
    },
    {
      "epoch": 31.65712097106829,
      "grad_norm": 1.1721453666687012,
      "learning_rate": 6.96016030174446e-06,
      "loss": 0.0297,
      "step": 67150
    },
    {
      "epoch": 31.68069058982971,
      "grad_norm": 0.960400402545929,
      "learning_rate": 6.94051548011944e-06,
      "loss": 0.0272,
      "step": 67200
    },
    {
      "epoch": 31.704260208591126,
      "grad_norm": 1.1582214832305908,
      "learning_rate": 6.920870658494421e-06,
      "loss": 0.0259,
      "step": 67250
    },
    {
      "epoch": 31.727829827352544,
      "grad_norm": 0.8177645206451416,
      "learning_rate": 6.901225836869402e-06,
      "loss": 0.0263,
      "step": 67300
    },
    {
      "epoch": 31.751399446113957,
      "grad_norm": 1.1947131156921387,
      "learning_rate": 6.8815810152443815e-06,
      "loss": 0.0276,
      "step": 67350
    },
    {
      "epoch": 31.774969064875375,
      "grad_norm": 1.3635445833206177,
      "learning_rate": 6.861936193619362e-06,
      "loss": 0.0295,
      "step": 67400
    },
    {
      "epoch": 31.798538683636792,
      "grad_norm": 0.8363726735115051,
      "learning_rate": 6.842291371994342e-06,
      "loss": 0.0271,
      "step": 67450
    },
    {
      "epoch": 31.82210830239821,
      "grad_norm": 1.9453133344650269,
      "learning_rate": 6.822646550369323e-06,
      "loss": 0.0322,
      "step": 67500
    },
    {
      "epoch": 31.845677921159627,
      "grad_norm": 1.1097487211227417,
      "learning_rate": 6.803001728744303e-06,
      "loss": 0.0278,
      "step": 67550
    },
    {
      "epoch": 31.86924753992104,
      "grad_norm": 1.6211479902267456,
      "learning_rate": 6.783356907119284e-06,
      "loss": 0.0318,
      "step": 67600
    },
    {
      "epoch": 31.89281715868246,
      "grad_norm": 1.2017467021942139,
      "learning_rate": 6.763712085494264e-06,
      "loss": 0.0272,
      "step": 67650
    },
    {
      "epoch": 31.916386777443876,
      "grad_norm": 1.323400855064392,
      "learning_rate": 6.744067263869244e-06,
      "loss": 0.0334,
      "step": 67700
    },
    {
      "epoch": 31.939956396205293,
      "grad_norm": 1.3893035650253296,
      "learning_rate": 6.724422442244224e-06,
      "loss": 0.027,
      "step": 67750
    },
    {
      "epoch": 31.963526014966707,
      "grad_norm": 1.5395725965499878,
      "learning_rate": 6.704777620619204e-06,
      "loss": 0.0303,
      "step": 67800
    },
    {
      "epoch": 31.987095633728124,
      "grad_norm": 1.8511215448379517,
      "learning_rate": 6.685132798994186e-06,
      "loss": 0.0278,
      "step": 67850
    },
    {
      "epoch": 32.010370632255025,
      "grad_norm": 0.759831964969635,
      "learning_rate": 6.665487977369166e-06,
      "loss": 0.0289,
      "step": 67900
    },
    {
      "epoch": 32.03394025101644,
      "grad_norm": 0.7282164692878723,
      "learning_rate": 6.645843155744146e-06,
      "loss": 0.0221,
      "step": 67950
    },
    {
      "epoch": 32.05750986977785,
      "grad_norm": 0.5565329194068909,
      "learning_rate": 6.626198334119126e-06,
      "loss": 0.0265,
      "step": 68000
    },
    {
      "epoch": 32.081079488539274,
      "grad_norm": 1.055575966835022,
      "learning_rate": 6.606553512494107e-06,
      "loss": 0.0247,
      "step": 68050
    },
    {
      "epoch": 32.10464910730069,
      "grad_norm": 1.1363054513931274,
      "learning_rate": 6.586908690869087e-06,
      "loss": 0.0254,
      "step": 68100
    },
    {
      "epoch": 32.12821872606211,
      "grad_norm": 0.5168552994728088,
      "learning_rate": 6.567263869244067e-06,
      "loss": 0.0198,
      "step": 68150
    },
    {
      "epoch": 32.15178834482352,
      "grad_norm": 3.6761109828948975,
      "learning_rate": 6.547619047619048e-06,
      "loss": 0.0273,
      "step": 68200
    },
    {
      "epoch": 32.175357963584936,
      "grad_norm": 1.062924861907959,
      "learning_rate": 6.527974225994028e-06,
      "loss": 0.0214,
      "step": 68250
    },
    {
      "epoch": 32.19892758234636,
      "grad_norm": 1.1209654808044434,
      "learning_rate": 6.508329404369009e-06,
      "loss": 0.022,
      "step": 68300
    },
    {
      "epoch": 32.22249720110777,
      "grad_norm": 0.8536813259124756,
      "learning_rate": 6.488684582743988e-06,
      "loss": 0.0251,
      "step": 68350
    },
    {
      "epoch": 32.24606681986919,
      "grad_norm": 0.849697470664978,
      "learning_rate": 6.469039761118969e-06,
      "loss": 0.0236,
      "step": 68400
    },
    {
      "epoch": 32.269636438630606,
      "grad_norm": 0.30001741647720337,
      "learning_rate": 6.44939493949395e-06,
      "loss": 0.0254,
      "step": 68450
    },
    {
      "epoch": 32.29320605739202,
      "grad_norm": 1.536691665649414,
      "learning_rate": 6.4297501178689305e-06,
      "loss": 0.0282,
      "step": 68500
    },
    {
      "epoch": 32.31677567615344,
      "grad_norm": 1.2677522897720337,
      "learning_rate": 6.41010529624391e-06,
      "loss": 0.027,
      "step": 68550
    },
    {
      "epoch": 32.340345294914854,
      "grad_norm": 0.6481729745864868,
      "learning_rate": 6.39046047461889e-06,
      "loss": 0.025,
      "step": 68600
    },
    {
      "epoch": 32.36391491367627,
      "grad_norm": 1.933615803718567,
      "learning_rate": 6.3712085494263715e-06,
      "loss": 0.0253,
      "step": 68650
    },
    {
      "epoch": 32.38748453243769,
      "grad_norm": 1.2986918687820435,
      "learning_rate": 6.351563727801352e-06,
      "loss": 0.024,
      "step": 68700
    },
    {
      "epoch": 32.4110541511991,
      "grad_norm": 0.9926459789276123,
      "learning_rate": 6.331918906176332e-06,
      "loss": 0.0267,
      "step": 68750
    },
    {
      "epoch": 32.434623769960524,
      "grad_norm": 1.5479587316513062,
      "learning_rate": 6.312274084551313e-06,
      "loss": 0.0298,
      "step": 68800
    },
    {
      "epoch": 32.45819338872194,
      "grad_norm": 1.2805163860321045,
      "learning_rate": 6.2926292629262925e-06,
      "loss": 0.0278,
      "step": 68850
    },
    {
      "epoch": 32.48176300748335,
      "grad_norm": 2.540496349334717,
      "learning_rate": 6.272984441301273e-06,
      "loss": 0.0286,
      "step": 68900
    },
    {
      "epoch": 32.50533262624477,
      "grad_norm": 1.5515497922897339,
      "learning_rate": 6.253339619676254e-06,
      "loss": 0.0315,
      "step": 68950
    },
    {
      "epoch": 32.528902245006186,
      "grad_norm": 0.5390152335166931,
      "learning_rate": 6.233694798051234e-06,
      "loss": 0.0281,
      "step": 69000
    },
    {
      "epoch": 32.5524718637676,
      "grad_norm": 1.025605320930481,
      "learning_rate": 6.2140499764262144e-06,
      "loss": 0.0322,
      "step": 69050
    },
    {
      "epoch": 32.57604148252902,
      "grad_norm": 1.662474274635315,
      "learning_rate": 6.194405154801194e-06,
      "loss": 0.0263,
      "step": 69100
    },
    {
      "epoch": 32.599611101290435,
      "grad_norm": 0.9966272115707397,
      "learning_rate": 6.174760333176175e-06,
      "loss": 0.0249,
      "step": 69150
    },
    {
      "epoch": 32.623180720051856,
      "grad_norm": 1.2849314212799072,
      "learning_rate": 6.155115511551155e-06,
      "loss": 0.028,
      "step": 69200
    },
    {
      "epoch": 32.64675033881327,
      "grad_norm": 0.9306769371032715,
      "learning_rate": 6.135470689926136e-06,
      "loss": 0.0235,
      "step": 69250
    },
    {
      "epoch": 32.670319957574684,
      "grad_norm": 1.389162540435791,
      "learning_rate": 6.115825868301116e-06,
      "loss": 0.0314,
      "step": 69300
    },
    {
      "epoch": 32.693889576336105,
      "grad_norm": 0.45782989263534546,
      "learning_rate": 6.096181046676097e-06,
      "loss": 0.0263,
      "step": 69350
    },
    {
      "epoch": 32.71745919509752,
      "grad_norm": 1.015197515487671,
      "learning_rate": 6.076536225051077e-06,
      "loss": 0.0308,
      "step": 69400
    },
    {
      "epoch": 32.74102881385893,
      "grad_norm": 2.0941965579986572,
      "learning_rate": 6.0568914034260565e-06,
      "loss": 0.028,
      "step": 69450
    },
    {
      "epoch": 32.76459843262035,
      "grad_norm": 1.366628646850586,
      "learning_rate": 6.037246581801037e-06,
      "loss": 0.0263,
      "step": 69500
    },
    {
      "epoch": 32.78816805138177,
      "grad_norm": 1.3074579238891602,
      "learning_rate": 6.017601760176017e-06,
      "loss": 0.0302,
      "step": 69550
    },
    {
      "epoch": 32.81173767014319,
      "grad_norm": 1.7249698638916016,
      "learning_rate": 5.997956938550999e-06,
      "loss": 0.0289,
      "step": 69600
    },
    {
      "epoch": 32.8353072889046,
      "grad_norm": 1.0933901071548462,
      "learning_rate": 5.9783121169259784e-06,
      "loss": 0.0319,
      "step": 69650
    },
    {
      "epoch": 32.858876907666016,
      "grad_norm": 1.7186380624771118,
      "learning_rate": 5.958667295300959e-06,
      "loss": 0.0284,
      "step": 69700
    },
    {
      "epoch": 32.88244652642744,
      "grad_norm": 1.1653810739517212,
      "learning_rate": 5.939022473675939e-06,
      "loss": 0.026,
      "step": 69750
    },
    {
      "epoch": 32.90601614518885,
      "grad_norm": 1.1189274787902832,
      "learning_rate": 5.91937765205092e-06,
      "loss": 0.0295,
      "step": 69800
    },
    {
      "epoch": 32.92958576395027,
      "grad_norm": 1.3682163953781128,
      "learning_rate": 5.8997328304258995e-06,
      "loss": 0.0309,
      "step": 69850
    },
    {
      "epoch": 32.953155382711685,
      "grad_norm": 1.7120345830917358,
      "learning_rate": 5.88008800880088e-06,
      "loss": 0.0285,
      "step": 69900
    },
    {
      "epoch": 32.9767250014731,
      "grad_norm": 1.1363420486450195,
      "learning_rate": 5.860443187175861e-06,
      "loss": 0.0267,
      "step": 69950
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.01998734660446644,
      "learning_rate": 5.840798365550841e-06,
      "loss": 0.0229,
      "step": 70000
    },
    {
      "epoch": 33.0,
      "eval_loss": 0.2671591341495514,
      "eval_runtime": 165.3137,
      "eval_samples_per_second": 175.993,
      "eval_steps_per_second": 44.001,
      "step": 70000
    },
    {
      "epoch": 33.026869365388016,
      "grad_norm": 1.398227334022522,
      "learning_rate": 5.821153543925821e-06,
      "loss": 0.0248,
      "step": 70050
    },
    {
      "epoch": 33.05043898414943,
      "grad_norm": 1.9250942468643188,
      "learning_rate": 5.801508722300801e-06,
      "loss": 0.0239,
      "step": 70100
    },
    {
      "epoch": 33.07400860291085,
      "grad_norm": 0.7342861890792847,
      "learning_rate": 5.781863900675782e-06,
      "loss": 0.0232,
      "step": 70150
    },
    {
      "epoch": 33.097578221672265,
      "grad_norm": 1.0306414365768433,
      "learning_rate": 5.762219079050763e-06,
      "loss": 0.0253,
      "step": 70200
    },
    {
      "epoch": 33.12114784043368,
      "grad_norm": 1.638119101524353,
      "learning_rate": 5.742574257425743e-06,
      "loss": 0.0254,
      "step": 70250
    },
    {
      "epoch": 33.1447174591951,
      "grad_norm": 0.81952965259552,
      "learning_rate": 5.722929435800723e-06,
      "loss": 0.0243,
      "step": 70300
    },
    {
      "epoch": 33.16828707795651,
      "grad_norm": 2.0201854705810547,
      "learning_rate": 5.703284614175703e-06,
      "loss": 0.0262,
      "step": 70350
    },
    {
      "epoch": 33.19185669671793,
      "grad_norm": 0.6928601861000061,
      "learning_rate": 5.683639792550684e-06,
      "loss": 0.0223,
      "step": 70400
    },
    {
      "epoch": 33.21542631547935,
      "grad_norm": 1.2917793989181519,
      "learning_rate": 5.6639949709256635e-06,
      "loss": 0.0274,
      "step": 70450
    },
    {
      "epoch": 33.23899593424076,
      "grad_norm": 2.673046350479126,
      "learning_rate": 5.644350149300645e-06,
      "loss": 0.0218,
      "step": 70500
    },
    {
      "epoch": 33.26256555300218,
      "grad_norm": 1.7267851829528809,
      "learning_rate": 5.624705327675625e-06,
      "loss": 0.0222,
      "step": 70550
    },
    {
      "epoch": 33.2861351717636,
      "grad_norm": 0.9960225224494934,
      "learning_rate": 5.6050605060506056e-06,
      "loss": 0.0271,
      "step": 70600
    },
    {
      "epoch": 33.30970479052501,
      "grad_norm": 1.7245240211486816,
      "learning_rate": 5.585415684425585e-06,
      "loss": 0.0283,
      "step": 70650
    },
    {
      "epoch": 33.33327440928643,
      "grad_norm": 1.4228465557098389,
      "learning_rate": 5.565770862800566e-06,
      "loss": 0.031,
      "step": 70700
    },
    {
      "epoch": 33.356844028047846,
      "grad_norm": 1.6003530025482178,
      "learning_rate": 5.546126041175546e-06,
      "loss": 0.0268,
      "step": 70750
    },
    {
      "epoch": 33.38041364680926,
      "grad_norm": 1.3814125061035156,
      "learning_rate": 5.526481219550527e-06,
      "loss": 0.026,
      "step": 70800
    },
    {
      "epoch": 33.40398326557068,
      "grad_norm": 0.8364442586898804,
      "learning_rate": 5.506836397925507e-06,
      "loss": 0.0276,
      "step": 70850
    },
    {
      "epoch": 33.427552884332094,
      "grad_norm": 0.7529289722442627,
      "learning_rate": 5.487191576300487e-06,
      "loss": 0.0263,
      "step": 70900
    },
    {
      "epoch": 33.451122503093515,
      "grad_norm": 0.4217100143432617,
      "learning_rate": 5.467546754675468e-06,
      "loss": 0.0297,
      "step": 70950
    },
    {
      "epoch": 33.47469212185493,
      "grad_norm": 0.6038124561309814,
      "learning_rate": 5.447901933050448e-06,
      "loss": 0.0256,
      "step": 71000
    },
    {
      "epoch": 33.49826174061634,
      "grad_norm": 2.7485547065734863,
      "learning_rate": 5.428257111425428e-06,
      "loss": 0.0258,
      "step": 71050
    },
    {
      "epoch": 33.521831359377764,
      "grad_norm": 1.4568965435028076,
      "learning_rate": 5.408612289800409e-06,
      "loss": 0.0289,
      "step": 71100
    },
    {
      "epoch": 33.54540097813918,
      "grad_norm": 0.9741525053977966,
      "learning_rate": 5.38896746817539e-06,
      "loss": 0.0276,
      "step": 71150
    },
    {
      "epoch": 33.5689705969006,
      "grad_norm": 5.951512336730957,
      "learning_rate": 5.36932264655037e-06,
      "loss": 0.026,
      "step": 71200
    },
    {
      "epoch": 33.59254021566201,
      "grad_norm": 0.7078595757484436,
      "learning_rate": 5.3496778249253494e-06,
      "loss": 0.0265,
      "step": 71250
    },
    {
      "epoch": 33.616109834423426,
      "grad_norm": 0.8414310216903687,
      "learning_rate": 5.33003300330033e-06,
      "loss": 0.0264,
      "step": 71300
    },
    {
      "epoch": 33.63967945318485,
      "grad_norm": 0.8862309455871582,
      "learning_rate": 5.31038818167531e-06,
      "loss": 0.0243,
      "step": 71350
    },
    {
      "epoch": 33.66324907194626,
      "grad_norm": Infinity,
      "learning_rate": 5.2907433600502915e-06,
      "loss": 0.0236,
      "step": 71400
    },
    {
      "epoch": 33.686818690707675,
      "grad_norm": 2.141684055328369,
      "learning_rate": 5.271491434857772e-06,
      "loss": 0.0226,
      "step": 71450
    },
    {
      "epoch": 33.710388309469096,
      "grad_norm": 1.6662932634353638,
      "learning_rate": 5.251846613232752e-06,
      "loss": 0.0249,
      "step": 71500
    },
    {
      "epoch": 33.73395792823051,
      "grad_norm": 1.2032707929611206,
      "learning_rate": 5.232201791607732e-06,
      "loss": 0.0259,
      "step": 71550
    },
    {
      "epoch": 33.75752754699193,
      "grad_norm": 1.2550581693649292,
      "learning_rate": 5.212556969982713e-06,
      "loss": 0.0264,
      "step": 71600
    },
    {
      "epoch": 33.781097165753344,
      "grad_norm": 1.6842601299285889,
      "learning_rate": 5.192912148357693e-06,
      "loss": 0.0296,
      "step": 71650
    },
    {
      "epoch": 33.80466678451476,
      "grad_norm": 0.839036226272583,
      "learning_rate": 5.173267326732674e-06,
      "loss": 0.0239,
      "step": 71700
    },
    {
      "epoch": 33.82823640327618,
      "grad_norm": 7.659609317779541,
      "learning_rate": 5.1536225051076535e-06,
      "loss": 0.0297,
      "step": 71750
    },
    {
      "epoch": 33.85180602203759,
      "grad_norm": 1.4134615659713745,
      "learning_rate": 5.133977683482634e-06,
      "loss": 0.0284,
      "step": 71800
    },
    {
      "epoch": 33.87537564079901,
      "grad_norm": 1.2429064512252808,
      "learning_rate": 5.114332861857614e-06,
      "loss": 0.0238,
      "step": 71850
    },
    {
      "epoch": 33.89894525956043,
      "grad_norm": 1.0237646102905273,
      "learning_rate": 5.094688040232595e-06,
      "loss": 0.0319,
      "step": 71900
    },
    {
      "epoch": 33.92251487832184,
      "grad_norm": 1.1140377521514893,
      "learning_rate": 5.0750432186075754e-06,
      "loss": 0.0291,
      "step": 71950
    },
    {
      "epoch": 33.94608449708326,
      "grad_norm": 1.01846444606781,
      "learning_rate": 5.055398396982555e-06,
      "loss": 0.0296,
      "step": 72000
    },
    {
      "epoch": 33.969654115844676,
      "grad_norm": 1.119296669960022,
      "learning_rate": 5.035753575357536e-06,
      "loss": 0.0278,
      "step": 72050
    },
    {
      "epoch": 33.99322373460609,
      "grad_norm": 1.0376152992248535,
      "learning_rate": 5.016108753732516e-06,
      "loss": 0.0248,
      "step": 72100
    },
    {
      "epoch": 34.01697012550822,
      "grad_norm": 2.143798589706421,
      "learning_rate": 4.9964639321074965e-06,
      "loss": 0.0239,
      "step": 72150
    },
    {
      "epoch": 34.04053974426964,
      "grad_norm": 1.189496397972107,
      "learning_rate": 4.976819110482476e-06,
      "loss": 0.0257,
      "step": 72200
    },
    {
      "epoch": 34.06410936303105,
      "grad_norm": 1.3575694561004639,
      "learning_rate": 4.957174288857458e-06,
      "loss": 0.0246,
      "step": 72250
    },
    {
      "epoch": 34.08767898179247,
      "grad_norm": 1.0908666849136353,
      "learning_rate": 4.937529467232438e-06,
      "loss": 0.0276,
      "step": 72300
    },
    {
      "epoch": 34.111248600553886,
      "grad_norm": 0.968805730342865,
      "learning_rate": 4.917884645607418e-06,
      "loss": 0.0242,
      "step": 72350
    },
    {
      "epoch": 34.1348182193153,
      "grad_norm": 1.4757975339889526,
      "learning_rate": 4.898239823982398e-06,
      "loss": 0.024,
      "step": 72400
    },
    {
      "epoch": 34.15838783807672,
      "grad_norm": 0.8485345244407654,
      "learning_rate": 4.878595002357378e-06,
      "loss": 0.0252,
      "step": 72450
    },
    {
      "epoch": 34.181957456838134,
      "grad_norm": 1.7947062253952026,
      "learning_rate": 4.858950180732359e-06,
      "loss": 0.023,
      "step": 72500
    },
    {
      "epoch": 34.205527075599555,
      "grad_norm": 0.8154531717300415,
      "learning_rate": 4.8393053591073394e-06,
      "loss": 0.022,
      "step": 72550
    },
    {
      "epoch": 34.22909669436097,
      "grad_norm": 1.1131898164749146,
      "learning_rate": 4.81966053748232e-06,
      "loss": 0.0255,
      "step": 72600
    },
    {
      "epoch": 34.25266631312238,
      "grad_norm": 0.7064705491065979,
      "learning_rate": 4.8000157158573e-06,
      "loss": 0.0235,
      "step": 72650
    },
    {
      "epoch": 34.276235931883804,
      "grad_norm": 1.3280948400497437,
      "learning_rate": 4.780370894232281e-06,
      "loss": 0.0263,
      "step": 72700
    },
    {
      "epoch": 34.29980555064522,
      "grad_norm": 1.5968117713928223,
      "learning_rate": 4.7607260726072605e-06,
      "loss": 0.0231,
      "step": 72750
    },
    {
      "epoch": 34.32337516940663,
      "grad_norm": 0.4770084023475647,
      "learning_rate": 4.741081250982241e-06,
      "loss": 0.0205,
      "step": 72800
    },
    {
      "epoch": 34.34694478816805,
      "grad_norm": 1.1520593166351318,
      "learning_rate": 4.721436429357222e-06,
      "loss": 0.0251,
      "step": 72850
    },
    {
      "epoch": 34.370514406929466,
      "grad_norm": 1.5535396337509155,
      "learning_rate": 4.701791607732202e-06,
      "loss": 0.0259,
      "step": 72900
    },
    {
      "epoch": 34.39408402569089,
      "grad_norm": 0.8004440069198608,
      "learning_rate": 4.682146786107182e-06,
      "loss": 0.0236,
      "step": 72950
    },
    {
      "epoch": 34.4176536444523,
      "grad_norm": 0.8126634359359741,
      "learning_rate": 4.662501964482162e-06,
      "loss": 0.0241,
      "step": 73000
    },
    {
      "epoch": 34.441223263213715,
      "grad_norm": 1.7831252813339233,
      "learning_rate": 4.642857142857143e-06,
      "loss": 0.0272,
      "step": 73050
    },
    {
      "epoch": 34.464792881975136,
      "grad_norm": 0.21482670307159424,
      "learning_rate": 4.623212321232123e-06,
      "loss": 0.0253,
      "step": 73100
    },
    {
      "epoch": 34.48836250073655,
      "grad_norm": 0.5189471244812012,
      "learning_rate": 4.603567499607104e-06,
      "loss": 0.0245,
      "step": 73150
    },
    {
      "epoch": 34.51193211949797,
      "grad_norm": 1.042596459388733,
      "learning_rate": 4.583922677982084e-06,
      "loss": 0.0261,
      "step": 73200
    },
    {
      "epoch": 34.535501738259384,
      "grad_norm": 1.4418096542358398,
      "learning_rate": 4.564277856357065e-06,
      "loss": 0.0269,
      "step": 73250
    },
    {
      "epoch": 34.5590713570208,
      "grad_norm": 1.6551201343536377,
      "learning_rate": 4.544633034732045e-06,
      "loss": 0.0245,
      "step": 73300
    },
    {
      "epoch": 34.58264097578222,
      "grad_norm": 1.4393638372421265,
      "learning_rate": 4.5249882131070245e-06,
      "loss": 0.025,
      "step": 73350
    },
    {
      "epoch": 34.60621059454363,
      "grad_norm": 0.8167641758918762,
      "learning_rate": 4.505343391482005e-06,
      "loss": 0.0261,
      "step": 73400
    },
    {
      "epoch": 34.62978021330505,
      "grad_norm": 0.9219938516616821,
      "learning_rate": 4.485698569856986e-06,
      "loss": 0.0253,
      "step": 73450
    },
    {
      "epoch": 34.65334983206647,
      "grad_norm": 0.4497738480567932,
      "learning_rate": 4.466053748231967e-06,
      "loss": 0.026,
      "step": 73500
    },
    {
      "epoch": 34.67691945082788,
      "grad_norm": 1.2033088207244873,
      "learning_rate": 4.446408926606946e-06,
      "loss": 0.026,
      "step": 73550
    },
    {
      "epoch": 34.7004890695893,
      "grad_norm": 1.1382725238800049,
      "learning_rate": 4.426764104981927e-06,
      "loss": 0.0252,
      "step": 73600
    },
    {
      "epoch": 34.724058688350716,
      "grad_norm": 3.104313611984253,
      "learning_rate": 4.407119283356907e-06,
      "loss": 0.0287,
      "step": 73650
    },
    {
      "epoch": 34.74762830711213,
      "grad_norm": 1.1184455156326294,
      "learning_rate": 4.387474461731888e-06,
      "loss": 0.0255,
      "step": 73700
    },
    {
      "epoch": 34.77119792587355,
      "grad_norm": 0.9249743819236755,
      "learning_rate": 4.367829640106868e-06,
      "loss": 0.0277,
      "step": 73750
    },
    {
      "epoch": 34.794767544634965,
      "grad_norm": 1.5151863098144531,
      "learning_rate": 4.348184818481848e-06,
      "loss": 0.0269,
      "step": 73800
    },
    {
      "epoch": 34.81833716339638,
      "grad_norm": 0.8571919798851013,
      "learning_rate": 4.328539996856829e-06,
      "loss": 0.0246,
      "step": 73850
    },
    {
      "epoch": 34.8419067821578,
      "grad_norm": 1.9595242738723755,
      "learning_rate": 4.308895175231809e-06,
      "loss": 0.0246,
      "step": 73900
    },
    {
      "epoch": 34.86547640091921,
      "grad_norm": 1.7071378231048584,
      "learning_rate": 4.289250353606789e-06,
      "loss": 0.0276,
      "step": 73950
    },
    {
      "epoch": 34.889046019680634,
      "grad_norm": 1.2577463388442993,
      "learning_rate": 4.269605531981769e-06,
      "loss": 0.0249,
      "step": 74000
    },
    {
      "epoch": 34.91261563844205,
      "grad_norm": 0.7574151754379272,
      "learning_rate": 4.24996071035675e-06,
      "loss": 0.0239,
      "step": 74050
    },
    {
      "epoch": 34.93618525720346,
      "grad_norm": 1.2383142709732056,
      "learning_rate": 4.230315888731731e-06,
      "loss": 0.0253,
      "step": 74100
    },
    {
      "epoch": 34.95975487596488,
      "grad_norm": 1.2707239389419556,
      "learning_rate": 4.210671067106711e-06,
      "loss": 0.0245,
      "step": 74150
    },
    {
      "epoch": 34.9833244947263,
      "grad_norm": 1.5788947343826294,
      "learning_rate": 4.191026245481691e-06,
      "loss": 0.0267,
      "step": 74200
    },
    {
      "epoch": 35.0065994932532,
      "grad_norm": 1.8333139419555664,
      "learning_rate": 4.171381423856671e-06,
      "loss": 0.0258,
      "step": 74250
    },
    {
      "epoch": 35.03016911201461,
      "grad_norm": 1.4974607229232788,
      "learning_rate": 4.151736602231652e-06,
      "loss": 0.0238,
      "step": 74300
    },
    {
      "epoch": 35.05373873077603,
      "grad_norm": 1.1511878967285156,
      "learning_rate": 4.1320917806066315e-06,
      "loss": 0.0257,
      "step": 74350
    },
    {
      "epoch": 35.07730834953745,
      "grad_norm": 0.717970609664917,
      "learning_rate": 4.112446958981613e-06,
      "loss": 0.0222,
      "step": 74400
    },
    {
      "epoch": 35.10087796829886,
      "grad_norm": 1.034053087234497,
      "learning_rate": 4.092802137356593e-06,
      "loss": 0.0211,
      "step": 74450
    },
    {
      "epoch": 35.12444758706028,
      "grad_norm": 0.7410032153129578,
      "learning_rate": 4.073550212164073e-06,
      "loss": 0.0253,
      "step": 74500
    },
    {
      "epoch": 35.148017205821695,
      "grad_norm": 0.3726045489311218,
      "learning_rate": 4.053905390539054e-06,
      "loss": 0.0207,
      "step": 74550
    },
    {
      "epoch": 35.17158682458311,
      "grad_norm": 0.7298771739006042,
      "learning_rate": 4.034260568914035e-06,
      "loss": 0.0213,
      "step": 74600
    },
    {
      "epoch": 35.19515644334453,
      "grad_norm": 0.6873263120651245,
      "learning_rate": 4.0146157472890145e-06,
      "loss": 0.0236,
      "step": 74650
    },
    {
      "epoch": 35.218726062105944,
      "grad_norm": 1.2393686771392822,
      "learning_rate": 3.994970925663995e-06,
      "loss": 0.02,
      "step": 74700
    },
    {
      "epoch": 35.242295680867365,
      "grad_norm": 1.4799885749816895,
      "learning_rate": 3.975326104038975e-06,
      "loss": 0.0214,
      "step": 74750
    },
    {
      "epoch": 35.26586529962878,
      "grad_norm": 0.8561602830886841,
      "learning_rate": 3.955681282413956e-06,
      "loss": 0.025,
      "step": 74800
    },
    {
      "epoch": 35.28943491839019,
      "grad_norm": 1.637845516204834,
      "learning_rate": 3.936036460788936e-06,
      "loss": 0.026,
      "step": 74850
    },
    {
      "epoch": 35.31300453715161,
      "grad_norm": 1.1100445985794067,
      "learning_rate": 3.916391639163917e-06,
      "loss": 0.0219,
      "step": 74900
    },
    {
      "epoch": 35.33657415591303,
      "grad_norm": 0.7765010595321655,
      "learning_rate": 3.896746817538897e-06,
      "loss": 0.0221,
      "step": 74950
    },
    {
      "epoch": 35.36014377467445,
      "grad_norm": 1.3611687421798706,
      "learning_rate": 3.877101995913878e-06,
      "loss": 0.028,
      "step": 75000
    },
    {
      "epoch": 35.36014377467445,
      "eval_loss": 0.2759353220462799,
      "eval_runtime": 178.2955,
      "eval_samples_per_second": 163.179,
      "eval_steps_per_second": 40.797,
      "step": 75000
    },
    {
      "epoch": 35.38371339343586,
      "grad_norm": 1.03975248336792,
      "learning_rate": 3.8574571742888575e-06,
      "loss": 0.0246,
      "step": 75050
    },
    {
      "epoch": 35.407283012197276,
      "grad_norm": 0.8642367720603943,
      "learning_rate": 3.837812352663837e-06,
      "loss": 0.0208,
      "step": 75100
    },
    {
      "epoch": 35.4308526309587,
      "grad_norm": 1.221949815750122,
      "learning_rate": 3.818167531038818e-06,
      "loss": 0.0255,
      "step": 75150
    },
    {
      "epoch": 35.45442224972011,
      "grad_norm": 1.3577224016189575,
      "learning_rate": 3.7985227094137983e-06,
      "loss": 0.0196,
      "step": 75200
    },
    {
      "epoch": 35.477991868481524,
      "grad_norm": 1.8261467218399048,
      "learning_rate": 3.778877887788779e-06,
      "loss": 0.0231,
      "step": 75250
    },
    {
      "epoch": 35.501561487242945,
      "grad_norm": 1.4450551271438599,
      "learning_rate": 3.7592330661637592e-06,
      "loss": 0.0228,
      "step": 75300
    },
    {
      "epoch": 35.52513110600436,
      "grad_norm": 1.8262628316879272,
      "learning_rate": 3.7395882445387395e-06,
      "loss": 0.0254,
      "step": 75350
    },
    {
      "epoch": 35.54870072476578,
      "grad_norm": 0.4628553092479706,
      "learning_rate": 3.7199434229137198e-06,
      "loss": 0.0254,
      "step": 75400
    },
    {
      "epoch": 35.572270343527194,
      "grad_norm": 0.9572615027427673,
      "learning_rate": 3.7002986012887005e-06,
      "loss": 0.026,
      "step": 75450
    },
    {
      "epoch": 35.59583996228861,
      "grad_norm": 1.407948613166809,
      "learning_rate": 3.6806537796636807e-06,
      "loss": 0.0218,
      "step": 75500
    },
    {
      "epoch": 35.61940958105003,
      "grad_norm": 1.4199200868606567,
      "learning_rate": 3.661008958038661e-06,
      "loss": 0.024,
      "step": 75550
    },
    {
      "epoch": 35.64297919981144,
      "grad_norm": 0.868404746055603,
      "learning_rate": 3.6413641364136417e-06,
      "loss": 0.0227,
      "step": 75600
    },
    {
      "epoch": 35.666548818572856,
      "grad_norm": 0.8528924584388733,
      "learning_rate": 3.621719314788622e-06,
      "loss": 0.0257,
      "step": 75650
    },
    {
      "epoch": 35.69011843733428,
      "grad_norm": 1.5291812419891357,
      "learning_rate": 3.6020744931636018e-06,
      "loss": 0.0234,
      "step": 75700
    },
    {
      "epoch": 35.71368805609569,
      "grad_norm": 2.348891258239746,
      "learning_rate": 3.5824296715385825e-06,
      "loss": 0.025,
      "step": 75750
    },
    {
      "epoch": 35.73725767485711,
      "grad_norm": 1.458945870399475,
      "learning_rate": 3.5627848499135627e-06,
      "loss": 0.0251,
      "step": 75800
    },
    {
      "epoch": 35.760827293618526,
      "grad_norm": 0.7094916701316833,
      "learning_rate": 3.543140028288543e-06,
      "loss": 0.0249,
      "step": 75850
    },
    {
      "epoch": 35.78439691237994,
      "grad_norm": 0.2706506550312042,
      "learning_rate": 3.5234952066635237e-06,
      "loss": 0.0201,
      "step": 75900
    },
    {
      "epoch": 35.80796653114136,
      "grad_norm": 0.45017117261886597,
      "learning_rate": 3.503850385038504e-06,
      "loss": 0.0253,
      "step": 75950
    },
    {
      "epoch": 35.831536149902774,
      "grad_norm": 1.985114574432373,
      "learning_rate": 3.484205563413484e-06,
      "loss": 0.029,
      "step": 76000
    },
    {
      "epoch": 35.85510576866419,
      "grad_norm": 1.2818317413330078,
      "learning_rate": 3.464560741788465e-06,
      "loss": 0.0252,
      "step": 76050
    },
    {
      "epoch": 35.87867538742561,
      "grad_norm": 1.2045540809631348,
      "learning_rate": 3.444915920163445e-06,
      "loss": 0.0263,
      "step": 76100
    },
    {
      "epoch": 35.90224500618702,
      "grad_norm": 1.3545118570327759,
      "learning_rate": 3.425271098538425e-06,
      "loss": 0.0299,
      "step": 76150
    },
    {
      "epoch": 35.925814624948444,
      "grad_norm": 0.8531763553619385,
      "learning_rate": 3.4056262769134057e-06,
      "loss": 0.0241,
      "step": 76200
    },
    {
      "epoch": 35.94938424370986,
      "grad_norm": 1.113437533378601,
      "learning_rate": 3.385981455288386e-06,
      "loss": 0.0276,
      "step": 76250
    },
    {
      "epoch": 35.97295386247127,
      "grad_norm": 1.5059360265731812,
      "learning_rate": 3.366336633663366e-06,
      "loss": 0.0274,
      "step": 76300
    },
    {
      "epoch": 35.99652348123269,
      "grad_norm": 0.9450069665908813,
      "learning_rate": 3.346691812038347e-06,
      "loss": 0.0258,
      "step": 76350
    },
    {
      "epoch": 36.01979847975959,
      "grad_norm": 1.2947866916656494,
      "learning_rate": 3.327046990413327e-06,
      "loss": 0.0255,
      "step": 76400
    },
    {
      "epoch": 36.04336809852101,
      "grad_norm": 0.654771089553833,
      "learning_rate": 3.3074021687883074e-06,
      "loss": 0.0212,
      "step": 76450
    },
    {
      "epoch": 36.06693771728242,
      "grad_norm": 0.497175395488739,
      "learning_rate": 3.287757347163288e-06,
      "loss": 0.0224,
      "step": 76500
    },
    {
      "epoch": 36.09050733604384,
      "grad_norm": 0.6971431374549866,
      "learning_rate": 3.2681125255382684e-06,
      "loss": 0.0185,
      "step": 76550
    },
    {
      "epoch": 36.114076954805256,
      "grad_norm": 1.3995996713638306,
      "learning_rate": 3.2484677039132482e-06,
      "loss": 0.022,
      "step": 76600
    },
    {
      "epoch": 36.13764657356667,
      "grad_norm": 0.7131780385971069,
      "learning_rate": 3.228822882288229e-06,
      "loss": 0.0198,
      "step": 76650
    },
    {
      "epoch": 36.16121619232809,
      "grad_norm": 0.7352547645568848,
      "learning_rate": 3.209178060663209e-06,
      "loss": 0.0222,
      "step": 76700
    },
    {
      "epoch": 36.184785811089505,
      "grad_norm": 0.35770806670188904,
      "learning_rate": 3.1895332390381894e-06,
      "loss": 0.0234,
      "step": 76750
    },
    {
      "epoch": 36.208355429850926,
      "grad_norm": 0.2441006600856781,
      "learning_rate": 3.16988841741317e-06,
      "loss": 0.0233,
      "step": 76800
    },
    {
      "epoch": 36.23192504861234,
      "grad_norm": 2.3936727046966553,
      "learning_rate": 3.1502435957881504e-06,
      "loss": 0.0259,
      "step": 76850
    },
    {
      "epoch": 36.25549466737375,
      "grad_norm": 1.007898211479187,
      "learning_rate": 3.1305987741631306e-06,
      "loss": 0.0248,
      "step": 76900
    },
    {
      "epoch": 36.279064286135174,
      "grad_norm": 1.244150161743164,
      "learning_rate": 3.1109539525381113e-06,
      "loss": 0.0218,
      "step": 76950
    },
    {
      "epoch": 36.30263390489659,
      "grad_norm": 0.971170961856842,
      "learning_rate": 3.0913091309130916e-06,
      "loss": 0.0215,
      "step": 77000
    },
    {
      "epoch": 36.326203523658,
      "grad_norm": 1.569713830947876,
      "learning_rate": 3.0716643092880714e-06,
      "loss": 0.022,
      "step": 77050
    },
    {
      "epoch": 36.34977314241942,
      "grad_norm": 0.6715372204780579,
      "learning_rate": 3.052019487663052e-06,
      "loss": 0.0181,
      "step": 77100
    },
    {
      "epoch": 36.37334276118084,
      "grad_norm": 0.7382215261459351,
      "learning_rate": 3.0323746660380324e-06,
      "loss": 0.0224,
      "step": 77150
    },
    {
      "epoch": 36.39691237994226,
      "grad_norm": 2.748875379562378,
      "learning_rate": 3.0127298444130127e-06,
      "loss": 0.0217,
      "step": 77200
    },
    {
      "epoch": 36.42048199870367,
      "grad_norm": 1.1053472757339478,
      "learning_rate": 2.9930850227879933e-06,
      "loss": 0.0267,
      "step": 77250
    },
    {
      "epoch": 36.444051617465085,
      "grad_norm": 0.671048104763031,
      "learning_rate": 2.9734402011629736e-06,
      "loss": 0.0236,
      "step": 77300
    },
    {
      "epoch": 36.467621236226506,
      "grad_norm": 1.0993670225143433,
      "learning_rate": 2.953795379537954e-06,
      "loss": 0.0245,
      "step": 77350
    },
    {
      "epoch": 36.49119085498792,
      "grad_norm": 1.6248754262924194,
      "learning_rate": 2.9341505579129346e-06,
      "loss": 0.0236,
      "step": 77400
    },
    {
      "epoch": 36.514760473749334,
      "grad_norm": 1.5004024505615234,
      "learning_rate": 2.914505736287915e-06,
      "loss": 0.0213,
      "step": 77450
    },
    {
      "epoch": 36.538330092510755,
      "grad_norm": 0.9243518114089966,
      "learning_rate": 2.8948609146628947e-06,
      "loss": 0.0236,
      "step": 77500
    },
    {
      "epoch": 36.56189971127217,
      "grad_norm": 1.2779380083084106,
      "learning_rate": 2.8752160930378753e-06,
      "loss": 0.0251,
      "step": 77550
    },
    {
      "epoch": 36.58546933003359,
      "grad_norm": 0.9440696835517883,
      "learning_rate": 2.8555712714128556e-06,
      "loss": 0.0228,
      "step": 77600
    },
    {
      "epoch": 36.609038948795,
      "grad_norm": 1.6488356590270996,
      "learning_rate": 2.835926449787836e-06,
      "loss": 0.025,
      "step": 77650
    },
    {
      "epoch": 36.63260856755642,
      "grad_norm": 1.492343544960022,
      "learning_rate": 2.816281628162816e-06,
      "loss": 0.0296,
      "step": 77700
    },
    {
      "epoch": 36.65617818631784,
      "grad_norm": 0.7687000632286072,
      "learning_rate": 2.796636806537797e-06,
      "loss": 0.023,
      "step": 77750
    },
    {
      "epoch": 36.67974780507925,
      "grad_norm": 0.790308952331543,
      "learning_rate": 2.776991984912777e-06,
      "loss": 0.0228,
      "step": 77800
    },
    {
      "epoch": 36.70331742384067,
      "grad_norm": 0.694385826587677,
      "learning_rate": 2.7573471632877574e-06,
      "loss": 0.0268,
      "step": 77850
    },
    {
      "epoch": 36.72688704260209,
      "grad_norm": 0.46065396070480347,
      "learning_rate": 2.737702341662738e-06,
      "loss": 0.0257,
      "step": 77900
    },
    {
      "epoch": 36.7504566613635,
      "grad_norm": 1.7806016206741333,
      "learning_rate": 2.718057520037718e-06,
      "loss": 0.0231,
      "step": 77950
    },
    {
      "epoch": 36.77402628012492,
      "grad_norm": 1.9519245624542236,
      "learning_rate": 2.698412698412698e-06,
      "loss": 0.0247,
      "step": 78000
    },
    {
      "epoch": 36.797595898886335,
      "grad_norm": 0.3408622741699219,
      "learning_rate": 2.678767876787679e-06,
      "loss": 0.027,
      "step": 78050
    },
    {
      "epoch": 36.82116551764775,
      "grad_norm": 0.8856157064437866,
      "learning_rate": 2.659123055162659e-06,
      "loss": 0.022,
      "step": 78100
    },
    {
      "epoch": 36.84473513640917,
      "grad_norm": 1.0888773202896118,
      "learning_rate": 2.6394782335376394e-06,
      "loss": 0.0207,
      "step": 78150
    },
    {
      "epoch": 36.868304755170584,
      "grad_norm": 0.9893152117729187,
      "learning_rate": 2.6202263083451202e-06,
      "loss": 0.026,
      "step": 78200
    },
    {
      "epoch": 36.891874373932005,
      "grad_norm": 1.653696060180664,
      "learning_rate": 2.600581486720101e-06,
      "loss": 0.0267,
      "step": 78250
    },
    {
      "epoch": 36.91544399269342,
      "grad_norm": 1.313584327697754,
      "learning_rate": 2.5809366650950808e-06,
      "loss": 0.0246,
      "step": 78300
    },
    {
      "epoch": 36.93901361145483,
      "grad_norm": 0.952829897403717,
      "learning_rate": 2.561291843470061e-06,
      "loss": 0.0262,
      "step": 78350
    },
    {
      "epoch": 36.962583230216254,
      "grad_norm": 1.5845471620559692,
      "learning_rate": 2.5416470218450417e-06,
      "loss": 0.0228,
      "step": 78400
    },
    {
      "epoch": 36.98615284897767,
      "grad_norm": 1.0750147104263306,
      "learning_rate": 2.522002200220022e-06,
      "loss": 0.0235,
      "step": 78450
    },
    {
      "epoch": 37.00942784750457,
      "grad_norm": 0.8175989389419556,
      "learning_rate": 2.5023573785950023e-06,
      "loss": 0.0207,
      "step": 78500
    },
    {
      "epoch": 37.03299746626598,
      "grad_norm": 1.1339696645736694,
      "learning_rate": 2.482712556969983e-06,
      "loss": 0.0219,
      "step": 78550
    },
    {
      "epoch": 37.0565670850274,
      "grad_norm": 0.9282217025756836,
      "learning_rate": 2.463067735344963e-06,
      "loss": 0.0206,
      "step": 78600
    },
    {
      "epoch": 37.08013670378882,
      "grad_norm": 1.2696620225906372,
      "learning_rate": 2.4434229137199435e-06,
      "loss": 0.0213,
      "step": 78650
    },
    {
      "epoch": 37.10370632255023,
      "grad_norm": 1.6304599046707153,
      "learning_rate": 2.423778092094924e-06,
      "loss": 0.0206,
      "step": 78700
    },
    {
      "epoch": 37.12727594131165,
      "grad_norm": 1.5498583316802979,
      "learning_rate": 2.404133270469904e-06,
      "loss": 0.0208,
      "step": 78750
    },
    {
      "epoch": 37.150845560073066,
      "grad_norm": 0.8721471428871155,
      "learning_rate": 2.3844884488448843e-06,
      "loss": 0.0256,
      "step": 78800
    },
    {
      "epoch": 37.17441517883448,
      "grad_norm": 1.2457484006881714,
      "learning_rate": 2.364843627219865e-06,
      "loss": 0.0236,
      "step": 78850
    },
    {
      "epoch": 37.1979847975959,
      "grad_norm": 0.678806483745575,
      "learning_rate": 2.345198805594845e-06,
      "loss": 0.0216,
      "step": 78900
    },
    {
      "epoch": 37.221554416357314,
      "grad_norm": 0.5860145688056946,
      "learning_rate": 2.3255539839698255e-06,
      "loss": 0.0214,
      "step": 78950
    },
    {
      "epoch": 37.245124035118735,
      "grad_norm": 0.973284125328064,
      "learning_rate": 2.305909162344806e-06,
      "loss": 0.0264,
      "step": 79000
    },
    {
      "epoch": 37.26869365388015,
      "grad_norm": 0.7554807066917419,
      "learning_rate": 2.2862643407197864e-06,
      "loss": 0.0188,
      "step": 79050
    },
    {
      "epoch": 37.29226327264156,
      "grad_norm": 0.9021898508071899,
      "learning_rate": 2.2666195190947667e-06,
      "loss": 0.0198,
      "step": 79100
    },
    {
      "epoch": 37.315832891402984,
      "grad_norm": 1.4745019674301147,
      "learning_rate": 2.2469746974697474e-06,
      "loss": 0.023,
      "step": 79150
    },
    {
      "epoch": 37.3394025101644,
      "grad_norm": 1.2529488801956177,
      "learning_rate": 2.2273298758447272e-06,
      "loss": 0.022,
      "step": 79200
    },
    {
      "epoch": 37.36297212892581,
      "grad_norm": 1.8015743494033813,
      "learning_rate": 2.2076850542197075e-06,
      "loss": 0.0215,
      "step": 79250
    },
    {
      "epoch": 37.38654174768723,
      "grad_norm": 1.8183053731918335,
      "learning_rate": 2.188040232594688e-06,
      "loss": 0.0223,
      "step": 79300
    },
    {
      "epoch": 37.410111366448646,
      "grad_norm": 2.346338987350464,
      "learning_rate": 2.1683954109696684e-06,
      "loss": 0.0231,
      "step": 79350
    },
    {
      "epoch": 37.43368098521007,
      "grad_norm": 1.0597003698349,
      "learning_rate": 2.1487505893446487e-06,
      "loss": 0.0233,
      "step": 79400
    },
    {
      "epoch": 37.45725060397148,
      "grad_norm": 0.8611049056053162,
      "learning_rate": 2.1291057677196294e-06,
      "loss": 0.0238,
      "step": 79450
    },
    {
      "epoch": 37.480820222732895,
      "grad_norm": 1.6857208013534546,
      "learning_rate": 2.1094609460946096e-06,
      "loss": 0.0207,
      "step": 79500
    },
    {
      "epoch": 37.504389841494316,
      "grad_norm": 1.7561438083648682,
      "learning_rate": 2.08981612446959e-06,
      "loss": 0.0247,
      "step": 79550
    },
    {
      "epoch": 37.52795946025573,
      "grad_norm": 1.18071711063385,
      "learning_rate": 2.0701713028445706e-06,
      "loss": 0.0246,
      "step": 79600
    },
    {
      "epoch": 37.55152907901714,
      "grad_norm": 2.39884090423584,
      "learning_rate": 2.0505264812195504e-06,
      "loss": 0.0215,
      "step": 79650
    },
    {
      "epoch": 37.575098697778564,
      "grad_norm": 0.6528719067573547,
      "learning_rate": 2.0308816595945307e-06,
      "loss": 0.0248,
      "step": 79700
    },
    {
      "epoch": 37.59866831653998,
      "grad_norm": 1.5174283981323242,
      "learning_rate": 2.0112368379695114e-06,
      "loss": 0.0224,
      "step": 79750
    },
    {
      "epoch": 37.6222379353014,
      "grad_norm": 0.8300720453262329,
      "learning_rate": 1.9915920163444917e-06,
      "loss": 0.0242,
      "step": 79800
    },
    {
      "epoch": 37.64580755406281,
      "grad_norm": 0.6965876221656799,
      "learning_rate": 1.971947194719472e-06,
      "loss": 0.0241,
      "step": 79850
    },
    {
      "epoch": 37.66937717282423,
      "grad_norm": 1.3077824115753174,
      "learning_rate": 1.9523023730944526e-06,
      "loss": 0.028,
      "step": 79900
    },
    {
      "epoch": 37.69294679158565,
      "grad_norm": 1.7629978656768799,
      "learning_rate": 1.932657551469433e-06,
      "loss": 0.0251,
      "step": 79950
    },
    {
      "epoch": 37.71651641034706,
      "grad_norm": 1.5647541284561157,
      "learning_rate": 1.913012729844413e-06,
      "loss": 0.0229,
      "step": 80000
    },
    {
      "epoch": 37.71651641034706,
      "eval_loss": 0.27592340111732483,
      "eval_runtime": 163.3682,
      "eval_samples_per_second": 178.088,
      "eval_steps_per_second": 44.525,
      "step": 80000
    },
    {
      "epoch": 37.74008602910848,
      "grad_norm": 0.33984842896461487,
      "learning_rate": 1.8933679082193936e-06,
      "loss": 0.0211,
      "step": 80050
    },
    {
      "epoch": 37.763655647869896,
      "grad_norm": 0.7790636420249939,
      "learning_rate": 1.8737230865943739e-06,
      "loss": 0.0222,
      "step": 80100
    },
    {
      "epoch": 37.78722526663131,
      "grad_norm": 1.2112979888916016,
      "learning_rate": 1.8540782649693541e-06,
      "loss": 0.022,
      "step": 80150
    },
    {
      "epoch": 37.81079488539273,
      "grad_norm": 0.9697446823120117,
      "learning_rate": 1.8344334433443344e-06,
      "loss": 0.022,
      "step": 80200
    },
    {
      "epoch": 37.834364504154145,
      "grad_norm": 0.29209765791893005,
      "learning_rate": 1.8147886217193149e-06,
      "loss": 0.0202,
      "step": 80250
    },
    {
      "epoch": 37.85793412291556,
      "grad_norm": 1.3661117553710938,
      "learning_rate": 1.7951438000942951e-06,
      "loss": 0.0223,
      "step": 80300
    },
    {
      "epoch": 37.88150374167698,
      "grad_norm": 1.4908140897750854,
      "learning_rate": 1.7754989784692756e-06,
      "loss": 0.0243,
      "step": 80350
    },
    {
      "epoch": 37.905073360438394,
      "grad_norm": 1.1121543645858765,
      "learning_rate": 1.755854156844256e-06,
      "loss": 0.0212,
      "step": 80400
    },
    {
      "epoch": 37.928642979199815,
      "grad_norm": 1.2983720302581787,
      "learning_rate": 1.7362093352192361e-06,
      "loss": 0.0271,
      "step": 80450
    },
    {
      "epoch": 37.95221259796123,
      "grad_norm": 1.4426720142364502,
      "learning_rate": 1.7165645135942166e-06,
      "loss": 0.0221,
      "step": 80500
    },
    {
      "epoch": 37.97578221672264,
      "grad_norm": 1.5514453649520874,
      "learning_rate": 1.6969196919691969e-06,
      "loss": 0.0272,
      "step": 80550
    },
    {
      "epoch": 37.99935183548406,
      "grad_norm": 0.844972550868988,
      "learning_rate": 1.6772748703441774e-06,
      "loss": 0.0225,
      "step": 80600
    },
    {
      "epoch": 38.02262683401096,
      "grad_norm": 1.6178110837936401,
      "learning_rate": 1.6576300487191576e-06,
      "loss": 0.0193,
      "step": 80650
    },
    {
      "epoch": 38.04619645277238,
      "grad_norm": 1.3807404041290283,
      "learning_rate": 1.6379852270941379e-06,
      "loss": 0.0234,
      "step": 80700
    },
    {
      "epoch": 38.06976607153379,
      "grad_norm": 1.2096760272979736,
      "learning_rate": 1.6183404054691184e-06,
      "loss": 0.0277,
      "step": 80750
    },
    {
      "epoch": 38.09333569029521,
      "grad_norm": 1.152690052986145,
      "learning_rate": 1.5986955838440988e-06,
      "loss": 0.0231,
      "step": 80800
    },
    {
      "epoch": 38.11690530905663,
      "grad_norm": 2.014840602874756,
      "learning_rate": 1.579050762219079e-06,
      "loss": 0.0211,
      "step": 80850
    },
    {
      "epoch": 38.14047492781804,
      "grad_norm": 0.3333435654640198,
      "learning_rate": 1.5594059405940594e-06,
      "loss": 0.0226,
      "step": 80900
    },
    {
      "epoch": 38.16404454657946,
      "grad_norm": 0.7619212865829468,
      "learning_rate": 1.5397611189690398e-06,
      "loss": 0.0211,
      "step": 80950
    },
    {
      "epoch": 38.187614165340875,
      "grad_norm": 0.45973244309425354,
      "learning_rate": 1.52011629734402e-06,
      "loss": 0.0198,
      "step": 81000
    },
    {
      "epoch": 38.21118378410229,
      "grad_norm": 0.22104203701019287,
      "learning_rate": 1.5004714757190006e-06,
      "loss": 0.0183,
      "step": 81050
    },
    {
      "epoch": 38.23475340286371,
      "grad_norm": 1.3658262491226196,
      "learning_rate": 1.4808266540939808e-06,
      "loss": 0.0204,
      "step": 81100
    },
    {
      "epoch": 38.258323021625124,
      "grad_norm": 2.2539620399475098,
      "learning_rate": 1.461181832468961e-06,
      "loss": 0.0237,
      "step": 81150
    },
    {
      "epoch": 38.281892640386545,
      "grad_norm": 0.6619176864624023,
      "learning_rate": 1.4415370108439416e-06,
      "loss": 0.0221,
      "step": 81200
    },
    {
      "epoch": 38.30546225914796,
      "grad_norm": 1.071667194366455,
      "learning_rate": 1.421892189218922e-06,
      "loss": 0.0201,
      "step": 81250
    },
    {
      "epoch": 38.32903187790937,
      "grad_norm": 0.7358114719390869,
      "learning_rate": 1.4022473675939023e-06,
      "loss": 0.0203,
      "step": 81300
    },
    {
      "epoch": 38.35260149667079,
      "grad_norm": 0.42737072706222534,
      "learning_rate": 1.3826025459688826e-06,
      "loss": 0.0239,
      "step": 81350
    },
    {
      "epoch": 38.37617111543221,
      "grad_norm": 1.266525149345398,
      "learning_rate": 1.3633506207763635e-06,
      "loss": 0.0203,
      "step": 81400
    },
    {
      "epoch": 38.39974073419362,
      "grad_norm": 1.1107969284057617,
      "learning_rate": 1.3437057991513437e-06,
      "loss": 0.0195,
      "step": 81450
    },
    {
      "epoch": 38.42331035295504,
      "grad_norm": 0.7913179397583008,
      "learning_rate": 1.324060977526324e-06,
      "loss": 0.0199,
      "step": 81500
    },
    {
      "epoch": 38.446879971716456,
      "grad_norm": 1.476545810699463,
      "learning_rate": 1.3044161559013045e-06,
      "loss": 0.0206,
      "step": 81550
    },
    {
      "epoch": 38.47044959047788,
      "grad_norm": 0.46558699011802673,
      "learning_rate": 1.284771334276285e-06,
      "loss": 0.0224,
      "step": 81600
    },
    {
      "epoch": 38.49401920923929,
      "grad_norm": 2.181037664413452,
      "learning_rate": 1.2651265126512652e-06,
      "loss": 0.0245,
      "step": 81650
    },
    {
      "epoch": 38.517588828000704,
      "grad_norm": 1.4090615510940552,
      "learning_rate": 1.2454816910262455e-06,
      "loss": 0.0242,
      "step": 81700
    },
    {
      "epoch": 38.541158446762125,
      "grad_norm": 0.5110472440719604,
      "learning_rate": 1.2258368694012257e-06,
      "loss": 0.0174,
      "step": 81750
    },
    {
      "epoch": 38.56472806552354,
      "grad_norm": 0.7326122522354126,
      "learning_rate": 1.2061920477762062e-06,
      "loss": 0.0231,
      "step": 81800
    },
    {
      "epoch": 38.58829768428496,
      "grad_norm": 1.1855084896087646,
      "learning_rate": 1.1865472261511867e-06,
      "loss": 0.0172,
      "step": 81850
    },
    {
      "epoch": 38.611867303046374,
      "grad_norm": 0.9744023680686951,
      "learning_rate": 1.1669024045261667e-06,
      "loss": 0.0201,
      "step": 81900
    },
    {
      "epoch": 38.63543692180779,
      "grad_norm": 0.3290623426437378,
      "learning_rate": 1.1472575829011472e-06,
      "loss": 0.0165,
      "step": 81950
    },
    {
      "epoch": 38.65900654056921,
      "grad_norm": 0.6197929978370667,
      "learning_rate": 1.1276127612761277e-06,
      "loss": 0.0223,
      "step": 82000
    },
    {
      "epoch": 38.68257615933062,
      "grad_norm": 1.2738662958145142,
      "learning_rate": 1.107967939651108e-06,
      "loss": 0.0219,
      "step": 82050
    },
    {
      "epoch": 38.706145778092036,
      "grad_norm": 0.5650861859321594,
      "learning_rate": 1.0883231180260884e-06,
      "loss": 0.0235,
      "step": 82100
    },
    {
      "epoch": 38.72971539685346,
      "grad_norm": 1.0646735429763794,
      "learning_rate": 1.0686782964010687e-06,
      "loss": 0.0215,
      "step": 82150
    },
    {
      "epoch": 38.75328501561487,
      "grad_norm": 0.9960146546363831,
      "learning_rate": 1.049033474776049e-06,
      "loss": 0.0226,
      "step": 82200
    },
    {
      "epoch": 38.77685463437629,
      "grad_norm": 0.8602478504180908,
      "learning_rate": 1.0293886531510294e-06,
      "loss": 0.0217,
      "step": 82250
    },
    {
      "epoch": 38.800424253137706,
      "grad_norm": 0.660849928855896,
      "learning_rate": 1.00974383152601e-06,
      "loss": 0.0243,
      "step": 82300
    },
    {
      "epoch": 38.82399387189912,
      "grad_norm": 1.0078648328781128,
      "learning_rate": 9.9009900990099e-07,
      "loss": 0.0241,
      "step": 82350
    },
    {
      "epoch": 38.84756349066054,
      "grad_norm": 1.4042754173278809,
      "learning_rate": 9.704541882759704e-07,
      "loss": 0.0193,
      "step": 82400
    },
    {
      "epoch": 38.871133109421955,
      "grad_norm": 1.667493462562561,
      "learning_rate": 9.508093666509508e-07,
      "loss": 0.0218,
      "step": 82450
    },
    {
      "epoch": 38.89470272818337,
      "grad_norm": 0.7910239100456238,
      "learning_rate": 9.311645450259313e-07,
      "loss": 0.0238,
      "step": 82500
    },
    {
      "epoch": 38.91827234694479,
      "grad_norm": 0.9727099537849426,
      "learning_rate": 9.115197234009115e-07,
      "loss": 0.0204,
      "step": 82550
    },
    {
      "epoch": 38.9418419657062,
      "grad_norm": 1.7936453819274902,
      "learning_rate": 8.918749017758919e-07,
      "loss": 0.0197,
      "step": 82600
    },
    {
      "epoch": 38.965411584467624,
      "grad_norm": 0.6281057000160217,
      "learning_rate": 8.722300801508722e-07,
      "loss": 0.0246,
      "step": 82650
    },
    {
      "epoch": 38.98898120322904,
      "grad_norm": 1.8302141427993774,
      "learning_rate": 8.525852585258527e-07,
      "loss": 0.0224,
      "step": 82700
    },
    {
      "epoch": 39.01225620175594,
      "grad_norm": 1.1077748537063599,
      "learning_rate": 8.329404369008329e-07,
      "loss": 0.0216,
      "step": 82750
    },
    {
      "epoch": 39.03582582051735,
      "grad_norm": 2.046473264694214,
      "learning_rate": 8.132956152758133e-07,
      "loss": 0.0241,
      "step": 82800
    },
    {
      "epoch": 39.05939543927877,
      "grad_norm": 1.6544852256774902,
      "learning_rate": 7.936507936507937e-07,
      "loss": 0.0202,
      "step": 82850
    },
    {
      "epoch": 39.08296505804019,
      "grad_norm": 1.1676864624023438,
      "learning_rate": 7.74005972025774e-07,
      "loss": 0.0256,
      "step": 82900
    },
    {
      "epoch": 39.1065346768016,
      "grad_norm": 0.9692486524581909,
      "learning_rate": 7.543611504007544e-07,
      "loss": 0.0185,
      "step": 82950
    },
    {
      "epoch": 39.13010429556302,
      "grad_norm": 0.8819231986999512,
      "learning_rate": 7.347163287757348e-07,
      "loss": 0.0218,
      "step": 83000
    },
    {
      "epoch": 39.153673914324436,
      "grad_norm": 0.6431484222412109,
      "learning_rate": 7.150715071507151e-07,
      "loss": 0.0207,
      "step": 83050
    },
    {
      "epoch": 39.17724353308585,
      "grad_norm": 0.5082546472549438,
      "learning_rate": 6.954266855256954e-07,
      "loss": 0.0171,
      "step": 83100
    },
    {
      "epoch": 39.20081315184727,
      "grad_norm": 1.467507004737854,
      "learning_rate": 6.757818639006759e-07,
      "loss": 0.0211,
      "step": 83150
    },
    {
      "epoch": 39.224382770608685,
      "grad_norm": 0.9863935708999634,
      "learning_rate": 6.561370422756561e-07,
      "loss": 0.0195,
      "step": 83200
    },
    {
      "epoch": 39.2479523893701,
      "grad_norm": 0.9061817526817322,
      "learning_rate": 6.364922206506365e-07,
      "loss": 0.0207,
      "step": 83250
    },
    {
      "epoch": 39.27152200813152,
      "grad_norm": 1.4258195161819458,
      "learning_rate": 6.168473990256168e-07,
      "loss": 0.0205,
      "step": 83300
    },
    {
      "epoch": 39.29509162689293,
      "grad_norm": 0.5463023781776428,
      "learning_rate": 5.972025774005972e-07,
      "loss": 0.022,
      "step": 83350
    },
    {
      "epoch": 39.318661245654354,
      "grad_norm": 0.49158671498298645,
      "learning_rate": 5.775577557755776e-07,
      "loss": 0.0226,
      "step": 83400
    },
    {
      "epoch": 39.34223086441577,
      "grad_norm": 0.535324215888977,
      "learning_rate": 5.579129341505579e-07,
      "loss": 0.0176,
      "step": 83450
    },
    {
      "epoch": 39.36580048317718,
      "grad_norm": 1.3216804265975952,
      "learning_rate": 5.382681125255384e-07,
      "loss": 0.0228,
      "step": 83500
    },
    {
      "epoch": 39.3893701019386,
      "grad_norm": 0.8406159281730652,
      "learning_rate": 5.186232909005186e-07,
      "loss": 0.0222,
      "step": 83550
    },
    {
      "epoch": 39.41293972070002,
      "grad_norm": 0.5714114904403687,
      "learning_rate": 4.98978469275499e-07,
      "loss": 0.021,
      "step": 83600
    },
    {
      "epoch": 39.43650933946144,
      "grad_norm": 0.9228162169456482,
      "learning_rate": 4.793336476504794e-07,
      "loss": 0.0207,
      "step": 83650
    },
    {
      "epoch": 39.46007895822285,
      "grad_norm": 1.8179272413253784,
      "learning_rate": 4.5968882602545973e-07,
      "loss": 0.021,
      "step": 83700
    },
    {
      "epoch": 39.483648576984265,
      "grad_norm": 1.0290136337280273,
      "learning_rate": 4.4004400440044005e-07,
      "loss": 0.0216,
      "step": 83750
    },
    {
      "epoch": 39.507218195745686,
      "grad_norm": 1.5494627952575684,
      "learning_rate": 4.203991827754204e-07,
      "loss": 0.0218,
      "step": 83800
    },
    {
      "epoch": 39.5307878145071,
      "grad_norm": 1.0665693283081055,
      "learning_rate": 4.007543611504008e-07,
      "loss": 0.0193,
      "step": 83850
    },
    {
      "epoch": 39.554357433268514,
      "grad_norm": 1.1000587940216064,
      "learning_rate": 3.811095395253811e-07,
      "loss": 0.0213,
      "step": 83900
    },
    {
      "epoch": 39.577927052029935,
      "grad_norm": 0.17178867757320404,
      "learning_rate": 3.6146471790036147e-07,
      "loss": 0.0209,
      "step": 83950
    },
    {
      "epoch": 39.60149667079135,
      "grad_norm": 2.3883254528045654,
      "learning_rate": 3.418198962753418e-07,
      "loss": 0.0195,
      "step": 84000
    },
    {
      "epoch": 39.62506628955277,
      "grad_norm": 0.7759448289871216,
      "learning_rate": 3.2217507465032216e-07,
      "loss": 0.0195,
      "step": 84050
    },
    {
      "epoch": 39.64863590831418,
      "grad_norm": 0.9549598097801208,
      "learning_rate": 3.025302530253026e-07,
      "loss": 0.0236,
      "step": 84100
    },
    {
      "epoch": 39.6722055270756,
      "grad_norm": 1.2017217874526978,
      "learning_rate": 2.828854314002829e-07,
      "loss": 0.0235,
      "step": 84150
    },
    {
      "epoch": 39.69577514583702,
      "grad_norm": 0.7281485199928284,
      "learning_rate": 2.6324060977526327e-07,
      "loss": 0.0184,
      "step": 84200
    },
    {
      "epoch": 39.71934476459843,
      "grad_norm": 0.6808325052261353,
      "learning_rate": 2.435957881502436e-07,
      "loss": 0.024,
      "step": 84250
    },
    {
      "epoch": 39.742914383359846,
      "grad_norm": 0.6317086219787598,
      "learning_rate": 2.2395096652522396e-07,
      "loss": 0.0236,
      "step": 84300
    },
    {
      "epoch": 39.76648400212127,
      "grad_norm": 0.9942632913589478,
      "learning_rate": 2.043061449002043e-07,
      "loss": 0.0195,
      "step": 84350
    },
    {
      "epoch": 39.79005362088268,
      "grad_norm": 1.144407033920288,
      "learning_rate": 1.8466132327518467e-07,
      "loss": 0.0229,
      "step": 84400
    },
    {
      "epoch": 39.8136232396441,
      "grad_norm": 0.9991743564605713,
      "learning_rate": 1.6501650165016504e-07,
      "loss": 0.0219,
      "step": 84450
    },
    {
      "epoch": 39.837192858405515,
      "grad_norm": 0.37234535813331604,
      "learning_rate": 1.4537168002514538e-07,
      "loss": 0.0239,
      "step": 84500
    },
    {
      "epoch": 39.86076247716693,
      "grad_norm": 0.6063280701637268,
      "learning_rate": 1.2572685840012572e-07,
      "loss": 0.0201,
      "step": 84550
    },
    {
      "epoch": 39.88433209592835,
      "grad_norm": 1.437660574913025,
      "learning_rate": 1.0608203677510608e-07,
      "loss": 0.0212,
      "step": 84600
    },
    {
      "epoch": 39.907901714689764,
      "grad_norm": 1.2078757286071777,
      "learning_rate": 8.643721515008644e-08,
      "loss": 0.0198,
      "step": 84650
    },
    {
      "epoch": 39.93147133345118,
      "grad_norm": 1.3294316530227661,
      "learning_rate": 6.67923935250668e-08,
      "loss": 0.0178,
      "step": 84700
    },
    {
      "epoch": 39.9550409522126,
      "grad_norm": 1.425585389137268,
      "learning_rate": 4.714757190004715e-08,
      "loss": 0.0184,
      "step": 84750
    },
    {
      "epoch": 39.97861057097401,
      "grad_norm": 0.6262964010238647,
      "learning_rate": 2.7502750275027503e-08,
      "loss": 0.0203,
      "step": 84800
    },
    {
      "epoch": 39.99746626598315,
      "step": 84840,
      "total_flos": 9200128794624000.0,
      "train_loss": 0.0,
      "train_runtime": 0.1554,
      "train_samples_per_second": 1747304.856,
      "train_steps_per_second": 54579.141
    }
  ],
  "logging_steps": 50,
  "max_steps": 8480,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 5000,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 2
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9200128794624000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
